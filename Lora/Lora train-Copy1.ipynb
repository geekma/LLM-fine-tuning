{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "546793ef-4f7a-4736-8f9b-1ce766e1c6d9",
   "metadata": {},
   "source": [
    "1. 安装组件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb280ce4-9150-427d-b3f0-637ff004bd87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa8b02-844d-4838-a031-bc7cd83aa620",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3f8f9f-4c33-47c2-bdad-a7183cf547e4",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers trl peft bitsandbytes accelerate datasets pandas modelscope swanlab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902590f7-567f-4776-aa77-14f58195ee85",
   "metadata": {},
   "source": [
    "2. 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0e6a5c6-c5c1-4bd0-8df8-6c18a048d101",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-08-27T04:26:55.244420Z",
     "iopub.status.busy": "2025-08-27T04:26:55.244198Z",
     "iopub.status.idle": "2025-08-27T04:27:06.470857Z",
     "shell.execute_reply": "2025-08-27T04:27:06.470342Z",
     "shell.execute_reply.started": "2025-08-27T04:26:55.244404Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 12:26:55,287 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from delicate_medical_r1_data. Please make sure that you can trust the external codes.\n",
      "2025-08-27 12:26:55,943 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from krisfu/delicate_medical_r1_data. Please make sure that you can trust the external codes.\n",
      "2025-08-27 12:26:55,944 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from krisfu/delicate_medical_r1_data. Please make sure that you can trust the external codes.\n",
      "2025-08-27 12:26:55,944 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from krisfu/delicate_medical_r1_data. Please make sure that you can trust the external codes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has been split successfully.\n",
      "Train Set Size: 2166\n",
      "Val Set Size: 241\n"
     ]
    }
   ],
   "source": [
    "from modelscope.msdatasets import MsDataset\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# 设置随机种子\n",
    "random.seed(42)\n",
    "\n",
    "# 加载数据集\n",
    "ds = MsDataset.load(\n",
    "    'krisfu/delicate_medical_r1_data',\n",
    "    subset_name='default',\n",
    "    split='train',\n",
    "    trust_remote_code=True  # 显式声明\n",
    ")\n",
    "data_list = list(ds)\n",
    "random.shuffle(data_list)\n",
    "\n",
    "# 划分训练集和验证集\n",
    "split_idx = int(len(data_list) * 0.9)\n",
    "train_data = data_list[:split_idx]\n",
    "val_data = data_list[split_idx:]\n",
    "\n",
    "# 创建 data 目录（如果不存在）\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# 保存到 data 目录下的 jsonl 文件\n",
    "with open('data/train.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in train_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('data/val.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in val_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"The dataset has been split successfully.\")\n",
    "print(f\"Train Set Size: {len(train_data)}\")\n",
    "print(f\"Val Set Size: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7098153b-c818-46c5-b5fd-14f099cb4222",
   "metadata": {},
   "source": [
    "3. 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caf70d57-d5d8-497f-bd57-4501021b5495",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-08-27T06:45:12.938433Z",
     "iopub.status.busy": "2025-08-27T06:45:12.937857Z",
     "iopub.status.idle": "2025-08-27T07:15:28.228612Z",
     "shell.execute_reply": "2025-08-27T07:15:28.228204Z",
     "shell.execute_reply.started": "2025-08-27T06:45:12.938406Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 开始QLoRA医学问答微调流程...\n",
      "✅ CUDA环境检查通过！\n",
      "✅ 所有项目目录已准备就绪\n",
      "\n",
      "📥 正在加载基础模型的分词器...\n",
      "Downloading Model from https://www.modelscope.cn to directory: ./model_cache/qwen/Qwen3-1.7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 14:45:14,158 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 分词器加载并扩展完成\n",
      "\n",
      "⚙️ 配置4-bit量化参数(QLoRA)...\n",
      "\n",
      "🔧 加载量化后的基础模型...\n",
      "⚠️ Flash Attention 2不可用，将使用标准注意力机制: /usr/local/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248c02731c2d4e0781aa2d39776def5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 量化模型加载完成 (标准注意力机制)\n",
      "trainable params: 6,422,528 || all params: 1,726,454,784 || trainable%: 0.3720\n",
      "✅ LoRA适配器已成功应用\n",
      "\n",
      "📊 准备并预处理数据...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e85ceca24945fcaa87b78dec82a998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd36f431ee24ce1be0866cca3ac2d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/241 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 数据准备与预处理完成\n",
      "\n",
      "⚙️ 配置训练参数并初始化训练器...\n",
      "✅ 训练器初始化完成\n",
      "\n",
      "🚀 开始模型训练...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [136/136 28:13, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 训练完成!\n",
      "\n",
      "🔗 正在将LoRA适配器权重合并到基础模型中，以生成可独立部署的完整模型...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411ff9aebd4544eda0c116159d072c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    -> 正在从最新的检查点加载LoRA适配器: ./output/checkpoints/qwen-medical-qlora-lr0.0001-bs32-r16/checkpoint-136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LoRA权重合并完成！\n",
      "\n",
      "💾 正在保存合并后的完整模型...\n",
      "✅ 最终的“官方改装版”模型已保存到: ./output/final_model/qwen-medical-qlora-lr0.0001-bs32-r16\n",
      "\n",
      "🎉 QLoRA医学问答微调流程全部完成!\n",
      "🔚 程序结束\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "医学问答模型QLoRA高效微调脚本 - 【V8.0 极致注释修复版】\n",
    "\n",
    "【核心修复】\n",
    "1.  [修复] 彻底解决 `UnboundLocalError`: 将所有import语句移至文件顶部，遵循Python最佳实践，解决了局部作用域导致的变量未定义问题。\n",
    "2.  [修复] 修正模型保存逻辑: 在最后一步确保保存的是合并后的完整模型(`merged_model`)，而不是训练时使用的量化模型(`model`)。\n",
    "3.  [新增] “官方改装版”模型导出: 在训练结束后，自动将LoRA权重与基础模型合并，生成一个独立的、开箱即用的完整模型，极大方便了后续的评估和部署。\n",
    "\n",
    "【保留功能】\n",
    "1.  QLoRA微调、Flash Attention 2、完整的训练流程。\n",
    "\"\"\"\n",
    "\n",
    "# =====================================================================================\n",
    "# 【！！！关键！！！】环境变量配置 - 必须在所有import之前\n",
    "# =====================================================================================\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"DISABLE_IPEX\"] = \"1\"\n",
    "os.environ[\"DEEPSPEED_XPU_ENABLE\"] = \"0\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# =====================================================================================\n",
    "# 导入所有必需的库 (所有import都应在此处)\n",
    "# =====================================================================================\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from modelscope import snapshot_download\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, # <--- 从transformers库导入核心的模型类\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel # <--- 从peft库导入LoRA相关工具\n",
    "\n",
    "# =====================================================================================\n",
    "# 【！！！核心重点！！！】全局配置与超参数\n",
    "# =====================================================================================\n",
    "\n",
    "# --- 路径配置 (Path Configuration) ---\n",
    "# 脚本会按照这些路径创建文件夹、读取数据和保存模型。\n",
    "BASE_DIR = \".\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "FORMATTED_DATA_DIR = os.path.join(BASE_DIR, \"data_formatted\")\n",
    "MODEL_CACHE_DIR = os.path.join(BASE_DIR, \"model_cache\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"output\")\n",
    "CHECKPOINTS_DIR = os.path.join(OUTPUT_DIR, \"checkpoints\") # 训练过程中的检查点（临时存档）\n",
    "FINAL_ADAPTER_DIR = os.path.join(OUTPUT_DIR, \"final_model\") # 【注意】现在保存的是完整模型，文件夹名已修改\n",
    "\n",
    "# --- 模型配置 (Model Configuration) ---\n",
    "MODEL_ID = \"qwen/Qwen3-1.7B\" # 你选择的基础模型ID，脚本会自动从网上下载\n",
    "\n",
    "# 【系统指令 SYSTEM_PROMPT】\n",
    "#   - 是什么：给模型设定的“人设”或“行为准则”，告诉它应该扮演什么角色。\n",
    "#   - 【！！！核心重点！！！】这里的PROMPT必须与你评估脚本中使用的SYSTEM_PROMPT完全一致！\n",
    "#     这相当于训练和考核时使用同一份说明书，否则模型会感到困惑，导致评估结果不准确。\n",
    "SYSTEM_PROMPT = \"\"\"你是一个专业、严谨的AI医学助手。你的任务是根据用户提出的问题，提供准确、易懂且具有安全提示的健康信息。请记住，你的回答不能替代执业医师的诊断，必须在回答结尾处声明这一点。\"\"\"\n",
    "\n",
    "# 【最大长度 MAX_LENGTH】\n",
    "#   - 是什么：一条训练数据（问题+答案）被转换成数字后，允许的最大长度。\n",
    "#   - 为什么是2048：对于Qwen3-1.7B模型来说，这是一个比较均衡的值，能容纳大部分的问答对。\n",
    "#   - 改了会怎样：如果你的问答文本非常长，需要适当增加这个值（如4096），但这会显著增加显存消耗。如果设置太小，过长的文本会被截断，导致信息丢失。\n",
    "MAX_LENGTH = 2048\n",
    "\n",
    "# --- 【！！！核心重点！！！】训练超参数 (Hyperparameters) ---\n",
    "# 这些参数直接影响模型的学习效果、速度和资源消耗，是微调的“灵魂”。\n",
    "\n",
    "# 【学习率 Learning Rate】\n",
    "#   - 是什么：可以理解为模型学习时“每一步走多大”。\n",
    "#   - 为什么是1e-4：对于LoRA这种只训练一小部分参数的方法，可以设置比全量微调（通常是2e-5）稍大的学习率，让“适配器”参数学得更快。1e-4是QLoRA一个经过大量验证的、效果很好的起始值。\n",
    "#   - 改了会怎样：太高（如1e-3）可能导致模型“跑偏”，训练不稳定；太低（如1e-5）学习会非常慢，甚至“学不动”。\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# 【单设备批量大小 PER_DEVICE_TRAIN_BATCH_SIZE】\n",
    "#   - 是什么：一次性喂给GPU多少条数据进行训练。\n",
    "#   - 为什么是2：这是为了在有限的显存（如24GB）下运行而做的优化。值越小，单次计算的显存占用越低。\n",
    "#   - 改了会怎样：值越大，训练速度越快，但显存占用越高。如果设置过大，会导致“CUDA out of memory”错误。\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 1\n",
    "\n",
    "# 【梯度累积步数 GRADIENT_ACCUMULATION_STEPS】\n",
    "#   - 是什么：通过“攒”梯度的方式，在不增加显存的情况下，实现等效于更大Batch Size的训练效果。\n",
    "#   - 为什么是16：配合上面的`BATCH_SIZE = 2`，我们的“有效批量大小” (Effective Batch Size) = 2 * 16 = 32。这通常是一个能让模型稳定学习的批量大小。\n",
    "#   - 改了会怎样：它和`BATCH_SIZE`是跷跷板关系。显存不足时，降低`BATCH_SIZE`，同时提高此值，以保持“有效批量大小”不变。\n",
    "GRADIENT_ACCUMULATION_STEPS = 32\n",
    "\n",
    "# 【训练轮数 NUM_TRAIN_EPOCHS】\n",
    "#   - 是什么：将整个训练数据集从头到尾完整地学习多少遍。\n",
    "#   - 为什么是2：对于微调任务，通常1-3个epoch就足够了，可以快速看到效果。\n",
    "#   - 改了会怎样：太少可能导致模型“没学会”；太多则可能发生“过拟合”（模型只会死记硬背训练数据，丧失了泛化能力）。\n",
    "NUM_TRAIN_EPOCHS = 2\n",
    "\n",
    "EVAL_STEPS = 20\n",
    "SAVE_STEPS = 400\n",
    "LOGGING_STEPS = 10\n",
    "\n",
    "# --- 【！！！核心重点！！！】LoRA配置 (LoRA Configuration) ---\n",
    "\n",
    "# 【LoRA秩 R】\n",
    "#   - 是什么：LoRA“升级套件”的“复杂度”或“大小”。\n",
    "#   - 为什么是16：R值通常在8, 16, 32, 64中选择。16是一个很好的起始点，在性能和参数量之间取得了平衡。\n",
    "#   - 改了会怎样：R值越大，可训练的参数越多，理论上拟合能力越强，但显存占用也越大，且过大也可能导致过拟合。\n",
    "LORA_R = 16\n",
    "\n",
    "# 【LoRA缩放因子 Alpha】\n",
    "#   - 是什么：一个缩放参数，用来调整LoRA权重的幅度。\n",
    "#   - 为什么是32：通常建议设置为R的两倍（`alpha = 2 * r`），这是一个经验性的、效果很好的设置。\n",
    "#   - 改了会怎样：它与学习率有类似的作用，调整它会影响LoRA模块的权重大小。保持`2*r`的比例通常是最佳实践。\n",
    "LORA_ALPHA = 32\n",
    "\n",
    "LORA_DROPOUT = 0.05 # 防止过拟合的技术，训练时随机“丢弃”一些神经元。\n",
    "\n",
    "# 【目标模块 TARGET_MODULES】\n",
    "#   - 是什么：告诉LoRA应该在模型的哪些部分“加装升级套件”。\n",
    "#   - 为什么是这几个：`q_proj`, `k_proj`, `v_proj`, `o_proj`是Transformer模型中注意力机制的核心组件，对它们进行微调通常效果最显著。\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "# --- 运行配置 ---\n",
    "EFFECTIVE_BATCH_SIZE = PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "RUN_NAME = f\"qwen-medical-qlora-lr{LEARNING_RATE}-bs{EFFECTIVE_BATCH_SIZE}-r{LORA_R}\"\n",
    "\n",
    "# =====================================================================================\n",
    "# 工具函数 (无需修改)\n",
    "# =====================================================================================\n",
    "\n",
    "def check_environment():\n",
    "    # ... (此函数无需修改)\n",
    "    if not torch.cuda.is_available(): raise RuntimeError(\"❌ CUDA不可用！\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"✅ CUDA环境检查通过！\")\n",
    "\n",
    "def setup_directories():\n",
    "    # ... (此函数无需修改)\n",
    "    for directory in [DATA_DIR, FORMATTED_DATA_DIR, MODEL_CACHE_DIR, CHECKPOINTS_DIR, FINAL_ADAPTER_DIR]:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    print(\"✅ 所有项目目录已准备就绪\")\n",
    "\n",
    "def dataset_jsonl_transfer(origin_path, new_path):\n",
    "    # ... (此函数无需修改)\n",
    "    messages = []\n",
    "    with open(origin_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                messages.append({\n",
    "                    \"input\": data[\"question\"],\n",
    "                    \"output\": f'<|FunctionCallBegin|>{data[\"think\"]}<|FunctionCallEnd|>\\n{data[\"answer\"]}'\n",
    "                })\n",
    "            except (KeyError, json.JSONDecodeError): continue\n",
    "    with open(new_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for message in messages:\n",
    "            file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"✅ 数据转换完成: {len(messages)} 条有效数据\")\n",
    "\n",
    "def process_func(example, tokenizer):\n",
    "    # ... (此函数无需修改)\n",
    "    instruction = tokenizer(f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n<|im_start|>user\\n{example['input']}<|im_end|>\\n<|im_start|>assistant\\n\", add_special_tokens=False)\n",
    "    response = tokenizer(example['output'], add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": [1] * len(input_ids), \"labels\": labels}\n",
    "\n",
    "# =====================================================================================\n",
    "# 【！！！核心重点！！！】主训练流程\n",
    "# =====================================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"主训练流程\"\"\"\n",
    "    print(\"🚀 开始QLoRA医学问答微调流程...\")\n",
    "    \n",
    "    # --- 步骤1: 环境与路径初始化 ---\n",
    "    check_environment()\n",
    "    setup_directories()\n",
    "\n",
    "    # --- 步骤2: 加载分词器并扩展 ---\n",
    "    print(\"\\n📥 正在加载基础模型的分词器...\")\n",
    "    model_dir = snapshot_download(MODEL_ID, cache_dir=MODEL_CACHE_DIR, revision=\"master\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True)\n",
    "    \n",
    "    # 【关键】添加新的特殊词汇，让模型能“听懂”我们的特殊指令\n",
    "    special_tokens_dict = {'additional_special_tokens': ['<|FunctionCallBegin|>', '<|FunctionCallEnd|>']}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"✅ 分词器加载并扩展完成\")\n",
    "\n",
    "    # --- 步骤3: 配置QLoRA量化 ---\n",
    "    # 这是实现“低显存”微调的核心。它将模型参数从32位浮点数压缩到4位整数，大大减少显存占用。\n",
    "    print(\"\\n⚙️ 配置4-bit量化参数(QLoRA)...\")\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    # --- 步骤4: 加载量化模型 ---\n",
    "    print(\"\\n🔧 加载量化后的基础模型...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # 优先尝试使用Flash Attention 2，这是一种高效的注意力计算方法，可以提速并节省显存。\n",
    "    # 如果环境不支持，会自动切换到标准的注意力机制。\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dir, device_map=\"auto\", torch_dtype=torch.bfloat16,\n",
    "            quantization_config=quantization_config,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "        )\n",
    "        print(\"✅ 量化模型加载完成 (Flash Attention 2 已启用)\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Flash Attention 2不可用，将使用标准注意力机制: {e}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dir, device_map=\"auto\", torch_dtype=torch.bfloat16,\n",
    "            quantization_config=quantization_config,\n",
    "        )\n",
    "        print(\"✅ 量化模型加载完成 (标准注意力机制)\")\n",
    "    \n",
    "    # 【关键】调整模型词嵌入层的大小，以匹配我们扩展后的分词器。这是避免后续尺寸不匹配错误的核心步骤。\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # --- 步骤5: 应用LoRA适配器 ---\n",
    "    lora_config = LoraConfig(\n",
    "        r=LORA_R, lora_alpha=LORA_ALPHA, target_modules=TARGET_MODULES,\n",
    "        lora_dropout=LORA_DROPOUT, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters() # 打印出可训练参数的比例，你会发现它非常小！\n",
    "    print(\"✅ LoRA适配器已成功应用\")\n",
    "\n",
    "    # --- 步骤6 & 7: 数据准备与预处理 ---\n",
    "    # (这部分代码无需修改，它会自动处理数据格式转换和Tokenization)\n",
    "    print(\"\\n📊 准备并预处理数据...\")\n",
    "    train_original_path = os.path.join(DATA_DIR, \"train.jsonl\")\n",
    "    val_original_path = os.path.join(DATA_DIR, \"val.jsonl\")\n",
    "    train_formatted_path = os.path.join(FORMATTED_DATA_DIR, \"train_formatted.jsonl\")\n",
    "    val_formatted_path = os.path.join(FORMATTED_DATA_DIR, \"val_formatted.jsonl\")\n",
    "    if not os.path.exists(train_formatted_path): dataset_jsonl_transfer(train_original_path, train_formatted_path)\n",
    "    if not os.path.exists(val_formatted_path): dataset_jsonl_transfer(val_original_path, val_formatted_path)\n",
    "    train_dataset = Dataset.from_pandas(pd.read_json(train_formatted_path, lines=True))\n",
    "    eval_dataset = Dataset.from_pandas(pd.read_json(val_formatted_path, lines=True))\n",
    "    tokenized_train_dataset = train_dataset.map(lambda x: process_func(x, tokenizer), remove_columns=train_dataset.column_names)\n",
    "    tokenized_eval_dataset = eval_dataset.map(lambda x: process_func(x, tokenizer), remove_columns=eval_dataset.column_names)\n",
    "    print(\"✅ 数据准备与预处理完成\")\n",
    "\n",
    "    # --- 步骤8 & 9: 配置训练参数并初始化Trainer ---\n",
    "    print(\"\\n⚙️ 配置训练参数并初始化训练器...\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=os.path.join(CHECKPOINTS_DIR, RUN_NAME),\n",
    "        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.1,\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=EVAL_STEPS,\n",
    "        save_steps=SAVE_STEPS,\n",
    "        save_total_limit=2,\n",
    "        bf16=True, # 使用bf16混合精度训练，可以提速并节省显存\n",
    "        gradient_checkpointing=True, # 关键的显存优化技术，用时间换空间\n",
    "        optim=\"paged_adamw_8bit\", # 使用分页优化器，进一步节省显存\n",
    "        remove_unused_columns=False,\n",
    "        #report_to=\"none\", # 禁用外部日志上报\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model, args=training_args, train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_eval_dataset,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)\n",
    "    )\n",
    "    print(\"✅ 训练器初始化完成\")\n",
    "\n",
    "    # --- 步骤10: 开始训练 ---\n",
    "    print(\"\\n🚀 开始模型训练...\")\n",
    "    trainer.train()\n",
    "    print(\"🎉 训练完成!\")\n",
    "\n",
    "    # --- 【！！！核心重点：合并权重并保存为完整模型！！！】 ---\n",
    "    print(\"\\n🔗 正在将LoRA适配器权重合并到基础模型中，以生成可独立部署的完整模型...\")\n",
    "    \n",
    "    # 释放显存，为加载全精度模型做准备\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # 重新加载全精度的基础模型\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir, torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True\n",
    "    )\n",
    "    # 再次扩展词汇表，确保结构一致\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # 从最新的检查点加载我们训练好的LoRA适配器权重\n",
    "    lora_model_path = os.path.join(CHECKPOINTS_DIR, RUN_NAME)\n",
    "    latest_checkpoint = max([d for d in os.listdir(lora_model_path) if d.startswith(\"checkpoint-\")], key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    adapter_path = os.path.join(lora_model_path, latest_checkpoint)\n",
    "    print(f\"    -> 正在从最新的检查点加载LoRA适配器: {adapter_path}\")\n",
    "    \n",
    "    model_to_merge = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "    # 执行合并！这将返回一个全新的、完整的、微调后的模型\n",
    "    merged_model = model_to_merge.merge_and_unload()\n",
    "    print(\"✅ LoRA权重合并完成！\")\n",
    "    \n",
    "    # --- 步骤11: 保存最终的完整模型 ---\n",
    "    print(\"\\n💾 正在保存合并后的完整模型...\")\n",
    "    final_model_path = os.path.join(FINAL_ADAPTER_DIR, RUN_NAME)\n",
    "    \n",
    "    # 【关键修复】确保我们保存的是`merged_model`，而不是旧的`model`\n",
    "    merged_model.save_pretrained(final_model_path)\n",
    "    tokenizer.save_pretrained(final_model_path)\n",
    "    \n",
    "    print(f\"✅ 最终的“官方改装版”模型已保存到: {final_model_path}\")\n",
    "    print(\"\\n🎉 QLoRA医学问答微调流程全部完成!\")\n",
    "\n",
    "# =====================================================================================\n",
    "# 程序入口\n",
    "# =====================================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 程序执行失败: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        print(\"🔚 程序结束\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
