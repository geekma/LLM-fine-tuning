{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "546793ef-4f7a-4736-8f9b-1ce766e1c6d9",
   "metadata": {},
   "source": [
    "1. å®‰è£…ç»„ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb280ce4-9150-427d-b3f0-637ff004bd87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa8b02-844d-4838-a031-bc7cd83aa620",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3f8f9f-4c33-47c2-bdad-a7183cf547e4",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers trl peft bitsandbytes accelerate datasets pandas modelscope swanlab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902590f7-567f-4776-aa77-14f58195ee85",
   "metadata": {},
   "source": [
    "2. åŠ è½½æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0e6a5c6-c5c1-4bd0-8df8-6c18a048d101",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-08-27T04:26:55.244420Z",
     "iopub.status.busy": "2025-08-27T04:26:55.244198Z",
     "iopub.status.idle": "2025-08-27T04:27:06.470857Z",
     "shell.execute_reply": "2025-08-27T04:27:06.470342Z",
     "shell.execute_reply.started": "2025-08-27T04:26:55.244404Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 12:26:55,287 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from delicate_medical_r1_data. Please make sure that you can trust the external codes.\n",
      "2025-08-27 12:26:55,943 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from krisfu/delicate_medical_r1_data. Please make sure that you can trust the external codes.\n",
      "2025-08-27 12:26:55,944 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from krisfu/delicate_medical_r1_data. Please make sure that you can trust the external codes.\n",
      "2025-08-27 12:26:55,944 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from krisfu/delicate_medical_r1_data. Please make sure that you can trust the external codes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has been split successfully.\n",
      "Train Set Size: 2166\n",
      "Val Set Size: 241\n"
     ]
    }
   ],
   "source": [
    "from modelscope.msdatasets import MsDataset\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "random.seed(42)\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "ds = MsDataset.load(\n",
    "    'krisfu/delicate_medical_r1_data',\n",
    "    subset_name='default',\n",
    "    split='train',\n",
    "    trust_remote_code=True  # æ˜¾å¼å£°æ˜\n",
    ")\n",
    "data_list = list(ds)\n",
    "random.shuffle(data_list)\n",
    "\n",
    "# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "split_idx = int(len(data_list) * 0.9)\n",
    "train_data = data_list[:split_idx]\n",
    "val_data = data_list[split_idx:]\n",
    "\n",
    "# åˆ›å»º data ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# ä¿å­˜åˆ° data ç›®å½•ä¸‹çš„ jsonl æ–‡ä»¶\n",
    "with open('data/train.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in train_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('data/val.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in val_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"The dataset has been split successfully.\")\n",
    "print(f\"Train Set Size: {len(train_data)}\")\n",
    "print(f\"Val Set Size: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7098153b-c818-46c5-b5fd-14f099cb4222",
   "metadata": {},
   "source": [
    "3. å¼€å§‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caf70d57-d5d8-497f-bd57-4501021b5495",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-08-27T07:16:48.799991Z",
     "iopub.status.busy": "2025-08-27T07:16:48.799653Z",
     "iopub.status.idle": "2025-08-27T07:18:44.240307Z",
     "shell.execute_reply": "2025-08-27T07:18:44.239443Z",
     "shell.execute_reply.started": "2025-08-27T07:16:48.799975Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹QLoRAåŒ»å­¦é—®ç­”å¾®è°ƒæµç¨‹...\n",
      "âœ… CUDAç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼\n",
      "ğŸ“Š å¯ç”¨GPUæ•°é‡: 1\n",
      "ğŸ”§ å½“å‰CUDAè®¾å¤‡: NVIDIA A10\n",
      "ğŸ’¾ GPUæ€»æ˜¾å­˜: 23.7 GB\n",
      "âœ… æ‰€æœ‰é¡¹ç›®ç›®å½•å·²å‡†å¤‡å°±ç»ª\n",
      "\n",
      "ğŸ“¥ æ­£åœ¨ä»'Qwen/Qwen3-1.7B'åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨...\n",
      "Downloading Model from https://www.modelscope.cn to directory: ./model_cache/Qwen/Qwen3-1.7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 15:16:50,047 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åˆ†è¯å™¨åŠ è½½å¹¶æ‰©å±•å®Œæˆ\n",
      "\n",
      "âš™ï¸ é…ç½®4-bité‡åŒ–å‚æ•°(QLoRA)...\n",
      "\n",
      "ğŸ”§ åŠ è½½é‡åŒ–æ¨¡å‹å¹¶åº”ç”¨LoRA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4186461270964fad8ea6672bbba6d99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… é‡åŒ–æ¨¡å‹åŠ è½½å®Œæˆ (Flash Attention 2 å·²å¯ç”¨)\n",
      "trainable params: 12,845,056 || all params: 1,732,877,312 || trainable%: 0.7413\n",
      "âœ… LoRAé€‚é…å™¨å·²æˆåŠŸåº”ç”¨\n",
      "\n",
      "ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfe7fd0b0354d58b98c037555cf3c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15051107f9494888933efee23845feaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/241 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ\n",
      "\n",
      "âš™ï¸ é…ç½®è®­ç»ƒå‚æ•°...\n",
      "\n",
      "ğŸƒâ€â™‚ï¸ åˆå§‹åŒ–è®­ç»ƒå™¨...\n",
      "\n",
      "âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: module 'torch.xpu' has no attribute 'get_arch_list'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_524/2761892960.py\", line 530, in <module>\n",
      "    main()\n",
      "  File \"/tmp/ipykernel_524/2761892960.py\", line 455, in main\n",
      "    trainer = Trainer(\n",
      "              ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/transformers/trainer.py\", line 631, in __init__\n",
      "    unwrapped_model = self.accelerator.unwrap_model(model)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/accelerate/accelerator.py\", line 3128, in unwrap_model\n",
      "    return extract_model_from_parallel(model, keep_fp32_wrapper, keep_torch_compile)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/accelerate/utils/other.py\", line 250, in extract_model_from_parallel\n",
      "    from deepspeed import DeepSpeedEngine\n",
      "  File \"/usr/local/lib/python3.11/site-packages/deepspeed/__init__.py\", line 25, in <module>\n",
      "    from . import ops\n",
      "  File \"/usr/local/lib/python3.11/site-packages/deepspeed/ops/__init__.py\", line 15, in <module>\n",
      "    from ..git_version_info import compatible_ops as __compatible_ops__\n",
      "  File \"/usr/local/lib/python3.11/site-packages/deepspeed/git_version_info.py\", line 29, in <module>\n",
      "    op_compatible = builder.is_compatible()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/deepspeed/ops/op_builder/fp_quantizer.py\", line 35, in is_compatible\n",
      "    sys_cuda_major, _ = installed_cuda_version()\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py\", line 48, in installed_cuda_version\n",
      "    import torch.utils.cpp_extension\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 304, in <module>\n",
      "    f'-Xs \"-device {_get_sycl_arch_list()}\"',\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 293, in _get_sycl_arch_list\n",
      "    arch_list = torch.xpu.get_arch_list()\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'torch.xpu' has no attribute 'get_arch_list'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'swanlab' has no attribute 'is_running'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 541\u001b[39m\n\u001b[32m    538\u001b[39m     traceback.print_exc()\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    540\u001b[39m     \u001b[38;5;66;03m# æ— è®ºç¨‹åºæ˜¯æˆåŠŸç»“æŸè¿˜æ˜¯ä¸­é€”å¤±è´¥,éƒ½ç¡®ä¿å…³é—­SwanLabçš„æ—¥å¿—è®°å½•\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mswanlab\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mswanlab\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_running\u001b[49m():\n\u001b[32m    542\u001b[39m         swanlab.finish()\n\u001b[32m    543\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸ”š ç¨‹åºç»“æŸ\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'swanlab' has no attribute 'is_running'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "åŒ»å­¦é—®ç­”æ¨¡å‹QLoRAé«˜æ•ˆå¾®è°ƒè„šæœ¬ (V3.0 ç»ˆææ–°æ‰‹æ³¨é‡Šç‰ˆ)\n",
    "\n",
    "ã€æœ¬è„šæœ¬çš„ä½¿å‘½ã€‘\n",
    "è¿™ä»½è„šæœ¬æ˜¯ä¸ºæ‰€æœ‰å¯¹å¤§æ¨¡å‹å¾®è°ƒæ„Ÿå…´è¶£çš„æ–°æ‰‹æœ‹å‹ä»¬é‡èº«æ‰“é€ çš„â€œä¿å§†çº§â€æ•™ç¨‹ã€‚\n",
    "æˆ‘ä»¬åšä¿¡,æœ€å¥½çš„å­¦ä¹ æ˜¯â€œåœ¨å®è·µä¸­å­¦ä¹ â€ã€‚å› æ­¤,æˆ‘ä»¬ä¸ä»…æä¾›äº†å®Œæ•´ã€å¯è¿è¡Œçš„ä»£ç ,\n",
    "æ›´åœ¨æ¯ä¸€ä¸ªå…³é”®ç¯èŠ‚éƒ½é™„ä¸Šäº†è¯¦å°½çš„ã€å£è¯­åŒ–çš„â€œæè‡´æ³¨é‡Šâ€ã€‚\n",
    "\n",
    "æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ï¼š\n",
    "1.  ã€çŸ¥å…¶ç„¶ã€‘: æ‚¨åªéœ€ä¿®æ”¹å°‘æ•°å‡ ä¸ªè·¯å¾„å’Œå‚æ•°,å°±èƒ½æˆåŠŸè¿è¡Œç¬¬ä¸€æ¬¡å¾®è°ƒã€‚\n",
    "2.  ã€çŸ¥å…¶æ‰€ä»¥ç„¶ã€‘: é€šè¿‡é˜…è¯»æ³¨é‡Š,æ‚¨èƒ½ç†è§£æ¯ä¸ªå‚æ•°çš„æ„ä¹‰ã€æ¯ä¸ªå‡½æ•°çš„ä½œç”¨,ä»¥åŠå®ƒä»¬å¯¹æœ€ç»ˆç»“æœçš„å½±å“ã€‚\n",
    "3.  ã€èµ‹äºˆæ‚¨è°ƒä¼˜çš„èƒ½åŠ›ã€‘: åœ¨ç†è§£çš„åŸºç¡€ä¸Š,æ‚¨å¯ä»¥è‡ªä¿¡åœ°è°ƒæ•´è¶…å‚æ•°,é’ˆå¯¹æ‚¨è‡ªå·±çš„ä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚\n",
    "\n",
    "ã€æ ¸å¿ƒç‰¹æ€§ã€‘\n",
    "1.  [ç¨³å®šå¯é ] å½»åº•è§£å†³äº†æ–°æ‰‹å¸¸é‡åˆ°çš„\"torch.xpu\"ç¯å¢ƒé”™è¯¯ã€‚\n",
    "2.  [åŠŸèƒ½å®Œæ•´] åŒ…å«äº†QLoRAã€Flash Attention 2ã€è‡ªåŠ¨åŒ–å®éªŒå‘½åç­‰æ‰€æœ‰å…ˆè¿›ç‰¹æ€§ã€‚\n",
    "3.  [æè‡´æ³¨é‡Š] å¯èƒ½æ˜¯æ‚¨èƒ½æ‰¾åˆ°çš„ã€æ³¨é‡Šæœ€è¯¦å°½çš„ä¸­æ–‡å¾®è°ƒè„šæœ¬ã€‚\n",
    "4.  [å®éªŒå¯è¿½æº¯] æ·±åº¦é›†æˆäº†SwanLab,è®©æ‚¨çš„æ¯ä¸€æ¬¡å°è¯•éƒ½æœ‰è¿¹å¯å¾ªã€‚\n",
    "\"\"\"\n",
    "\n",
    "# =====================================================================================\n",
    "# ã€ç¬¬é›¶æ­¥ï¼šç¯å¢ƒå‡†å¤‡ã€‘- åœ¨ä»£ç è¿è¡Œå‰,è®©è®¡ç®—æœºåšå¥½å‡†å¤‡\n",
    "# =====================================================================================\n",
    "import os\n",
    "\n",
    "# ã€ï¼ï¼ï¼æ ¸å¿ƒé‡ç‚¹ï¼ï¼ï¼ã€‘ç¯å¢ƒå˜é‡é…ç½® - å¿…é¡»åœ¨æ‰€æœ‰import PyTorchç­‰åº“ä¹‹å‰æ‰§è¡Œ\n",
    "# è¿™éƒ¨åˆ†ä»£ç å°±åƒåœ¨æ¼”å‡ºå¼€å§‹å‰å¸ƒç½®èˆå°,ç¡®ä¿æ‰€æœ‰æ¼”å‘˜ï¼ˆåº“ï¼‰éƒ½èƒ½åœ¨æ­£ç¡®çš„ç¯å¢ƒä¸‹å·¥ä½œã€‚\n",
    "\n",
    "# ã€GPUè®¾å¤‡é€‰æ‹©ã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šå‘Šè¯‰ç¨‹åºæˆ‘ä»¬æƒ³ä½¿ç”¨å“ªä¸€å—NVIDIAæ˜¾å¡ï¼ˆGPUï¼‰ã€‚è®¡ç®—æœºä¸­å¯èƒ½æœ‰å¤šå—æ˜¾å¡,ç¼–å·ä»0å¼€å§‹ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆæ˜¯\"0\"ï¼šé€šå¸¸,æˆ‘ä»¬ä½¿ç”¨æ€§èƒ½æœ€å¥½æˆ–è€…å½“å‰ç©ºé—²çš„ç¬¬ä¸€å—æ˜¾å¡ã€‚\n",
    "#   - æ”¹äº†ä¼šæ€æ ·ï¼šå¦‚æœæ‚¨æœ‰å¤šå—æ˜¾å¡,å¯ä»¥è®¾ç½®ä¸º\"0\", \"1\", \"2\"ç­‰,æˆ–è€…\"0,1\"æ¥åŒæ—¶ä½¿ç”¨ä¸¤å—ï¼ˆéœ€è¦ä¿®æ”¹ä»£ç ä»¥æ”¯æŒå¤šå¡è®­ç»ƒï¼‰ã€‚\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# ã€ï¼ï¼ï¼æ ¸å¿ƒä¿®å¤ï¼ï¼ï¼ã€‘è§£å†³ 'torch.xpu' é”™è¯¯çš„â€œé­”æ³•å¼€å…³â€\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šè¿™æ˜¯ä¸€ç»„ç¯å¢ƒå˜é‡,ç”¨æ¥é˜»æ­¢ç¨‹åºå»æ¢æµ‹å’Œä½¿ç”¨Intelçš„XPUï¼ˆä¸€ç§éNVIDIAçš„å¤„ç†å™¨ï¼‰ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆéœ€è¦ï¼šå¾ˆå¤šå¼€æºåº“ï¼ˆå¦‚transformers, accelerateï¼‰é»˜è®¤ä¼šæ£€æŸ¥æ‰€æœ‰ç±»å‹çš„ç¡¬ä»¶,\n",
    "#              ä½†åœ¨ä¸€ä¸ªåªæœ‰NVIDIA GPUçš„ç¯å¢ƒä¸‹,è¿™ç§æ£€æŸ¥å¯èƒ½ä¼šå› ä¸ºç¼ºå°‘Intelé©±åŠ¨è€ŒæŠ¥é”™ã€‚\n",
    "#   - è®¾ç½®äº†ä¼šæ€æ ·ï¼šè¿™ä¸¤è¡Œä»£ç åƒâ€œå‘Šç¤ºç‰Œâ€,å‘Šè¯‰æ‰€æœ‰åº“ï¼šâ€œæ­¤è·¯ä¸é€š,è¯·ç›´æ¥èµ°NVIDIA CUDAé€šé“â€,ä»è€Œé¿å…äº†é”™è¯¯ã€‚\n",
    "os.environ[\"DISABLE_IPEX\"] = \"1\"\n",
    "os.environ[\"DEEPSPEED_XPU_ENABLE\"] = \"0\"\n",
    "\n",
    "# ã€å†…å­˜åˆ†é…ä¼˜åŒ–ã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šå‘Šè¯‰PyTorchå¦‚ä½•ç®¡ç†GPUå†…å­˜ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆè®¾ç½®ï¼šå¯ä»¥å‡å°‘å†…å­˜ç¢ç‰‡,æœ‰æ—¶èƒ½é¿å…\"CUDA out of memory\"ï¼ˆæ˜¾å­˜ä¸è¶³ï¼‰çš„é”™è¯¯,è®©ç¨‹åºèƒ½æ›´çµæ´»åœ°ä½¿ç”¨æ˜¾å­˜ã€‚\n",
    "#   - ç±»æ¯”ï¼šå°±åƒä¸€ä¸ªèªæ˜çš„è¡Œææ‰“åŒ…å‘˜,å®ƒä¼šå°½é‡æŠŠå¤§å°ä¸ä¸€çš„è¡Œæï¼ˆæ•°æ®ï¼‰ç´§å‡‘åœ°å¡è¿›åå¤‡ç®±ï¼ˆæ˜¾å­˜ï¼‰,è€Œä¸æ˜¯éšæ„ä¹±æ”¾å¯¼è‡´ç©ºé—´æµªè´¹ã€‚\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# ã€ç¬¬ä¸€æ­¥ï¼šå¯¼å…¥å·¥å…·åŒ…ã€‘ - å‡†å¤‡å¥½æˆ‘ä»¬éœ€è¦çš„â€œç‘å£«å†›åˆ€â€\n",
    "# =====================================================================================\n",
    "# å¯¼å…¥æˆ‘ä»¬å®Œæˆä»»åŠ¡æ‰€éœ€è¦çš„æ‰€æœ‰Pythonåº“\n",
    "\n",
    "import json                 # ç”¨äºè¯»å†™JSONæ ¼å¼çš„æ•°æ®æ–‡ä»¶ã€‚\n",
    "import pandas as pd         # ä¸€ä¸ªå¼ºå¤§çš„æ•°æ®åˆ†æå·¥å…·,èƒ½æ–¹ä¾¿åœ°è¯»å–å’Œå¤„ç†æˆ‘ä»¬çš„æ•°æ®é›†ã€‚\n",
    "import torch                # PyTorch,æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæ¡†æ¶,æ‰€æœ‰æ¨¡å‹è®­ç»ƒå’Œè®¡ç®—éƒ½åŸºäºå®ƒã€‚\n",
    "from datasets import Dataset # Hugging Faceçš„datasetsåº“,ç”¨äºé«˜æ•ˆåœ°åŠ è½½å’Œå¤„ç†å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®ã€‚\n",
    "from modelscope import snapshot_download # ModelScopeçš„å·¥å…·,ç”¨äºä»å›½å†…é•œåƒæºä¸‹è½½æ¨¡å‹,é€Ÿåº¦æ›´å¿«ã€‚\n",
    "\n",
    "# ã€Transformersåº“ã€‘ - è¿™æ˜¯ä¸å¤§è¯­è¨€æ¨¡å‹äº¤äº’çš„æ ¸å¿ƒåº“\n",
    "from transformers import (\n",
    "    AutoTokenizer,          # è‡ªåŠ¨åˆ†è¯å™¨,èƒ½å°†æ–‡æœ¬è½¬æ¢æˆæ¨¡å‹èƒ½ç†è§£çš„æ•°å­—ï¼ˆTokenï¼‰ã€‚\n",
    "    AutoModelForCausalLM,   # è‡ªåŠ¨æ¨¡å‹åŠ è½½å™¨,ç”¨äºåŠ è½½åƒQwenè¿™æ ·çš„ç”Ÿæˆå¼å¤§æ¨¡å‹ã€‚\n",
    "    TrainingArguments,      # è®­ç»ƒå‚æ•°é…ç½®å™¨,ä¸€ä¸ªåŒ…å«äº†æ‰€æœ‰è®­ç»ƒé€‰é¡¹çš„â€œæ§åˆ¶é¢æ¿â€ã€‚\n",
    "    Trainer,                # è®­ç»ƒå™¨,å°è£…äº†å¤æ‚çš„è®­ç»ƒå¾ªç¯,è®©æˆ‘ä»¬ç”¨å‡ è¡Œä»£ç å°±èƒ½å¼€å§‹è®­ç»ƒã€‚\n",
    "    DataCollatorForSeq2Seq, # æ•°æ®æ•´ç†å™¨,èƒ½å°†å¤šæ¡é•¿çŸ­ä¸ä¸€çš„æ•°æ®æ™ºèƒ½åœ°æ‰“åŒ…æˆä¸€ä¸ªæ‰¹æ¬¡ï¼ˆBatchï¼‰ã€‚\n",
    "    BitsAndBytesConfig      # QLoRAçš„é‡åŒ–é…ç½®å™¨,ç”¨æ¥è®¾ç½®å¦‚ä½•å°†æ¨¡å‹ä»32ä½å‹ç¼©åˆ°4ä½ã€‚\n",
    ")\n",
    "import swanlab              # ä¸€ä¸ªå®éªŒè·Ÿè¸ªå·¥å…·,èƒ½åƒâ€œé»‘åŒ£å­â€ä¸€æ ·è®°å½•æˆ‘ä»¬è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ‰€æœ‰æŒ‡æ ‡ã€‚\n",
    "\n",
    "# ã€PEFTåº“ã€‘ - å®ç°é«˜æ•ˆå¾®è°ƒçš„â€œé­”æ³•â€æ‰€åœ¨\n",
    "# Parameter-Efficient Fine-Tuning (å‚æ•°é«˜æ•ˆå¾®è°ƒ)\n",
    "from peft import (\n",
    "    LoraConfig,             # LoRAçš„é…ç½®å™¨,ç”¨æ¥å®šä¹‰â€œå°æ¨¡å‹â€ï¼ˆé€‚é…å™¨ï¼‰çš„ç»“æ„ã€‚\n",
    "    get_peft_model,         # å°†LoRAé€‚é…å™¨â€œå®‰è£…â€åˆ°å¤§æ¨¡å‹ä¸Šçš„å·¥å…·å‡½æ•°ã€‚\n",
    "    prepare_model_for_kbit_training, # ä¸ºk-bitï¼ˆå¦‚4-bitï¼‰é‡åŒ–åçš„æ¨¡å‹åšè®­ç»ƒå‰çš„å‡†å¤‡å·¥ä½œã€‚\n",
    "    PeftModel               # ç”¨äºåœ¨æ¨ç†æ—¶åŠ è½½è®­ç»ƒå¥½çš„LoRAé€‚é…å™¨ã€‚\n",
    ")\n",
    "\n",
    "\n",
    "def check_environment():\n",
    "    \"\"\"ç¯å¢ƒæ£€æŸ¥å‡½æ•°,ç¡®ä¿CUDAï¼ˆNVIDIA GPUçš„è®¡ç®—å¹³å°ï¼‰å¯ç”¨ã€‚\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"âŒ CUDAä¸å¯ç”¨ï¼æ­¤è„šæœ¬éœ€è¦NVIDIA GPUæ¥è¿è¡ŒQLoRAå¾®è°ƒã€‚\")\n",
    "    print(f\"âœ… CUDAç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼\")\n",
    "    print(f\"ğŸ“Š å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}\")\n",
    "    print(f\"ğŸ”§ å½“å‰CUDAè®¾å¤‡: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ğŸ’¾ GPUæ€»æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    # æ¸…ç©ºä¸€ä¸‹æ˜¾å­˜,ä»¥é˜²ä¹‹å‰æœ‰å…¶ä»–ç¨‹åºå ç”¨äº†æ˜¾å­˜ã€‚\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# =====================================================================================\n",
    "# ã€ç¬¬äºŒæ­¥ï¼šå…¨å±€é…ç½®ã€‘ - æ­å»ºæˆ‘ä»¬å¾®è°ƒå®éªŒçš„â€œæ§åˆ¶ä¸­å¿ƒâ€\n",
    "# åœ¨è¿™é‡Œ,æ‚¨å¯ä»¥åƒå¯¼æ¼”ä¸€æ ·,è®¾å®šæ•´ä¸ªå¾®è°ƒå‰§æœ¬çš„æ ¸å¿ƒè¦ç´ ã€‚\n",
    "# =====================================================================================\n",
    "\n",
    "# --- è·¯å¾„é…ç½® (Path Configuration) ---\n",
    "# è‰¯å¥½çš„è·¯å¾„ç®¡ç†æ˜¯é¡¹ç›®æˆåŠŸçš„å¼€å§‹,è®©æ‰€æœ‰æ–‡ä»¶å„å¾—å…¶æ‰€ã€‚\n",
    "BASE_DIR = \".\"  # é¡¹ç›®æ ¹ç›®å½•,\".\"ä»£è¡¨å½“å‰æ–‡ä»¶å¤¹\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")                       # å­˜æ”¾åŸå§‹æ•°æ®çš„æ–‡ä»¶å¤¹\n",
    "FORMATTED_DATA_DIR = os.path.join(BASE_DIR, \"data_formatted\")   # å­˜æ”¾å¤„ç†åæ•°æ®çš„æ–‡ä»¶å¤¹\n",
    "MODEL_CACHE_DIR = os.path.join(BASE_DIR, \"model_cache\")         # å­˜æ”¾ä¸‹è½½çš„åŸºç¡€æ¨¡å‹çš„æ–‡ä»¶å¤¹,é¿å…é‡å¤ä¸‹è½½\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"output\")                   # å­˜æ”¾æ‰€æœ‰è®­ç»ƒè¾“å‡ºçš„æ ¹æ–‡ä»¶å¤¹\n",
    "CHECKPOINTS_DIR = os.path.join(OUTPUT_DIR, \"checkpoints\")       # å­˜æ”¾è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ£€æŸ¥ç‚¹ï¼ˆä¸´æ—¶å­˜æ¡£ï¼‰\n",
    "FINAL_ADAPTER_DIR = os.path.join(OUTPUT_DIR, \"final_adapter\")   # å­˜æ”¾æœ€ç»ˆè®­ç»ƒå¥½çš„ã€å¯ä¾›åˆ†äº«çš„LoRAé€‚é…å™¨\n",
    "\n",
    "# --- æ¨¡å‹ä¸æç¤ºè¯é…ç½® (Model and Prompt Configuration) ---\n",
    "# ã€æ¨¡å‹ID MODEL_IDã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šæ‚¨é€‰æ‹©çš„åŸºç¡€å¤§æ¨¡å‹çš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚å¯ä»¥æ¥è‡ªHugging Face Hubæˆ–ModelScopeã€‚\n",
    "#   - ä¸ºä»€ä¹ˆæ˜¯\"Qwen/Qwen3-1.7B\"ï¼šè¿™æ˜¯ä¸€ä¸ªä¼˜ç§€çš„ä¸­è‹±æ–‡å¼€æºæ¨¡å‹,å‚æ•°é‡è¾ƒå°ï¼ˆ17äº¿ï¼‰,é€‚åˆåœ¨æ¶ˆè´¹çº§æ˜¾å¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚\n",
    "#   - æ”¹äº†ä¼šæ€æ ·ï¼šæ‚¨å¯ä»¥æ¢æˆä»»ä½•æ‚¨æƒ³å¾®è°ƒçš„æ¨¡å‹ID,æ¯”å¦‚\"Qwen/Qwen3-7B\", \"meta-llama/Llama-3-8b\"ç­‰,ä½†è¯·æ³¨æ„æ¨¡å‹è¶Šå¤§,æ˜¾å­˜æ¶ˆè€—ä¹Ÿè¶Šå¤§ã€‚\n",
    "MODEL_ID = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "# ã€ï¼ï¼ï¼æ ¸å¿ƒé‡ç‚¹ï¼ï¼ï¼ã€‘ç³»ç»Ÿæç¤ºè¯ (SYSTEM_PROMPT)\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šç»™æ¨¡å‹è®¾å®šçš„â€œäººè®¾â€æˆ–â€œè¡Œä¸ºå‡†åˆ™â€,å‘Šè¯‰å®ƒåº”è¯¥æ‰®æ¼”ä»€ä¹ˆè§’è‰²,éµå¾ªå“ªäº›è§„åˆ™ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆé‡è¦ï¼šè¿™é‡Œçš„PROMPTå¿…é¡»ä¸æ‚¨æœªæ¥è¿›è¡Œæ¨ç†ã€è¯„ä¼°ã€éƒ¨ç½²æ—¶ä½¿ç”¨çš„SYSTEM_PROMPTå®Œå…¨ä¸€è‡´ï¼\n",
    "#   - ç±»æ¯”ï¼šè¿™ç›¸å½“äºåœ¨åŸ¹è®­å‘˜å·¥ï¼ˆå¾®è°ƒï¼‰å’Œè€ƒæ ¸å‘˜å·¥ï¼ˆæ¨ç†ï¼‰æ—¶,ä½¿ç”¨çš„æ˜¯åŒä¸€ä»½å²—ä½è¯´æ˜ä¹¦ã€‚å¦‚æœè¯´æ˜ä¹¦ä¸ä¸€è‡´,å‘˜å·¥ï¼ˆæ¨¡å‹ï¼‰å°±ä¼šæ„Ÿåˆ°å›°æƒ‘,è¡¨ç°è‡ªç„¶ä¼šæ‰“æŠ˜æ‰£ã€‚\n",
    "SYSTEM_PROMPT = \"\"\"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„AIåŒ»å­¦åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·æå‡ºçš„é—®é¢˜,æä¾›å‡†ç¡®ã€æ˜“æ‡‚ä¸”å…·æœ‰å®‰å…¨æç¤ºçš„å¥åº·ä¿¡æ¯ã€‚è¯·è®°ä½,ä½ çš„å›ç­”ä¸èƒ½æ›¿ä»£æ‰§-ä¸šåŒ»å¸ˆçš„è¯Šæ–­,å¿…é¡»åœ¨å›ç­”ç»“å°¾å¤„å£°æ˜è¿™ä¸€ç‚¹ã€‚\"\"\"\n",
    "\n",
    "# ã€æœ€å¤§åºåˆ—é•¿åº¦ MAX_LENGTHã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šä¸€æ¡è®­ç»ƒæ•°æ®ï¼ˆé—®é¢˜+ç­”æ¡ˆï¼‰è¢«è½¬æ¢æˆæ•°å­—ï¼ˆTokenï¼‰å,å…è®¸çš„æœ€å¤§é•¿åº¦ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆæ˜¯2048ï¼šå¯¹äºå¤§å¤šæ•°é—®ç­”ä»»åŠ¡,è¿™æ˜¯ä¸€ä¸ªæ¯”è¾ƒå‡è¡¡çš„å€¼ã€‚å®ƒèƒ½å®¹çº³è¶³å¤Ÿé•¿çš„ä¸Šä¸‹æ–‡,åŒæ—¶æ˜¾å­˜æ¶ˆè€—ä¹Ÿåœ¨å¯æ¥å—èŒƒå›´å†…ã€‚\n",
    "#   - æ”¹äº†ä¼šæ€æ ·ï¼š\n",
    "#     - å¢åŠ ï¼ˆå¦‚4096ï¼‰ï¼šå¯ä»¥å¤„ç†æ›´é•¿çš„é—®ç­”å¯¹,ä½†ä¼šæ˜¾è‘—å¢åŠ æ˜¾å­˜æ¶ˆè€—ï¼ˆæ˜¾å­˜å ç”¨ä¸é•¿åº¦çš„å¹³æ–¹æˆæ­£æ¯”ï¼ï¼‰ã€‚\n",
    "#     - å‡å°‘ï¼ˆå¦‚1024ï¼‰ï¼šèŠ‚çœæ˜¾å­˜,è®­ç»ƒæ›´å¿«,ä½†å¦‚æœæ‚¨çš„æ•°æ®æ™®éè¾ƒé•¿,å¯èƒ½ä¼šå› æˆªæ–­è€Œä¸¢å¤±é‡è¦ä¿¡æ¯,å½±å“æ¨¡å‹å­¦ä¹ æ•ˆæœã€‚\n",
    "\n",
    "MAX_LENGTH = 2048\n",
    "\n",
    "# --- ã€ï¼ï¼ï¼æ ¸å¿ƒé‡ç‚¹ï¼ï¼ï¼ã€‘è®­ç»ƒè¶…å‚æ•° (Hyperparameters) ---\n",
    "# è¿™éƒ¨åˆ†æ˜¯å¾®è°ƒçš„â€œçµé­‚â€,æ˜¯æ‚¨å¯ä»¥è°ƒèŠ‚æ¥æå‡æ¨¡å‹æ•ˆæœçš„å…³é”®â€œæ—‹é’®â€ã€‚\n",
    "\n",
    "# ã€å­¦ä¹ ç‡ Learning Rateã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šå¯ä»¥ç†è§£ä¸ºæ¨¡å‹åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­â€œæ›´æ–°çŸ¥è¯†â€çš„æ­¥ä¼å¤§å°ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆæ˜¯1e-4ï¼šå¯¹äºåªè®­ç»ƒâ€œå°æœ¬å­â€ï¼ˆLoRAé€‚é…å™¨ï¼‰çš„QLoRAæ–¹æ³•,æˆ‘ä»¬å¯ä»¥è®©å®ƒå­¦å¾—å¿«ä¸€ç‚¹ã€‚1e-4æ˜¯ä¸€ä¸ªç»è¿‡å¤§é‡å®éªŒéªŒè¯çš„ã€å¯¹QLoRAæ¥è¯´æ•ˆæœå¾ˆå¥½çš„èµ·å§‹å€¼ã€‚ç›¸æ¯”ä¹‹ä¸‹,å…¨é‡å¾®è°ƒï¼ˆè®­ç»ƒæ•´ä¸ªæ¨¡å‹ï¼‰é€šå¸¸ä½¿ç”¨æ›´å°çš„å€¼ï¼ˆå¦‚2e-5ï¼‰,å› ä¸ºæ­¥ä¼å¤ªå¤§å®¹æ˜“â€œè·‘åâ€ã€‚\n",
    "#   - æ”¹äº†ä¼šæ€æ ·ï¼š\n",
    "#     - å¤ªé«˜ï¼ˆå¦‚1e-3ï¼‰ï¼šåƒæ˜¯ä¸€ä¸ªå­¦ç”Ÿçœ‹ä¹¦ä¸€ç›®åè¡Œ,å¯èƒ½ä»€ä¹ˆéƒ½æ²¡å­¦è¿›å»,å¯¼è‡´è®­ç»ƒä¸ç¨³å®š,æŸå¤±ï¼ˆlossï¼‰ä¸ä¸‹é™ç”šè‡³ä¸Šå‡ã€‚\n",
    "#     - å¤ªä½ï¼ˆå¦‚1e-5ï¼‰ï¼šåƒæ˜¯ä¸€ä¸ªå­¦ç”Ÿé€å­—é€å¥åœ°é’»ç‰›è§’å°–,å­¦ä¹ é€Ÿåº¦ä¼šéå¸¸æ…¢,å¯èƒ½è®­ç»ƒå¾ˆä¹…æ•ˆæœéƒ½æå‡ä¸æ˜æ˜¾ã€‚\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# ã€å•è®¾å¤‡æ‰¹é‡å¤§å° PER_DEVICE_TRAIN_BATCH_SIZEã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šä¸€æ¬¡æ€§å–‚ç»™GPUå¤šå°‘æ¡æ•°æ®è¿›è¡Œå­¦ä¹ ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆæ˜¯8ï¼šè¿™æ˜¯ä¸€ä¸ªåœ¨24GBæ˜¾å­˜ä¸‹æ¯”è¾ƒå‡è¡¡çš„å€¼ã€‚å€¼è¶Šå°,å•æ¬¡è®¡ç®—çš„æ˜¾å­˜å ç”¨è¶Šä½ã€‚\n",
    "#   - æ”¹äº†ä¼šæ€æ ·ï¼š\n",
    "#     - å¢å¤§ï¼šè®­ç»ƒé€Ÿåº¦ä¼šåŠ å¿«,ä½†æ˜¾å­˜å ç”¨ä¼šæ€¥å‰§å¢åŠ ã€‚å¦‚æœè®¾ç½®è¿‡å¤§,ä¼šç›´æ¥å¯¼è‡´â€œCUDA out of memoryâ€é”™è¯¯ã€‚\n",
    "#     - å‡å°ï¼šèƒ½æœ‰æ•ˆé™ä½æ˜¾å­˜å ç”¨,æ˜¯è§£å†³æ˜¾å­˜ä¸è¶³é—®é¢˜çš„é¦–é€‰æ–¹æ¡ˆã€‚ä½†è®­ç»ƒé€Ÿåº¦ä¼šå˜æ…¢ã€‚\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 8\n",
    "\n",
    "# ã€æ¢¯åº¦ç´¯ç§¯æ­¥æ•° GRADIENT_ACCUMULATION_STEPSã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šä¸€ç§â€œç”¨æ—¶é—´æ¢ç©ºé—´â€çš„æŠ€å·§,åœ¨ä¸å¢åŠ æ˜¾å­˜çš„æƒ…å†µä¸‹,å®ç°ç­‰æ•ˆäºæ›´å¤§Batch Sizeçš„è®­ç»ƒæ•ˆæœã€‚\n",
    "#   - ä¸ºä»€ä¹ˆæ˜¯8ï¼šé…åˆä¸Šé¢çš„`BATCH_SIZE = 8`,æˆ‘ä»¬çš„â€œæœ‰æ•ˆæ‰¹é‡å¤§å°â€(Effective Batch Size) = 8 * 8 = 64ã€‚è¿™é€šå¸¸æ˜¯ä¸€ä¸ªèƒ½è®©æ¨¡å‹ç¨³å®šå­¦ä¹ çš„æ‰¹é‡å¤§å°ã€‚\n",
    "#   - å·¥ä½œåŸç†ï¼šç¨‹åºä¼šå…ˆè®¡ç®—8æ¬¡å°æ‰¹é‡ï¼ˆæ¯æ¬¡8æ¡æ•°æ®ï¼‰çš„æ¢¯åº¦,ä½†å…ˆä¸æ›´æ–°æ¨¡å‹,è€Œæ˜¯æŠŠè¿™8æ¬¡çš„æ¢¯åº¦â€œç´¯ç§¯â€èµ·æ¥,æœ€åç”¨è¿™ä¸ªç´¯ç§¯çš„æ¢¯åº¦å®Œæˆä¸€æ¬¡æ¨¡å‹æ›´æ–°ã€‚è¿™åœ¨æ•°å­¦æ•ˆæœä¸Šç­‰åŒäºä¸€æ¬¡æ€§å¤„ç†64æ¡æ•°æ®ã€‚\n",
    "#   - æ”¹äº†ä¼šæ€æ ·ï¼šå®ƒå’Œ`BATCH_SIZE`æ˜¯è··è··æ¿å…³ç³»ã€‚å½“æ˜¾å­˜ä¸è¶³æ—¶,æ‚¨å¯ä»¥é™ä½`BATCH_SIZE`ï¼ˆæ¯”å¦‚é™åˆ°4ï¼‰,åŒæ—¶æé«˜æ­¤å€¼ï¼ˆæ¯”å¦‚æé«˜åˆ°16ï¼‰,ä»¥ä¿æŒâ€œæœ‰æ•ˆæ‰¹é‡å¤§å°â€ä¸å˜ï¼ˆ4 * 16 = 64ï¼‰,è¿™æ ·è®­ç»ƒæ•ˆæœåŸºæœ¬ä¸å—å½±å“ã€‚\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "\n",
    "# ã€è®­ç»ƒè½®æ•° NUM_TRAIN_EPOCHSã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šå°†æ•´ä¸ªè®­ç»ƒæ•°æ®é›†ä»å¤´åˆ°å°¾å®Œæ•´åœ°å­¦ä¹ å¤šå°‘éã€‚\n",
    "#   - ä¸ºä»€ä¹ˆæ˜¯2ï¼šå¯¹äºå¾®è°ƒä»»åŠ¡,å°¤å…¶æ˜¯åœ¨é«˜è´¨é‡çš„æ•°æ®é›†ä¸Š,é€šå¸¸1-3ä¸ªepochå°±è¶³å¤Ÿäº†ã€‚è¿™å¯ä»¥åœ¨è¾ƒçŸ­æ—¶é—´å†…çœ‹åˆ°æ˜æ˜¾æ•ˆæœ,åŒæ—¶é¿å…æ¨¡å‹â€œæ­»è®°ç¡¬èƒŒâ€ã€‚\n",
    "#   - æ”¹äº†ä¼šæ€æ ·ï¼š\n",
    "#     - å¤ªå°‘ï¼ˆå¦‚1ï¼‰ï¼šå¯èƒ½å¯¼è‡´æ¨¡å‹â€œæ²¡å­¦é€â€,æ•ˆæœæå‡ä¸æ˜æ˜¾ã€‚\n",
    "#     - å¤ªå¤šï¼ˆå¦‚5+ï¼‰ï¼šå¯èƒ½å‘ç”Ÿâ€œè¿‡æ‹Ÿåˆâ€ã€‚å³æ¨¡å‹åªä¼šæ­»è®°ç¡¬èƒŒè®­ç»ƒæ•°æ®,ä¸§å¤±äº†åœ¨æ–°é—®é¢˜ä¸Šçš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›,å°±åƒä¸€ä¸ªåªä¼šåšç»ƒä¹ å†Œä½†ä¸€åˆ°è€ƒåœºå°±è’™åœˆçš„å­¦ç”Ÿã€‚\n",
    "NUM_TRAIN_EPOCHS = 2\n",
    "\n",
    "# --- è®­ç»ƒè¿‡ç¨‹æ§åˆ¶å‚æ•° ---\n",
    "EVAL_STEPS = 100        # æ¯è®­ç»ƒ100æ­¥,å°±ç”¨éªŒè¯é›†è¯„ä¼°ä¸€æ¬¡æ¨¡å‹,åƒæ˜¯ä¸€æ¬¡æ¨¡æ‹Ÿè€ƒ,å¸®åŠ©æˆ‘ä»¬ç›‘æ§å­¦ä¹ è¿›åº¦ã€‚\n",
    "SAVE_STEPS = 400        # æ¯è®­ç»ƒ400æ­¥,å°±ä¿å­˜ä¸€æ¬¡æ¨¡å‹æ£€æŸ¥ç‚¹,é˜²æ­¢ç”µè„‘çªç„¶æ–­ç”µå¯¼è‡´è®­ç»ƒæˆæœç™½è´¹ã€‚\n",
    "LOGGING_STEPS = 10      # æ¯è®­ç»ƒ10æ­¥,å°±åœ¨æ§åˆ¶å°æ‰“å°ä¸€æ¬¡å½“å‰çš„è®­ç»ƒæŸå¤±ï¼ˆlossï¼‰,è®©æˆ‘ä»¬èƒ½å®æ—¶çœ‹åˆ°å­¦ä¹ çŠ¶æ€ã€‚\n",
    "\n",
    "# --- ã€ï¼ï¼ï¼æ ¸å¿ƒé‡ç‚¹ï¼ï¼ï¼ã€‘LoRAé…ç½® (LoRA Configuration) ---\n",
    "# LoRAæ˜¯é«˜æ•ˆå¾®è°ƒçš„é­”æ³•ã€‚å¯ä»¥æŠŠå®ƒæƒ³è±¡æˆæˆ‘ä»¬ä¸æ”¹å˜ä¸€ä¸ªåšå­¦çš„æ•™æˆï¼ˆåŸºç¡€å¤§æ¨¡å‹ï¼‰,\n",
    "# è€Œæ˜¯ç»™ä»–é…ä¸€ä¸ªâ€œä¸“ä¸šé¢†åŸŸç¬”è®°æœ¬â€ï¼ˆLoRAé€‚é…å™¨ï¼‰,æˆ‘ä»¬åªè®­ç»ƒè¿™ä¸ªç¬”è®°æœ¬,è®©æ•™æˆå­¦ä¼šæ–°çŸ¥è¯†ã€‚\n",
    "\n",
    "# ã€LoRAç§© R - ç¬”è®°æœ¬çš„â€œåšåº¦â€ã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šLoRAé€‚é…å™¨çŸ©é˜µçš„ç§©,å†³å®šäº†è¿™ä¸ªâ€œç¬”è®°æœ¬â€èƒ½è®°å½•å¤šå°‘æ–°çŸ¥è¯†ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆæ˜¯32ï¼šRå€¼é€šå¸¸åœ¨8, 16, 32, 64ä¸­é€‰æ‹©ã€‚32æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å¹³è¡¡ç‚¹,åœ¨æ€§èƒ½å’Œå‚æ•°é‡ä¹‹é—´å–å¾—äº†å¾ˆå¥½çš„å¹³è¡¡ã€‚\n",
    "#   - æ”¹äº†ä¼šæ€æ ·ï¼š\n",
    "#     - å¢å¤§ï¼ˆå¦‚64, 128ï¼‰ï¼šç¬”è®°æœ¬æ›´â€œåšâ€,å¯è®­ç»ƒçš„å‚æ•°æ›´å¤š,ç†è®ºä¸Šæ‹Ÿåˆèƒ½åŠ›æ›´å¼º,ä½†æ˜¾å­˜å ç”¨ä¹Ÿæ›´å¤§,ä¸”è¿‡å¤§ä¹Ÿå¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆã€‚\n",
    "#     - å‡å°ï¼ˆå¦‚16, 8ï¼‰ï¼šç¬”è®°æœ¬æ›´â€œè–„â€,è®­ç»ƒæ›´å¿«,æ˜¾å­˜æ›´çœ,ä½†å¯¹äºå¤æ‚ä»»åŠ¡å¯èƒ½â€œè®°ä¸ä½â€é‚£ä¹ˆå¤šçŸ¥è¯†,æ•ˆæœç¨å·®ã€‚\n",
    "LORA_R = 32\n",
    "\n",
    "# ã€LoRAç¼©æ”¾å› å­ Alpha - ç¬”è®°æœ¬å†…å®¹çš„â€œé‡è¦æ€§â€ã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šä¸€ä¸ªç¼©æ”¾å‚æ•°,ç”¨æ¥è°ƒæ•´LoRAé€‚é…å™¨ï¼ˆç¬”è®°æœ¬ï¼‰å¯¹æœ€ç»ˆç»“æœçš„å½±å“æƒé‡ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆæ˜¯64ï¼šä¸€ä¸ªå¹¿ä¸ºæµä¼ ä¸”æ•ˆæœå¾ˆå¥½çš„ç»éªŒæ³•åˆ™æ˜¯å°†alphaè®¾ç½®ä¸ºrçš„ä¸¤å€ (`alpha = 2 * r`)ã€‚\n",
    "#   - æ”¹äº†ä¼šæ€æ ·ï¼šå®ƒä¸å­¦ä¹ ç‡æœ‰ç±»ä¼¼çš„ä½œç”¨,è°ƒæ•´å®ƒä¼šå½±å“LoRAæ¨¡å—çš„æƒé‡å¤§å°ã€‚ä¿æŒ`2*r`çš„æ¯”ä¾‹é€šå¸¸æ˜¯æœ€ä½³å®è·µ,æ–°æ‰‹å¯ä»¥ä¸ç”¨ä¿®æ”¹ã€‚\n",
    "LORA_ALPHA = 64\n",
    "\n",
    "# ã€LoRA Dropout - é˜²æ­¢â€œæ­»è®°ç¡¬èƒŒâ€çš„é—å¿˜æœºåˆ¶ã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šåœ¨è®­ç»ƒæ—¶,éšæœºåœ°â€œä¸¢å¼ƒâ€ä¸€éƒ¨åˆ†LoRAå‚æ•°,ä¸è®©å®ƒä»¬å‚ä¸å½“æ¬¡è®¡ç®—ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆæ˜¯0.05ï¼šè¿™æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„å€¼ã€‚å®ƒå¯ä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚\n",
    "#   - ç±»æ¯”ï¼šå°±åƒæ˜¯æ•…æ„è®©å­¦ç”Ÿåœ¨å¤ä¹ æ—¶éšæœºå¿˜æ‰5%çš„çŸ¥è¯†ç‚¹,å¼ºè¿«ä»–å»ç†è§£çŸ¥è¯†æœ¬èº«,è€Œä¸æ˜¯æ­»è®°ç¡¬èƒŒç­”æ¡ˆã€‚\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# ã€ç›®æ ‡æ¨¡å— TARGET_MODULES - ç¬”è®°åº”è¯¥â€œè´´â€åœ¨å“ªé‡Œã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šå‘Šè¯‰LoRAåº”è¯¥åœ¨æ¨¡å‹çš„å“ªäº›éƒ¨åˆ†â€œåŠ è£…â€é€‚é…å™¨ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆæ˜¯è¿™å‡ ä¸ªï¼š`q_proj`, `k_proj`, `v_proj`, `o_proj`æ˜¯Transformeræ¨¡å‹ä¸­â€œæ³¨æ„åŠ›æœºåˆ¶â€çš„å››ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚æ³¨æ„åŠ›æœºåˆ¶å†³å®šäº†æ¨¡å‹å¦‚ä½•ç†è§£æ–‡æœ¬çš„ä¸Šä¸‹æ–‡å…³ç³»,å¯¹å®ƒä»¬è¿›è¡Œå¾®è°ƒé€šå¸¸æ•ˆæœæœ€æ˜¾è‘—ã€‚\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# ã€ç¬¬ä¸‰æ­¥ï¼šåŠ¨æ€å‘½åä¸å®éªŒè·Ÿè¸ªã€‘ - è®©æˆ‘ä»¬çš„å®éªŒäº•äº•æœ‰æ¡\n",
    "# =====================================================================================\n",
    "\n",
    "# --- åŠ¨æ€å‘½åé…ç½® (Dynamic Naming Configuration) ---\n",
    "# ã€åŸç†ã€‘ä»\"Qwen/Qwen3-1.7B\"è¿™æ ·çš„æ¨¡å‹IDä¸­,è‡ªåŠ¨æå–å‡º\"Qwen3-1.7B\"ä½œä¸ºåŸºç¡€åç§°ã€‚\n",
    "# ã€ä¼˜åŠ¿ã€‘å½“æ‚¨æ›´æ¢MODEL_IDæ—¶,æ‰€æœ‰ç›¸å…³çš„é¡¹ç›®åã€è¿è¡Œåå’Œæ–‡ä»¶å¤¹åéƒ½ä¼šè‡ªåŠ¨æ›´æ–°,æ— éœ€æ‰‹åŠ¨ä¿®æ”¹,\n",
    "#         è¿™ä½¿å¾—å®éªŒç®¡ç†æ›´åŠ æ¸…æ™°ã€è‡ªåŠ¨åŒ–,é¿å…äº†æ‰‹åŠ¨æ”¹åå¸¦æ¥çš„ç–å¿½å’Œé”™è¯¯ã€‚\n",
    "model_short_name = MODEL_ID.split('/')[-1]\n",
    "\n",
    "# --- SwanLabå®éªŒè·Ÿè¸ªé…ç½® ---\n",
    "# ã€åŠ¨æ€é¡¹ç›®åã€‘é¡¹ç›®åç°åœ¨ç”±æ¨¡å‹åŸºç¡€åå’Œä»»åŠ¡ç±»å‹ï¼ˆmedical-qloraï¼‰åŠ¨æ€æ„æˆã€‚\n",
    "#   - è¿™ä½¿å¾—åœ¨SwanLabä¸­,æ‰€æœ‰åŸºäºåŒä¸€ä¸ªåŸºç¡€æ¨¡å‹çš„å®éªŒéƒ½ä¼šè¢«è‡ªåŠ¨å½’ç±»åˆ°åŒä¸€ä¸ªé¡¹ç›®ä¸‹ã€‚\n",
    "os.environ[\"SWANLAB_PROJECT\"] = f\"{model_short_name}-medical-qlora\"\n",
    "\n",
    "EFFECTIVE_BATCH_SIZE = PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "# ã€åŠ¨æ€è¿è¡Œåã€‘ä¸ºæ¯æ¬¡è®­ç»ƒç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„ã€åŒ…å«æ ¸å¿ƒè¶…å‚æ•°çš„åç§°,ä¾¿äºåœ¨SwanLabä¸­å¯¹æ¯”ä¸åŒå®éªŒã€‚\n",
    "#   - ä¾‹å¦‚ï¼š\"Qwen3-1.7B-qlora-lr1e-4-bs64-r32\"\n",
    "#   - çœ‹åˆ°è¿™ä¸ªåå­—,æ‚¨å°±èƒ½ç«‹åˆ»çŸ¥é“è¿™æ¬¡å®éªŒæ˜¯ç”¨å“ªä¸ªæ¨¡å‹,å­¦ä¹ ç‡ã€æ‰¹é‡ã€LoRAç§©å„æ˜¯å¤šå°‘ã€‚\n",
    "RUN_NAME = f\"{model_short_name}-qlora-lr{LEARNING_RATE}-bs{EFFECTIVE_BATCH_SIZE}-r{LORA_R}\"\n",
    "\n",
    "# å°†æ‰€æœ‰è¶…å‚æ•°è®°å½•åˆ°SwanLabä¸­,æ–¹ä¾¿å›æº¯å’Œåˆ†æã€‚\n",
    "swanlab.config.update({\n",
    "    \"model\": MODEL_ID,\n",
    "    \"model_short_name\": model_short_name,\n",
    "    \"system_prompt\": SYSTEM_PROMPT,\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"epochs\": NUM_TRAIN_EPOCHS,\n",
    "    \"effective_batch_size\": EFFECTIVE_BATCH_SIZE,\n",
    "    \"lora_r\": LORA_R,\n",
    "    \"lora_alpha\": LORA_ALPHA,\n",
    "    \"lora_dropout\": LORA_DROPOUT,\n",
    "})\n",
    "\n",
    "# =====================================================================================\n",
    "# ã€ç¬¬å››æ­¥ï¼šå·¥å…·å‡½æ•°ã€‘ - å®šä¹‰ä¸€äº›æ–¹ä¾¿æˆ‘ä»¬é‡å¤ä½¿ç”¨çš„â€œå°å·¥å…·â€\n",
    "# è¿™éƒ¨åˆ†ä»£ç é€šå¸¸ä¸éœ€è¦ä¿®æ”¹ã€‚\n",
    "# =====================================================================================\n",
    "\n",
    "def setup_directories():\n",
    "    \"\"\"åˆ›å»ºé¡¹ç›®æ‰€éœ€çš„æ‰€æœ‰ç›®å½•ç»“æ„,å¦‚æœä¸å­˜åœ¨çš„è¯ã€‚\"\"\"\n",
    "    directories = [DATA_DIR, FORMATTED_DATA_DIR, MODEL_CACHE_DIR, CHECKPOINTS_DIR, FINAL_ADAPTER_DIR]\n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    print(\"âœ… æ‰€æœ‰é¡¹ç›®ç›®å½•å·²å‡†å¤‡å°±ç»ª\")\n",
    "\n",
    "def dataset_jsonl_transfer(origin_path, new_path):\n",
    "    \"\"\"å°†æˆ‘ä»¬è‡ªå®šä¹‰çš„åŸå§‹æ•°æ®æ ¼å¼,è½¬æ¢ä¸ºæ›´é€šç”¨çš„{\"input\": \"...\", \"output\": \"...\"}æ ¼å¼ã€‚\"\"\"\n",
    "    messages = []\n",
    "    try:\n",
    "        with open(origin_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            for line_num, line in enumerate(file, 1):\n",
    "                try:\n",
    "                    data = json.loads(line.strip())\n",
    "                    if \"question\" not in data or \"think\" not in data or \"answer\" not in data:\n",
    "                        print(f\"âš ï¸ è­¦å‘Šï¼šç¬¬{line_num}è¡Œç¼ºå°‘å¿…è¦å­—æ®µ,è·³è¿‡\")\n",
    "                        continue\n",
    "                    # å°†é—®é¢˜ä½œä¸ºè¾“å…¥\n",
    "                    input_text = data[\"question\"]\n",
    "                    # å°†æ€è€ƒè¿‡ç¨‹å’Œç­”æ¡ˆåˆå¹¶ä½œä¸ºè¾“å‡º\n",
    "                    output_text = f'<|FunctionCallBegin|>{data[\"think\"]}<|FunctionCallEnd|>\\n{data[\"answer\"]}'\n",
    "                    messages.append({\"input\": input_text, \"output\": output_text})\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"âš ï¸ è­¦å‘Šï¼šç¬¬{line_num}è¡ŒJSONæ ¼å¼é”™è¯¯,è·³è¿‡\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {origin_path}\")\n",
    "    \n",
    "    with open(new_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for message in messages:\n",
    "            file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"âœ… æ•°æ®è½¬æ¢å®Œæˆ: {len(messages)} æ¡æœ‰æ•ˆæ•°æ®\")\n",
    "\n",
    "\n",
    "def process_func(example, tokenizer):\n",
    "    \"\"\"\n",
    "    ã€ï¼ï¼ï¼æ ¸å¿ƒé‡ç‚¹ï¼ï¼ï¼ã€‘æ•°æ®å¤„ç†å‡½æ•° - å°†ä¸€æ¡æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹èƒ½â€œåƒâ€çš„æ ¼å¼\n",
    "    è¿™æ˜¯æ•´ä¸ªå¾®è°ƒæµç¨‹ä¸­æœ€å…³é”®çš„å‡½æ•°ä¹‹ä¸€,å®ƒå®šä¹‰äº†æ¨¡å‹å¦‚ä½•å­¦ä¹ ã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    # ã€æ„å»ºç¬¦åˆQwenèŠå¤©æ¨¡æ¿çš„è¾“å…¥æ ¼å¼ã€‘\n",
    "    #   - æ¨¡å‹å°±åƒä¸€ä¸ªæ¼”å‘˜,éœ€è¦æ¸…æ™°çš„å‰§æœ¬æ‰èƒ½æ¼”å¥½æˆã€‚è¿™ä¸ªæ¨¡æ¿å°±æ˜¯å‰§æœ¬ã€‚\n",
    "    #   - `<|im_start|>` å’Œ `<|im_end|>` æ˜¯Qwenæ¨¡å‹çº¦å®šçš„ç‰¹æ®Šæ ‡è®°,ç”¨æ¥åŒºåˆ†ä¸åŒè§’è‰²çš„å¯¹è¯ã€‚\n",
    "    #   - `system`: ç³»ç»ŸæŒ‡ä»¤,æˆ‘ä»¬åœ¨è¿™é‡Œæ”¾å…¥ä¹‹å‰å®šä¹‰çš„SYSTEM_PROMPTã€‚\n",
    "    #   - `user`: ç”¨æˆ·çš„æé—®,æˆ‘ä»¬æ”¾å…¥æ•°æ®ä¸­çš„'input'å­—æ®µã€‚\n",
    "    #   - `assistant`: AIåŠ©æ‰‹çš„å›ç­”éƒ¨åˆ†,æˆ‘ä»¬åœ¨è¿™é‡Œç•™ä¸€ä¸ªå¼€å¤´,è®©æ¨¡å‹æ¥ç€ç”Ÿæˆã€‚\n",
    "    instruction_text = (\n",
    "        f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{example['input']}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "    \n",
    "    # ä½¿ç”¨åˆ†è¯å™¨å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—IDã€‚`add_special_tokens=False`å› ä¸ºæˆ‘ä»¬å·²ç»æ‰‹åŠ¨æ·»åŠ äº†æ¨¡æ¿ã€‚\n",
    "    instruction = tokenizer(instruction_text, add_special_tokens=False)\n",
    "    response = tokenizer(example['output'], add_special_tokens=False)\n",
    "\n",
    "    # ã€æ‹¼æ¥å®Œæ•´çš„è¾“å…¥åºåˆ—ã€‘\n",
    "    # å®Œæ•´çš„è¾“å…¥ = ç³»ç»ŸæŒ‡ä»¤éƒ¨åˆ† + ç”¨æˆ·é—®é¢˜éƒ¨åˆ† + åŠ©æ‰‹å›ç­”éƒ¨åˆ† + ç»“æŸæ ‡è®°\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    \n",
    "    # ã€ï¼ï¼ï¼æ ¸å¿ƒé‡ç‚¹ï¼ï¼ï¼ã€‘è®¾è®¡æ ‡ç­¾(labels) - å‘Šè¯‰æ¨¡å‹åº”è¯¥å­¦ä¹ ä»€ä¹ˆ\n",
    "    #   - è¿™æ˜¯ç›‘ç£å­¦ä¹ çš„æ ¸å¿ƒï¼šä¸ºæ¨¡å‹çš„å­¦ä¹ â€œåˆ’é‡ç‚¹â€ã€‚\n",
    "    #   - æˆ‘ä»¬å¸Œæœ›æ¨¡å‹å­¦ä¼šâ€œåœ¨çœ‹åˆ°ç”¨æˆ·é—®é¢˜å,ç”Ÿæˆæˆ‘ä»¬æä¾›çš„ç­”æ¡ˆâ€,è€Œä¸æ˜¯å­¦ä¼šâ€œå¤è¿°ç”¨æˆ·çš„é—®é¢˜â€ã€‚\n",
    "    #   - å› æ­¤,æˆ‘ä»¬å°†ç³»ç»ŸæŒ‡ä»¤å’Œç”¨æˆ·é—®é¢˜éƒ¨åˆ†çš„æ ‡ç­¾è®¾ç½®ä¸º-100ã€‚\n",
    "    #   - åœ¨PyTorchä¸­,-100æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„å¿½ç•¥ç´¢å¼•,æ„å‘³ç€åœ¨è®¡ç®—æŸå¤±ï¼ˆloss,å³æ¨¡å‹çš„å›ç­”ä¸æ ‡å‡†ç­”æ¡ˆçš„å·®è·ï¼‰æ—¶,è¿™äº›éƒ¨åˆ†ä¸å‚ä¸è®¡ç®—ã€‚\n",
    "    #   - åªæœ‰åŠ©æ‰‹å›ç­”éƒ¨åˆ†çš„æ ‡ç­¾æ˜¯çœŸå®çš„Token ID,æ¨¡å‹ä¼šåŠªåŠ›è®©è‡ªå·±çš„é¢„æµ‹æ¥è¿‘è¿™äº›IDã€‚\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    \n",
    "    # ã€é•¿åº¦æˆªæ–­ã€‘\n",
    "    # å¦‚æœæ‹¼æ¥åçš„åºåˆ—è¶…è¿‡äº†æˆ‘ä»¬è®¾å®šçš„æœ€å¤§é•¿åº¦MAX_LENGTH,å°±è¿›è¡Œæˆªæ–­,é˜²æ­¢æ˜¾å­˜æº¢å‡ºã€‚\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    \n",
    "    # attention_maskï¼ˆæ³¨æ„åŠ›æ©ç ï¼‰å…¨ä¸º1,è¡¨ç¤ºæ¨¡å‹éœ€è¦å…³æ³¨åºåˆ—ä¸­çš„æ¯ä¸€ä¸ªtokenã€‚\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": [1] * len(input_ids), \"labels\": labels}\n",
    "\n",
    "def predict(messages, model, tokenizer):\n",
    "    \"\"\"ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œä¸€æ¬¡å¯¹è¯é¢„æµ‹ã€‚\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # `torch.no_grad()`è¡¨ç¤ºåœ¨æ¨ç†æ—¶æˆ‘ä»¬ä¸éœ€è¦è®¡ç®—æ¢¯åº¦,è¿™å¯ä»¥èŠ‚çœå¤§é‡æ˜¾å­˜å¹¶åŠ å¿«é€Ÿåº¦ã€‚\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=512,        # å›ç­”çš„æœ€å¤§é•¿åº¦\n",
    "            temperature=0.3,           # æ¸©åº¦è¾ƒä½,å›ç­”æ›´å…·ç¡®å®šæ€§,é€‚åˆä¸¥è°¨çš„åŒ»å­¦é¢†åŸŸ\n",
    "            repetition_penalty=1.1,    # è½»å¾®æƒ©ç½šé‡å¤,é¿å…æ¨¡å‹è¯´è½¦è½±è¾˜è¯\n",
    "            do_sample=True,            # å¯ç”¨é‡‡æ ·,è®©å›ç­”æ›´è‡ªç„¶\n",
    "            top_p=0.8,                 # ä»æ¦‚ç‡æœ€é«˜çš„80%è¯æ±‡ä¸­é‡‡æ ·\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "    # ä»ç”Ÿæˆç»“æœä¸­å‰”é™¤è¾“å…¥éƒ¨åˆ†,åªä¿ç•™æ–°ç”Ÿæˆçš„å›ç­”ã€‚\n",
    "    response_ids = generated_ids[0][len(model_inputs.input_ids[0]):]\n",
    "    response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# =====================================================================================\n",
    "# ã€ç¬¬äº”æ­¥ï¼šä¸»æ‰§è¡Œæµç¨‹ã€‘ - å°†æ‰€æœ‰é›¶ä»¶ç»„è£…èµ·æ¥,å¯åŠ¨å¾®è°ƒï¼\n",
    "# =====================================================================================\n",
    "def main():\n",
    "    \"\"\"ä¸»è®­ç»ƒæµç¨‹å‡½æ•°\"\"\"\n",
    "    \n",
    "    # --- æ­¥éª¤1: ç¯å¢ƒä¸è·¯å¾„åˆå§‹åŒ– ---\n",
    "    print(\"ğŸš€ å¼€å§‹QLoRAåŒ»å­¦é—®ç­”å¾®è°ƒæµç¨‹...\")\n",
    "    check_environment()\n",
    "    setup_directories()\n",
    "\n",
    "    # --- æ­¥éª¤2: åŠ è½½åˆ†è¯å™¨å’ŒåŸºç¡€æ¨¡å‹ ---\n",
    "    print(f\"\\nğŸ“¥ æ­£åœ¨ä»'{MODEL_ID}'åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨...\")\n",
    "    model_dir = snapshot_download(MODEL_ID, cache_dir=MODEL_CACHE_DIR, revision=\"master\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True)\n",
    "    # ä¸ºæˆ‘ä»¬è‡ªå®šä¹‰çš„ç‰¹æ®Šæ ‡è®°<|FunctionCall...|>æ‰©å±•è¯æ±‡è¡¨\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': ['<|FunctionCallBegin|>', '<|FunctionCallEnd|>']})\n",
    "    # å¦‚æœæ¨¡å‹æ²¡æœ‰é»˜è®¤çš„pad_token,å°±ç”¨eos_tokenï¼ˆç»“æŸæ ‡è®°ï¼‰ä»£æ›¿,è¿™æ˜¯å¸¸è§åšæ³•ã€‚\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"âœ… åˆ†è¯å™¨åŠ è½½å¹¶æ‰©å±•å®Œæˆ\")\n",
    "\n",
    "    # --- æ­¥éª¤3: é…ç½®QLoRAé‡åŒ– ---\n",
    "    print(\"\\nâš™ï¸ é…ç½®4-bité‡åŒ–å‚æ•°(QLoRA)...\")\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,                      # ã€æ ¸å¿ƒã€‘å¯ç”¨4-bité‡åŒ–,å°†æ¨¡å‹æƒé‡ä»32ä½å‹ç¼©åˆ°4ä½ã€‚\n",
    "        bnb_4bit_quant_type=\"nf4\",              # ä½¿ç”¨NF4ï¼ˆNormal Float 4ï¼‰é‡åŒ–ç±»å‹,è¿™æ˜¯ä¸€ç§ä¸“ä¸ºç¥ç»ç½‘ç»œä¼˜åŒ–çš„æ ¼å¼ã€‚\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,  # åœ¨è®¡ç®—æ—¶,ä¸´æ—¶å°†æƒé‡æ¢å¤åˆ°bfloat16ç²¾åº¦,ä»¥ä¿è¯è®¡ç®—çš„å‡†ç¡®æ€§ã€‚\n",
    "        bnb_4bit_use_double_quant=True,         # å¯ç”¨åŒé‡é‡åŒ–,è¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜ã€‚\n",
    "    )\n",
    "\n",
    "    # --- æ­¥éª¤4: åŠ è½½é‡åŒ–æ¨¡å‹å¹¶åº”ç”¨LoRA ---\n",
    "    print(\"\\nğŸ”§ åŠ è½½é‡åŒ–æ¨¡å‹å¹¶åº”ç”¨LoRA...\")\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dir,\n",
    "            device_map=\"auto\",                  # è‡ªåŠ¨å°†æ¨¡å‹åˆ†å±‚åŠ è½½åˆ°å¯ç”¨çš„GPUå’ŒCPUä¸Š,ä»¥æœ€å¤§åŒ–åˆ©ç”¨èµ„æºã€‚\n",
    "            torch_dtype=torch.bfloat16,         # æŒ‡å®šè®¡ç®—æ—¶ä½¿ç”¨çš„æ•°æ®ç±»å‹ã€‚\n",
    "            quantization_config=quantization_config, # åº”ç”¨æˆ‘ä»¬ä¸Šé¢å®šä¹‰çš„4-bité‡åŒ–é…ç½®ã€‚\n",
    "            attn_implementation=\"flash_attention_2\" # ã€æ€§èƒ½ä¼˜åŒ–ã€‘ä½¿ç”¨Flash Attention 2,èƒ½å¤§å¹…æå‡è®­ç»ƒé€Ÿåº¦å¹¶èŠ‚çœæ˜¾å­˜ã€‚\n",
    "        )\n",
    "        print(\"âœ… é‡åŒ–æ¨¡å‹åŠ è½½å®Œæˆ (Flash Attention 2 å·²å¯ç”¨)\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Flash Attention 2åŠ è½½å¤±è´¥,å°†ä½¿ç”¨æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶: {e}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dir, device_map=\"auto\", torch_dtype=torch.bfloat16,\n",
    "            quantization_config=quantization_config\n",
    "        )\n",
    "\n",
    "    # è°ƒæ•´æ¨¡å‹çš„è¯åµŒå…¥å±‚å¤§å°,ä»¥é€‚åº”æˆ‘ä»¬æ–°å¢çš„ç‰¹æ®ŠTokenã€‚\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    # ä¸ºé‡åŒ–åçš„æ¨¡å‹åšä¸€äº›è®­ç»ƒå‰çš„å‡†å¤‡å·¥ä½œã€‚\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    # å®šä¹‰LoRAé…ç½®ã€‚\n",
    "    lora_config = LoraConfig(\n",
    "        r=LORA_R, lora_alpha=LORA_ALPHA, target_modules=TARGET_MODULES,\n",
    "        lora_dropout=LORA_DROPOUT, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    # å°†LoRAé…ç½®â€œå®‰è£…â€åˆ°æ¨¡å‹ä¸Šã€‚\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    # æ‰“å°å‡ºå¯è®­ç»ƒå‚æ•°çš„æ•°é‡å’Œæ¯”ä¾‹,æ‚¨ä¼šå‘ç°å®ƒéå¸¸å°ï¼è¿™å°±æ˜¯QLoRAçš„é­…åŠ›ã€‚\n",
    "    model.print_trainable_parameters()\n",
    "    print(\"âœ… LoRAé€‚é…å™¨å·²æˆåŠŸåº”ç”¨\")\n",
    "\n",
    "    # --- æ­¥éª¤5: æ•°æ®é›†å‡†å¤‡ä¸é¢„å¤„ç† ---\n",
    "    print(\"\\nğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...\")\n",
    "    train_original_path = os.path.join(DATA_DIR, \"train.jsonl\")\n",
    "    val_original_path = os.path.join(DATA_DIR, \"val.jsonl\")\n",
    "    train_formatted_path = os.path.join(FORMATTED_DATA_DIR, \"train_formatted.jsonl\")\n",
    "    val_formatted_path = os.path.join(FORMATTED_DATA_DIR, \"val_formatted.jsonl\")\n",
    "    # å¦‚æœæ ¼å¼åŒ–æ•°æ®ä¸å­˜åœ¨,åˆ™è¿›è¡Œè½¬æ¢\n",
    "    if not os.path.exists(train_formatted_path): dataset_jsonl_transfer(train_original_path, train_formatted_path)\n",
    "    if not os.path.exists(val_formatted_path): dataset_jsonl_transfer(val_original_path, val_formatted_path)\n",
    "    # åŠ è½½æ•°æ®é›†\n",
    "    train_dataset = Dataset.from_pandas(pd.read_json(train_formatted_path, lines=True))\n",
    "    eval_dataset = Dataset.from_pandas(pd.read_json(val_formatted_path, lines=True))\n",
    "    # ä½¿ç”¨.map()æ–¹æ³•å¯¹æ•°æ®é›†ä¸­çš„æ¯ä¸€æ¡æ•°æ®åº”ç”¨æˆ‘ä»¬çš„process_funcå‡½æ•°\n",
    "    tokenized_train_dataset = train_dataset.map(lambda x: process_func(x, tokenizer), remove_columns=train_dataset.column_names)\n",
    "    tokenized_eval_dataset = eval_dataset.map(lambda x: process_func(x, tokenizer), remove_columns=eval_dataset.column_names)\n",
    "    print(\"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ\")\n",
    "\n",
    "    # --- æ­¥éª¤6: é…ç½®è®­ç»ƒå‚æ•° ---\n",
    "    print(\"\\nâš™ï¸ é…ç½®è®­ç»ƒå‚æ•°...\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=os.path.join(CHECKPOINTS_DIR, RUN_NAME), # æ‰€æœ‰è¾“å‡ºï¼ˆæ£€æŸ¥ç‚¹ã€æ—¥å¿—ç­‰ï¼‰éƒ½å°†ä¿å­˜åœ¨è¿™ä¸ªåŠ¨æ€ç”Ÿæˆçš„æ–‡ä»¶å¤¹ä¸­\n",
    "        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "        # ã€ä¼˜åŒ–å™¨ã€‘ä½¿ç”¨paged_adamw_8bit,è¿™æ˜¯QLoRAæ¨èçš„ã€èƒ½è¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜çš„ä¼˜åŒ–å™¨ã€‚\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        # ã€å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‘ä½¿ç”¨ä½™å¼¦é€€ç«è°ƒåº¦å™¨,å¯ä»¥åœ¨è®­ç»ƒå¼€å§‹æ—¶ç¼“æ…¢å¢åŠ å­¦ä¹ ç‡ï¼ˆé¢„çƒ­ï¼‰,ç„¶ååœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¹³æ»‘åœ°é™ä½å­¦ä¹ ç‡ã€‚\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.1, # å‰10%çš„è®­ç»ƒæ­¥æ•°ç”¨äºé¢„çƒ­ã€‚\n",
    "        # ã€è¯„ä¼°ä¸ä¿å­˜ã€‘\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=EVAL_STEPS,\n",
    "        save_steps=SAVE_STEPS,\n",
    "        save_total_limit=3, # æœ€å¤šåªä¿ç•™æœ€è¿‘çš„3ä¸ªæ£€æŸ¥ç‚¹,é¿å…å ç”¨è¿‡å¤šç£ç›˜ç©ºé—´ã€‚\n",
    "        # ã€æ—¥å¿—ä¸æŠ¥å‘Šã€‘\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        report_to=\"swanlab\", # å°†è®­ç»ƒæŒ‡æ ‡ä¸ŠæŠ¥ç»™SwanLabã€‚\n",
    "        run_name=RUN_NAME,   # åœ¨SwanLabä¸­æ˜¾ç¤ºçš„å®éªŒåç§°ã€‚\n",
    "        # ã€æ¨¡å‹åŠ è½½ç­–ç•¥ã€‘\n",
    "        load_best_model_at_end=True,        # è®­ç»ƒç»“æŸå,è‡ªåŠ¨åŠ è½½åœ¨éªŒè¯é›†ä¸Šè¡¨ç°æœ€å¥½çš„é‚£ä¸ªæ£€æŸ¥ç‚¹ã€‚\n",
    "        metric_for_best_model=\"eval_loss\",  # åˆ¤æ–­â€œæœ€ä½³â€çš„æ ‡å‡†æ˜¯éªŒè¯é›†æŸå¤±ï¼ˆeval_lossï¼‰æœ€ä½ã€‚\n",
    "        greater_is_better=False,            # æŸå¤±ï¼ˆlossï¼‰æ˜¯è¶Šå°è¶Šå¥½,æ‰€ä»¥è®¾ç½®ä¸ºFalseã€‚\n",
    "        # ã€ç²¾åº¦ä¸æ€§èƒ½ã€‘\n",
    "        bf16=True,                          # ä½¿ç”¨bfloat16æ··åˆç²¾åº¦è®­ç»ƒ,å¯ä»¥æé€Ÿå¹¶èŠ‚çœæ˜¾å­˜,ä¸”åœ¨ç°ä»£GPUä¸Šæ¯”fp16æ›´ç¨³å®šã€‚\n",
    "        gradient_checkpointing=True,        # ã€ï¼ï¼ï¼æ ¸å¿ƒæ˜¾å­˜ä¼˜åŒ–ï¼ï¼ï¼ã€‘å¦ä¸€ä¸ªâ€œç”¨æ—¶é—´æ¢ç©ºé—´â€çš„å…³é”®æŠ€æœ¯ã€‚å®ƒåœ¨åå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—ä¸­é—´æ¿€æ´»å€¼,è€Œä¸æ˜¯å…¨éƒ¨å­˜å‚¨ä¸‹æ¥,èƒ½èŠ‚çœå¤§é‡æ˜¾å­˜ã€‚\n",
    "    )\n",
    "    \n",
    "    # --- æ­¥éª¤7: åˆå§‹åŒ–è®­ç»ƒå™¨ ---\n",
    "    print(\"\\nğŸƒâ€â™‚ï¸ åˆå§‹åŒ–è®­ç»ƒå™¨...\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_eval_dataset,\n",
    "        # æ•°æ®æ•´ç†å™¨,è´Ÿè´£å°†æ•°æ®æ ·æœ¬æ™ºèƒ½åœ°æ‰“åŒ…æˆæ‰¹æ¬¡\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    "    )\n",
    "    print(\"âœ… è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ\")\n",
    "\n",
    "    # --- æ­¥éª¤8: å¼€å§‹è®­ç»ƒ ---\n",
    "    print(\"\\nğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"  ğŸ¦¢ SwanLab å®éªŒ: '{RUN_NAME}'\")\n",
    "    print(f\"  ğŸ“‚ é¡¹ç›®å: '{os.environ['SWANLAB_PROJECT']}'\")\n",
    "    print(f\"  ğŸ¯ æ¨¡å‹: {MODEL_ID}, ğŸ”„ è½®æ•°: {NUM_TRAIN_EPOCHS}, ğŸ“¦ æœ‰æ•ˆæ‰¹é‡: {EFFECTIVE_BATCH_SIZE}\")\n",
    "    print(\"=\"*80)\n",
    "    # è¿™è¡Œä»£ç ä¼šå¯åŠ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹,å¹¶è‡ªåŠ¨å¤„ç†æ‰€æœ‰å¾ªç¯ã€è¯„ä¼°ã€ä¿å­˜ç­‰ç¹çå·¥ä½œã€‚\n",
    "    trainer.train()\n",
    "    print(\"ğŸ‰ è®­ç»ƒå®Œæˆ!\")\n",
    "\n",
    "    # --- æ­¥éª¤9: ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---\n",
    "    print(\"\\nğŸ’¾ ä¿å­˜æœ€ç»ˆçš„LoRAé€‚é…å™¨...\")\n",
    "    # æˆ‘ä»¬åªä¿å­˜è½»é‡çº§çš„LoRAé€‚é…å™¨ï¼ˆâ€œç¬”è®°æœ¬â€ï¼‰,è€Œä¸æ˜¯æ•´ä¸ªå¤§æ¨¡å‹ã€‚\n",
    "    final_model_path = os.path.join(FINAL_ADAPTER_DIR, RUN_NAME)\n",
    "    model.save_pretrained(final_model_path)\n",
    "    tokenizer.save_pretrained(final_model_path) # åŒæ—¶ä¿å­˜åˆ†è¯å™¨,ç¡®ä¿æ¨ç†æ—¶èƒ½æ­£ç¡®åŠ è½½ã€‚\n",
    "    print(f\"âœ… æœ€ç»ˆé€‚é…å™¨å·²ä¿å­˜åˆ°: {final_model_path}\")\n",
    "\n",
    "    # --- æ­¥éª¤10: æ¨¡å‹æµ‹è¯• ---\n",
    "    print(\"\\nğŸ§ª è¿›è¡Œæ¨¡å‹æµ‹è¯•...\")\n",
    "    test_samples = eval_dataset.select(range(3)) # ä»éªŒè¯é›†ä¸­é€‰å‡ æ¡æ•°æ®è¿›è¡Œæµ‹è¯•\n",
    "    test_results = []\n",
    "    for sample in test_samples:\n",
    "        # æˆ‘ä»¬éœ€è¦å°†æ•°å­—IDè§£ç å›æ–‡æœ¬æ‰èƒ½è¿›è¡Œæµ‹è¯•\n",
    "        question = tokenizer.decode([token for token in sample['input_ids'] if token != tokenizer.pad_token_id], skip_special_tokens=True)\n",
    "        # ä»Qwenæ¨¡æ¿ä¸­æå–å‡ºçœŸæ­£çš„ç”¨æˆ·é—®é¢˜\n",
    "        user_question = question.split(\"<|im_start|>user\\n\")[1].split(\"<|im_end|>\")[0]\n",
    "        \n",
    "        messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": user_question}]\n",
    "        response = predict(messages, model, tokenizer)\n",
    "        \n",
    "        # å°†æ ‡å‡†ç­”æ¡ˆä¹Ÿè§£ç å‡ºæ¥\n",
    "        ground_truth = tokenizer.decode([token for token in sample['labels'] if token != -100], skip_special_tokens=True)\n",
    "\n",
    "        result_text = (\n",
    "            f\"ğŸ” **æµ‹è¯•æ ·æœ¬**:\\n\\n\"\n",
    "            f\"**â“ é—®é¢˜**: {user_question}\\n\\n\"\n",
    "            f\"**ğŸ¤– æ¨¡å‹å›ç­”**: {response}\\n\\n\"\n",
    "            f\"**âœ… æ ‡å‡†ç­”æ¡ˆ**: {ground_truth}\\n\"\n",
    "        )\n",
    "        test_results.append(swanlab.Text(result_text, caption=\"æµ‹è¯•æ ·æœ¬\"))\n",
    "        print(result_text + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "    if test_results:\n",
    "        swanlab.log({\"æµ‹è¯•æ ·ä¾‹\": test_results})\n",
    "\n",
    "    # --- æ­¥éª¤11: è®­ç»ƒæ€»ç»“ ---\n",
    "    print(\"\\nğŸ“‹ è®­ç»ƒæ€»ç»“ä¸ä½¿ç”¨æŒ‡å¯¼:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"âœ… æœ€ç»ˆé€‚é…å™¨å·²ä¿å­˜è‡³: {final_model_path}\")\n",
    "    print(\"\\nğŸ“ æ¨¡å‹ä½¿ç”¨æŒ‡å¯¼:\")\n",
    "    print(f\"1. åŠ è½½åŸºç¡€æ¨¡å‹: `AutoModelForCausalLM.from_pretrained('{MODEL_ID}')`\")\n",
    "    print(f\"2. åŠ è½½LoRAé€‚é…å™¨: `PeftModel.from_pretrained(base_model, '{final_model_path}')`\")\n",
    "    print(\"3. ä½¿ç”¨ç›¸åŒçš„SYSTEM_PROMPTå’ŒèŠå¤©æ¨¡æ¿è¿›è¡Œæ¨ç†ã€‚\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    swanlab.finish()\n",
    "    print(\"\\nğŸ‰ QLoRAåŒ»å­¦é—®ç­”å¾®è°ƒæµç¨‹å…¨éƒ¨å®Œæˆ!\")\n",
    "\n",
    "# =====================================================================================\n",
    "# ã€ç¬¬å…­æ­¥ï¼šç¨‹åºå…¥å£ã€‘ - æ•´ä¸ªè„šæœ¬ä»è¿™é‡Œå¼€å§‹æ‰§è¡Œ\n",
    "# =====================================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        # å…è®¸ç”¨æˆ·é€šè¿‡ Ctrl+C æ‰‹åŠ¨ä¸­æ–­è®­ç»ƒ\n",
    "        print(\"\\nâ¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­\")\n",
    "    except Exception as e:\n",
    "        # æ•è·å…¶ä»–æ‰€æœ‰å¯èƒ½çš„é”™è¯¯,å¹¶æ‰“å°è¯¦ç»†ä¿¡æ¯,æ–¹ä¾¿æ’æŸ¥é—®é¢˜\n",
    "        print(f\"\\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        # æ— è®ºç¨‹åºæ˜¯æˆåŠŸç»“æŸè¿˜æ˜¯ä¸­é€”å¤±è´¥,éƒ½ç¡®ä¿å…³é—­SwanLabçš„æ—¥å¿—è®°å½•\n",
    "        if 'swanlab' in globals():\n",
    "            swanlab.finish()\n",
    "        print(\"ğŸ”š ç¨‹åºç»“æŸ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
