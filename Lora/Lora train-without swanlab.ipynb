{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "546793ef-4f7a-4736-8f9b-1ce766e1c6d9",
   "metadata": {},
   "source": [
    "1. å®‰è£…ç»„ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb280ce4-9150-427d-b3f0-637ff004bd87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa8b02-844d-4838-a031-bc7cd83aa620",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3f8f9f-4c33-47c2-bdad-a7183cf547e4",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers trl peft bitsandbytes accelerate datasets pandas modelscope swanlab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902590f7-567f-4776-aa77-14f58195ee85",
   "metadata": {},
   "source": [
    "2. åŠ è½½æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0e6a5c6-c5c1-4bd0-8df8-6c18a048d101",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-08-27T04:26:55.244420Z",
     "iopub.status.busy": "2025-08-27T04:26:55.244198Z",
     "iopub.status.idle": "2025-08-27T04:27:06.470857Z",
     "shell.execute_reply": "2025-08-27T04:27:06.470342Z",
     "shell.execute_reply.started": "2025-08-27T04:26:55.244404Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 12:26:55,287 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from delicate_medical_r1_data. Please make sure that you can trust the external codes.\n",
      "2025-08-27 12:26:55,943 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from krisfu/delicate_medical_r1_data. Please make sure that you can trust the external codes.\n",
      "2025-08-27 12:26:55,944 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from krisfu/delicate_medical_r1_data. Please make sure that you can trust the external codes.\n",
      "2025-08-27 12:26:55,944 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from krisfu/delicate_medical_r1_data. Please make sure that you can trust the external codes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has been split successfully.\n",
      "Train Set Size: 2166\n",
      "Val Set Size: 241\n"
     ]
    }
   ],
   "source": [
    "from modelscope.msdatasets import MsDataset\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "random.seed(42)\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "ds = MsDataset.load(\n",
    "    'krisfu/delicate_medical_r1_data',\n",
    "    subset_name='default',\n",
    "    split='train',\n",
    "    trust_remote_code=True  # æ˜¾å¼å£°æ˜\n",
    ")\n",
    "data_list = list(ds)\n",
    "random.shuffle(data_list)\n",
    "\n",
    "# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "split_idx = int(len(data_list) * 0.9)\n",
    "train_data = data_list[:split_idx]\n",
    "val_data = data_list[split_idx:]\n",
    "\n",
    "# åˆ›å»º data ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# ä¿å­˜åˆ° data ç›®å½•ä¸‹çš„ jsonl æ–‡ä»¶\n",
    "with open('data/train.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in train_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('data/val.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in val_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"The dataset has been split successfully.\")\n",
    "print(f\"Train Set Size: {len(train_data)}\")\n",
    "print(f\"Val Set Size: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7098153b-c818-46c5-b5fd-14f099cb4222",
   "metadata": {},
   "source": [
    "3. å¼€å§‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caf70d57-d5d8-497f-bd57-4501021b5495",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-08-27T06:45:12.938433Z",
     "iopub.status.busy": "2025-08-27T06:45:12.937857Z",
     "iopub.status.idle": "2025-08-27T07:15:28.228612Z",
     "shell.execute_reply": "2025-08-27T07:15:28.228204Z",
     "shell.execute_reply.started": "2025-08-27T06:45:12.938406Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹QLoRAåŒ»å­¦é—®ç­”å¾®è°ƒæµç¨‹...\n",
      "âœ… CUDAç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼\n",
      "âœ… æ‰€æœ‰é¡¹ç›®ç›®å½•å·²å‡†å¤‡å°±ç»ª\n",
      "\n",
      "ğŸ“¥ æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹çš„åˆ†è¯å™¨...\n",
      "Downloading Model from https://www.modelscope.cn to directory: ./model_cache/qwen/Qwen3-1.7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 14:45:14,158 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åˆ†è¯å™¨åŠ è½½å¹¶æ‰©å±•å®Œæˆ\n",
      "\n",
      "âš™ï¸ é…ç½®4-bité‡åŒ–å‚æ•°(QLoRA)...\n",
      "\n",
      "ğŸ”§ åŠ è½½é‡åŒ–åçš„åŸºç¡€æ¨¡å‹...\n",
      "âš ï¸ Flash Attention 2ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶: /usr/local/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248c02731c2d4e0781aa2d39776def5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… é‡åŒ–æ¨¡å‹åŠ è½½å®Œæˆ (æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶)\n",
      "trainable params: 6,422,528 || all params: 1,726,454,784 || trainable%: 0.3720\n",
      "âœ… LoRAé€‚é…å™¨å·²æˆåŠŸåº”ç”¨\n",
      "\n",
      "ğŸ“Š å‡†å¤‡å¹¶é¢„å¤„ç†æ•°æ®...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e85ceca24945fcaa87b78dec82a998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd36f431ee24ce1be0866cca3ac2d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/241 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†å®Œæˆ\n",
      "\n",
      "âš™ï¸ é…ç½®è®­ç»ƒå‚æ•°å¹¶åˆå§‹åŒ–è®­ç»ƒå™¨...\n",
      "âœ… è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ\n",
      "\n",
      "ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [136/136 28:13, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ è®­ç»ƒå®Œæˆ!\n",
      "\n",
      "ğŸ”— æ­£åœ¨å°†LoRAé€‚é…å™¨æƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­ï¼Œä»¥ç”Ÿæˆå¯ç‹¬ç«‹éƒ¨ç½²çš„å®Œæ•´æ¨¡å‹...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411ff9aebd4544eda0c116159d072c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    -> æ­£åœ¨ä»æœ€æ–°çš„æ£€æŸ¥ç‚¹åŠ è½½LoRAé€‚é…å™¨: ./output/checkpoints/qwen-medical-qlora-lr0.0001-bs32-r16/checkpoint-136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LoRAæƒé‡åˆå¹¶å®Œæˆï¼\n",
      "\n",
      "ğŸ’¾ æ­£åœ¨ä¿å­˜åˆå¹¶åçš„å®Œæ•´æ¨¡å‹...\n",
      "âœ… æœ€ç»ˆçš„â€œå®˜æ–¹æ”¹è£…ç‰ˆâ€æ¨¡å‹å·²ä¿å­˜åˆ°: ./output/final_model/qwen-medical-qlora-lr0.0001-bs32-r16\n",
      "\n",
      "ğŸ‰ QLoRAåŒ»å­¦é—®ç­”å¾®è°ƒæµç¨‹å…¨éƒ¨å®Œæˆ!\n",
      "ğŸ”š ç¨‹åºç»“æŸ\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "åŒ»å­¦é—®ç­”æ¨¡å‹QLoRAé«˜æ•ˆå¾®è°ƒè„šæœ¬ - ã€V8.0 æè‡´æ³¨é‡Šä¿®å¤ç‰ˆã€‘\n",
    "\n",
    "ã€æ ¸å¿ƒä¿®å¤ã€‘\n",
    "1.  [ä¿®å¤] å½»åº•è§£å†³ `UnboundLocalError`: å°†æ‰€æœ‰importè¯­å¥ç§»è‡³æ–‡ä»¶é¡¶éƒ¨ï¼Œéµå¾ªPythonæœ€ä½³å®è·µï¼Œè§£å†³äº†å±€éƒ¨ä½œç”¨åŸŸå¯¼è‡´çš„å˜é‡æœªå®šä¹‰é—®é¢˜ã€‚\n",
    "2.  [ä¿®å¤] ä¿®æ­£æ¨¡å‹ä¿å­˜é€»è¾‘: åœ¨æœ€åä¸€æ­¥ç¡®ä¿ä¿å­˜çš„æ˜¯åˆå¹¶åçš„å®Œæ•´æ¨¡å‹(`merged_model`)ï¼Œè€Œä¸æ˜¯è®­ç»ƒæ—¶ä½¿ç”¨çš„é‡åŒ–æ¨¡å‹(`model`)ã€‚\n",
    "3.  [æ–°å¢] â€œå®˜æ–¹æ”¹è£…ç‰ˆâ€æ¨¡å‹å¯¼å‡º: åœ¨è®­ç»ƒç»“æŸåï¼Œè‡ªåŠ¨å°†LoRAæƒé‡ä¸åŸºç¡€æ¨¡å‹åˆå¹¶ï¼Œç”Ÿæˆä¸€ä¸ªç‹¬ç«‹çš„ã€å¼€ç®±å³ç”¨çš„å®Œæ•´æ¨¡å‹ï¼Œæå¤§æ–¹ä¾¿äº†åç»­çš„è¯„ä¼°å’Œéƒ¨ç½²ã€‚\n",
    "\n",
    "ã€ä¿ç•™åŠŸèƒ½ã€‘\n",
    "1.  QLoRAå¾®è°ƒã€Flash Attention 2ã€å®Œæ•´çš„è®­ç»ƒæµç¨‹ã€‚\n",
    "\"\"\"\n",
    "\n",
    "# =====================================================================================\n",
    "# ã€ï¼ï¼ï¼å…³é”®ï¼ï¼ï¼ã€‘ç¯å¢ƒå˜é‡é…ç½® - å¿…é¡»åœ¨æ‰€æœ‰importä¹‹å‰\n",
    "# =====================================================================================\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"DISABLE_IPEX\"] = \"1\"\n",
    "os.environ[\"DEEPSPEED_XPU_ENABLE\"] = \"0\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# =====================================================================================\n",
    "# å¯¼å…¥æ‰€æœ‰å¿…éœ€çš„åº“ (æ‰€æœ‰importéƒ½åº”åœ¨æ­¤å¤„)\n",
    "# =====================================================================================\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from modelscope import snapshot_download\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, # <--- ä»transformersåº“å¯¼å…¥æ ¸å¿ƒçš„æ¨¡å‹ç±»\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel # <--- ä»peftåº“å¯¼å…¥LoRAç›¸å…³å·¥å…·\n",
    "\n",
    "# =====================================================================================\n",
    "# ã€ï¼ï¼ï¼æ ¸å¿ƒé‡ç‚¹ï¼ï¼ï¼ã€‘å…¨å±€é…ç½®ä¸è¶…å‚æ•°\n",
    "# =====================================================================================\n",
    "\n",
    "# --- è·¯å¾„é…ç½® (Path Configuration) ---\n",
    "# è„šæœ¬ä¼šæŒ‰ç…§è¿™äº›è·¯å¾„åˆ›å»ºæ–‡ä»¶å¤¹ã€è¯»å–æ•°æ®å’Œä¿å­˜æ¨¡å‹ã€‚\n",
    "BASE_DIR = \".\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "FORMATTED_DATA_DIR = os.path.join(BASE_DIR, \"data_formatted\")\n",
    "MODEL_CACHE_DIR = os.path.join(BASE_DIR, \"model_cache\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"output\")\n",
    "CHECKPOINTS_DIR = os.path.join(OUTPUT_DIR, \"checkpoints\") # è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ£€æŸ¥ç‚¹ï¼ˆä¸´æ—¶å­˜æ¡£ï¼‰\n",
    "FINAL_ADAPTER_DIR = os.path.join(OUTPUT_DIR, \"final_model\") # ã€æ³¨æ„ã€‘ç°åœ¨ä¿å­˜çš„æ˜¯å®Œæ•´æ¨¡å‹ï¼Œæ–‡ä»¶å¤¹åå·²ä¿®æ”¹\n",
    "\n",
    "# --- æ¨¡å‹é…ç½® (Model Configuration) ---\n",
    "MODEL_ID = \"qwen/Qwen3-1.7B\" # ä½ é€‰æ‹©çš„åŸºç¡€æ¨¡å‹IDï¼Œè„šæœ¬ä¼šè‡ªåŠ¨ä»ç½‘ä¸Šä¸‹è½½\n",
    "\n",
    "# ã€ç³»ç»ŸæŒ‡ä»¤ SYSTEM_PROMPTã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šç»™æ¨¡å‹è®¾å®šçš„â€œäººè®¾â€æˆ–â€œè¡Œä¸ºå‡†åˆ™â€ï¼Œå‘Šè¯‰å®ƒåº”è¯¥æ‰®æ¼”ä»€ä¹ˆè§’è‰²ã€‚\n",
    "#   - ã€ï¼ï¼ï¼æ ¸å¿ƒé‡ç‚¹ï¼ï¼ï¼ã€‘è¿™é‡Œçš„PROMPTå¿…é¡»ä¸ä½ è¯„ä¼°è„šæœ¬ä¸­ä½¿ç”¨çš„SYSTEM_PROMPTå®Œå…¨ä¸€è‡´ï¼\n",
    "#     è¿™ç›¸å½“äºè®­ç»ƒå’Œè€ƒæ ¸æ—¶ä½¿ç”¨åŒä¸€ä»½è¯´æ˜ä¹¦ï¼Œå¦åˆ™æ¨¡å‹ä¼šæ„Ÿåˆ°å›°æƒ‘ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å‡†ç¡®ã€‚\n",
    "SYSTEM_PROMPT = \"\"\"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„AIåŒ»å­¦åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·æå‡ºçš„é—®é¢˜ï¼Œæä¾›å‡†ç¡®ã€æ˜“æ‡‚ä¸”å…·æœ‰å®‰å…¨æç¤ºçš„å¥åº·ä¿¡æ¯ã€‚è¯·è®°ä½ï¼Œä½ çš„å›ç­”ä¸èƒ½æ›¿ä»£æ‰§ä¸šåŒ»å¸ˆçš„è¯Šæ–­ï¼Œå¿…é¡»åœ¨å›ç­”ç»“å°¾å¤„å£°æ˜è¿™ä¸€ç‚¹ã€‚\"\"\"\n",
    "\n",
    "# ã€æœ€å¤§é•¿åº¦ MAX_LENGTHã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šä¸€æ¡è®­ç»ƒæ•°æ®ï¼ˆé—®é¢˜+ç­”æ¡ˆï¼‰è¢«è½¬æ¢æˆæ•°å­—åï¼Œå…è®¸çš„æœ€å¤§é•¿åº¦ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆæ˜¯2048ï¼šå¯¹äºQwen3-1.7Bæ¨¡å‹æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¯”è¾ƒå‡è¡¡çš„å€¼ï¼Œèƒ½å®¹çº³å¤§éƒ¨åˆ†çš„é—®ç­”å¯¹ã€‚\n",
    "#   - æ”¹äº†ä¼šæ€æ ·ï¼šå¦‚æœä½ çš„é—®ç­”æ–‡æœ¬éå¸¸é•¿ï¼Œéœ€è¦é€‚å½“å¢åŠ è¿™ä¸ªå€¼ï¼ˆå¦‚4096ï¼‰ï¼Œä½†è¿™ä¼šæ˜¾è‘—å¢åŠ æ˜¾å­˜æ¶ˆè€—ã€‚å¦‚æœè®¾ç½®å¤ªå°ï¼Œè¿‡é•¿çš„æ–‡æœ¬ä¼šè¢«æˆªæ–­ï¼Œå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ã€‚\n",
    "MAX_LENGTH = 2048\n",
    "\n",
    "# --- ã€ï¼ï¼ï¼æ ¸å¿ƒé‡ç‚¹ï¼ï¼ï¼ã€‘è®­ç»ƒè¶…å‚æ•° (Hyperparameters) ---\n",
    "# è¿™äº›å‚æ•°ç›´æ¥å½±å“æ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€é€Ÿåº¦å’Œèµ„æºæ¶ˆè€—ï¼Œæ˜¯å¾®è°ƒçš„â€œçµé­‚â€ã€‚\n",
    "\n",
    "# ã€å­¦ä¹ ç‡ Learning Rateã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šå¯ä»¥ç†è§£ä¸ºæ¨¡å‹å­¦ä¹ æ—¶â€œæ¯ä¸€æ­¥èµ°å¤šå¤§â€ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆæ˜¯1e-4ï¼šå¯¹äºLoRAè¿™ç§åªè®­ç»ƒä¸€å°éƒ¨åˆ†å‚æ•°çš„æ–¹æ³•ï¼Œå¯ä»¥è®¾ç½®æ¯”å…¨é‡å¾®è°ƒï¼ˆé€šå¸¸æ˜¯2e-5ï¼‰ç¨å¤§çš„å­¦ä¹ ç‡ï¼Œè®©â€œé€‚é…å™¨â€å‚æ•°å­¦å¾—æ›´å¿«ã€‚1e-4æ˜¯QLoRAä¸€ä¸ªç»è¿‡å¤§é‡éªŒè¯çš„ã€æ•ˆæœå¾ˆå¥½çš„èµ·å§‹å€¼ã€‚\n",
    "#   - æ”¹äº†ä¼šæ€æ ·ï¼šå¤ªé«˜ï¼ˆå¦‚1e-3ï¼‰å¯èƒ½å¯¼è‡´æ¨¡å‹â€œè·‘åâ€ï¼Œè®­ç»ƒä¸ç¨³å®šï¼›å¤ªä½ï¼ˆå¦‚1e-5ï¼‰å­¦ä¹ ä¼šéå¸¸æ…¢ï¼Œç”šè‡³â€œå­¦ä¸åŠ¨â€ã€‚\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# ã€å•è®¾å¤‡æ‰¹é‡å¤§å° PER_DEVICE_TRAIN_BATCH_SIZEã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šä¸€æ¬¡æ€§å–‚ç»™GPUå¤šå°‘æ¡æ•°æ®è¿›è¡Œè®­ç»ƒã€‚\n",
    "#   - ä¸ºä»€ä¹ˆæ˜¯2ï¼šè¿™æ˜¯ä¸ºäº†åœ¨æœ‰é™çš„æ˜¾å­˜ï¼ˆå¦‚24GBï¼‰ä¸‹è¿è¡Œè€Œåšçš„ä¼˜åŒ–ã€‚å€¼è¶Šå°ï¼Œå•æ¬¡è®¡ç®—çš„æ˜¾å­˜å ç”¨è¶Šä½ã€‚\n",
    "#   - æ”¹äº†ä¼šæ€æ ·ï¼šå€¼è¶Šå¤§ï¼Œè®­ç»ƒé€Ÿåº¦è¶Šå¿«ï¼Œä½†æ˜¾å­˜å ç”¨è¶Šé«˜ã€‚å¦‚æœè®¾ç½®è¿‡å¤§ï¼Œä¼šå¯¼è‡´â€œCUDA out of memoryâ€é”™è¯¯ã€‚\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 1\n",
    "\n",
    "# ã€æ¢¯åº¦ç´¯ç§¯æ­¥æ•° GRADIENT_ACCUMULATION_STEPSã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šé€šè¿‡â€œæ”’â€æ¢¯åº¦çš„æ–¹å¼ï¼Œåœ¨ä¸å¢åŠ æ˜¾å­˜çš„æƒ…å†µä¸‹ï¼Œå®ç°ç­‰æ•ˆäºæ›´å¤§Batch Sizeçš„è®­ç»ƒæ•ˆæœã€‚\n",
    "#   - ä¸ºä»€ä¹ˆæ˜¯16ï¼šé…åˆä¸Šé¢çš„`BATCH_SIZE = 2`ï¼Œæˆ‘ä»¬çš„â€œæœ‰æ•ˆæ‰¹é‡å¤§å°â€ (Effective Batch Size) = 2 * 16 = 32ã€‚è¿™é€šå¸¸æ˜¯ä¸€ä¸ªèƒ½è®©æ¨¡å‹ç¨³å®šå­¦ä¹ çš„æ‰¹é‡å¤§å°ã€‚\n",
    "#   - æ”¹äº†ä¼šæ€æ ·ï¼šå®ƒå’Œ`BATCH_SIZE`æ˜¯è··è··æ¿å…³ç³»ã€‚æ˜¾å­˜ä¸è¶³æ—¶ï¼Œé™ä½`BATCH_SIZE`ï¼ŒåŒæ—¶æé«˜æ­¤å€¼ï¼Œä»¥ä¿æŒâ€œæœ‰æ•ˆæ‰¹é‡å¤§å°â€ä¸å˜ã€‚\n",
    "GRADIENT_ACCUMULATION_STEPS = 32\n",
    "\n",
    "# ã€è®­ç»ƒè½®æ•° NUM_TRAIN_EPOCHSã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šå°†æ•´ä¸ªè®­ç»ƒæ•°æ®é›†ä»å¤´åˆ°å°¾å®Œæ•´åœ°å­¦ä¹ å¤šå°‘éã€‚\n",
    "#   - ä¸ºä»€ä¹ˆæ˜¯2ï¼šå¯¹äºå¾®è°ƒä»»åŠ¡ï¼Œé€šå¸¸1-3ä¸ªepochå°±è¶³å¤Ÿäº†ï¼Œå¯ä»¥å¿«é€Ÿçœ‹åˆ°æ•ˆæœã€‚\n",
    "#   - æ”¹äº†ä¼šæ€æ ·ï¼šå¤ªå°‘å¯èƒ½å¯¼è‡´æ¨¡å‹â€œæ²¡å­¦ä¼šâ€ï¼›å¤ªå¤šåˆ™å¯èƒ½å‘ç”Ÿâ€œè¿‡æ‹Ÿåˆâ€ï¼ˆæ¨¡å‹åªä¼šæ­»è®°ç¡¬èƒŒè®­ç»ƒæ•°æ®ï¼Œä¸§å¤±äº†æ³›åŒ–èƒ½åŠ›ï¼‰ã€‚\n",
    "NUM_TRAIN_EPOCHS = 2\n",
    "\n",
    "EVAL_STEPS = 20\n",
    "SAVE_STEPS = 400\n",
    "LOGGING_STEPS = 10\n",
    "\n",
    "# --- ã€ï¼ï¼ï¼æ ¸å¿ƒé‡ç‚¹ï¼ï¼ï¼ã€‘LoRAé…ç½® (LoRA Configuration) ---\n",
    "\n",
    "# ã€LoRAç§© Rã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šLoRAâ€œå‡çº§å¥—ä»¶â€çš„â€œå¤æ‚åº¦â€æˆ–â€œå¤§å°â€ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆæ˜¯16ï¼šRå€¼é€šå¸¸åœ¨8, 16, 32, 64ä¸­é€‰æ‹©ã€‚16æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·å§‹ç‚¹ï¼Œåœ¨æ€§èƒ½å’Œå‚æ•°é‡ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚\n",
    "#   - æ”¹äº†ä¼šæ€æ ·ï¼šRå€¼è¶Šå¤§ï¼Œå¯è®­ç»ƒçš„å‚æ•°è¶Šå¤šï¼Œç†è®ºä¸Šæ‹Ÿåˆèƒ½åŠ›è¶Šå¼ºï¼Œä½†æ˜¾å­˜å ç”¨ä¹Ÿè¶Šå¤§ï¼Œä¸”è¿‡å¤§ä¹Ÿå¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆã€‚\n",
    "LORA_R = 16\n",
    "\n",
    "# ã€LoRAç¼©æ”¾å› å­ Alphaã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šä¸€ä¸ªç¼©æ”¾å‚æ•°ï¼Œç”¨æ¥è°ƒæ•´LoRAæƒé‡çš„å¹…åº¦ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆæ˜¯32ï¼šé€šå¸¸å»ºè®®è®¾ç½®ä¸ºRçš„ä¸¤å€ï¼ˆ`alpha = 2 * r`ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»éªŒæ€§çš„ã€æ•ˆæœå¾ˆå¥½çš„è®¾ç½®ã€‚\n",
    "#   - æ”¹äº†ä¼šæ€æ ·ï¼šå®ƒä¸å­¦ä¹ ç‡æœ‰ç±»ä¼¼çš„ä½œç”¨ï¼Œè°ƒæ•´å®ƒä¼šå½±å“LoRAæ¨¡å—çš„æƒé‡å¤§å°ã€‚ä¿æŒ`2*r`çš„æ¯”ä¾‹é€šå¸¸æ˜¯æœ€ä½³å®è·µã€‚\n",
    "LORA_ALPHA = 32\n",
    "\n",
    "LORA_DROPOUT = 0.05 # é˜²æ­¢è¿‡æ‹Ÿåˆçš„æŠ€æœ¯ï¼Œè®­ç»ƒæ—¶éšæœºâ€œä¸¢å¼ƒâ€ä¸€äº›ç¥ç»å…ƒã€‚\n",
    "\n",
    "# ã€ç›®æ ‡æ¨¡å— TARGET_MODULESã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šå‘Šè¯‰LoRAåº”è¯¥åœ¨æ¨¡å‹çš„å“ªäº›éƒ¨åˆ†â€œåŠ è£…å‡çº§å¥—ä»¶â€ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆæ˜¯è¿™å‡ ä¸ªï¼š`q_proj`, `k_proj`, `v_proj`, `o_proj`æ˜¯Transformeræ¨¡å‹ä¸­æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒç»„ä»¶ï¼Œå¯¹å®ƒä»¬è¿›è¡Œå¾®è°ƒé€šå¸¸æ•ˆæœæœ€æ˜¾è‘—ã€‚\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "# --- è¿è¡Œé…ç½® ---\n",
    "EFFECTIVE_BATCH_SIZE = PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "RUN_NAME = f\"qwen-medical-qlora-lr{LEARNING_RATE}-bs{EFFECTIVE_BATCH_SIZE}-r{LORA_R}\"\n",
    "\n",
    "# =====================================================================================\n",
    "# å·¥å…·å‡½æ•° (æ— éœ€ä¿®æ”¹)\n",
    "# =====================================================================================\n",
    "\n",
    "def check_environment():\n",
    "    # ... (æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹)\n",
    "    if not torch.cuda.is_available(): raise RuntimeError(\"âŒ CUDAä¸å¯ç”¨ï¼\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"âœ… CUDAç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼\")\n",
    "\n",
    "def setup_directories():\n",
    "    # ... (æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹)\n",
    "    for directory in [DATA_DIR, FORMATTED_DATA_DIR, MODEL_CACHE_DIR, CHECKPOINTS_DIR, FINAL_ADAPTER_DIR]:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    print(\"âœ… æ‰€æœ‰é¡¹ç›®ç›®å½•å·²å‡†å¤‡å°±ç»ª\")\n",
    "\n",
    "def dataset_jsonl_transfer(origin_path, new_path):\n",
    "    # ... (æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹)\n",
    "    messages = []\n",
    "    with open(origin_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                messages.append({\n",
    "                    \"input\": data[\"question\"],\n",
    "                    \"output\": f'<|FunctionCallBegin|>{data[\"think\"]}<|FunctionCallEnd|>\\n{data[\"answer\"]}'\n",
    "                })\n",
    "            except (KeyError, json.JSONDecodeError): continue\n",
    "    with open(new_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for message in messages:\n",
    "            file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"âœ… æ•°æ®è½¬æ¢å®Œæˆ: {len(messages)} æ¡æœ‰æ•ˆæ•°æ®\")\n",
    "\n",
    "def process_func(example, tokenizer):\n",
    "    # ... (æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹)\n",
    "    instruction = tokenizer(f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n<|im_start|>user\\n{example['input']}<|im_end|>\\n<|im_start|>assistant\\n\", add_special_tokens=False)\n",
    "    response = tokenizer(example['output'], add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": [1] * len(input_ids), \"labels\": labels}\n",
    "\n",
    "# =====================================================================================\n",
    "# ã€ï¼ï¼ï¼æ ¸å¿ƒé‡ç‚¹ï¼ï¼ï¼ã€‘ä¸»è®­ç»ƒæµç¨‹\n",
    "# =====================================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»è®­ç»ƒæµç¨‹\"\"\"\n",
    "    print(\"ğŸš€ å¼€å§‹QLoRAåŒ»å­¦é—®ç­”å¾®è°ƒæµç¨‹...\")\n",
    "    \n",
    "    # --- æ­¥éª¤1: ç¯å¢ƒä¸è·¯å¾„åˆå§‹åŒ– ---\n",
    "    check_environment()\n",
    "    setup_directories()\n",
    "\n",
    "    # --- æ­¥éª¤2: åŠ è½½åˆ†è¯å™¨å¹¶æ‰©å±• ---\n",
    "    print(\"\\nğŸ“¥ æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹çš„åˆ†è¯å™¨...\")\n",
    "    model_dir = snapshot_download(MODEL_ID, cache_dir=MODEL_CACHE_DIR, revision=\"master\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True)\n",
    "    \n",
    "    # ã€å…³é”®ã€‘æ·»åŠ æ–°çš„ç‰¹æ®Šè¯æ±‡ï¼Œè®©æ¨¡å‹èƒ½â€œå¬æ‡‚â€æˆ‘ä»¬çš„ç‰¹æ®ŠæŒ‡ä»¤\n",
    "    special_tokens_dict = {'additional_special_tokens': ['<|FunctionCallBegin|>', '<|FunctionCallEnd|>']}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"âœ… åˆ†è¯å™¨åŠ è½½å¹¶æ‰©å±•å®Œæˆ\")\n",
    "\n",
    "    # --- æ­¥éª¤3: é…ç½®QLoRAé‡åŒ– ---\n",
    "    # è¿™æ˜¯å®ç°â€œä½æ˜¾å­˜â€å¾®è°ƒçš„æ ¸å¿ƒã€‚å®ƒå°†æ¨¡å‹å‚æ•°ä»32ä½æµ®ç‚¹æ•°å‹ç¼©åˆ°4ä½æ•´æ•°ï¼Œå¤§å¤§å‡å°‘æ˜¾å­˜å ç”¨ã€‚\n",
    "    print(\"\\nâš™ï¸ é…ç½®4-bité‡åŒ–å‚æ•°(QLoRA)...\")\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    # --- æ­¥éª¤4: åŠ è½½é‡åŒ–æ¨¡å‹ ---\n",
    "    print(\"\\nğŸ”§ åŠ è½½é‡åŒ–åçš„åŸºç¡€æ¨¡å‹...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # ä¼˜å…ˆå°è¯•ä½¿ç”¨Flash Attention 2ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—æ–¹æ³•ï¼Œå¯ä»¥æé€Ÿå¹¶èŠ‚çœæ˜¾å­˜ã€‚\n",
    "    # å¦‚æœç¯å¢ƒä¸æ”¯æŒï¼Œä¼šè‡ªåŠ¨åˆ‡æ¢åˆ°æ ‡å‡†çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dir, device_map=\"auto\", torch_dtype=torch.bfloat16,\n",
    "            quantization_config=quantization_config,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "        )\n",
    "        print(\"âœ… é‡åŒ–æ¨¡å‹åŠ è½½å®Œæˆ (Flash Attention 2 å·²å¯ç”¨)\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Flash Attention 2ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶: {e}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dir, device_map=\"auto\", torch_dtype=torch.bfloat16,\n",
    "            quantization_config=quantization_config,\n",
    "        )\n",
    "        print(\"âœ… é‡åŒ–æ¨¡å‹åŠ è½½å®Œæˆ (æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶)\")\n",
    "    \n",
    "    # ã€å…³é”®ã€‘è°ƒæ•´æ¨¡å‹è¯åµŒå…¥å±‚çš„å¤§å°ï¼Œä»¥åŒ¹é…æˆ‘ä»¬æ‰©å±•åçš„åˆ†è¯å™¨ã€‚è¿™æ˜¯é¿å…åç»­å°ºå¯¸ä¸åŒ¹é…é”™è¯¯çš„æ ¸å¿ƒæ­¥éª¤ã€‚\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # --- æ­¥éª¤5: åº”ç”¨LoRAé€‚é…å™¨ ---\n",
    "    lora_config = LoraConfig(\n",
    "        r=LORA_R, lora_alpha=LORA_ALPHA, target_modules=TARGET_MODULES,\n",
    "        lora_dropout=LORA_DROPOUT, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters() # æ‰“å°å‡ºå¯è®­ç»ƒå‚æ•°çš„æ¯”ä¾‹ï¼Œä½ ä¼šå‘ç°å®ƒéå¸¸å°ï¼\n",
    "    print(\"âœ… LoRAé€‚é…å™¨å·²æˆåŠŸåº”ç”¨\")\n",
    "\n",
    "    # --- æ­¥éª¤6 & 7: æ•°æ®å‡†å¤‡ä¸é¢„å¤„ç† ---\n",
    "    # (è¿™éƒ¨åˆ†ä»£ç æ— éœ€ä¿®æ”¹ï¼Œå®ƒä¼šè‡ªåŠ¨å¤„ç†æ•°æ®æ ¼å¼è½¬æ¢å’ŒTokenization)\n",
    "    print(\"\\nğŸ“Š å‡†å¤‡å¹¶é¢„å¤„ç†æ•°æ®...\")\n",
    "    train_original_path = os.path.join(DATA_DIR, \"train.jsonl\")\n",
    "    val_original_path = os.path.join(DATA_DIR, \"val.jsonl\")\n",
    "    train_formatted_path = os.path.join(FORMATTED_DATA_DIR, \"train_formatted.jsonl\")\n",
    "    val_formatted_path = os.path.join(FORMATTED_DATA_DIR, \"val_formatted.jsonl\")\n",
    "    if not os.path.exists(train_formatted_path): dataset_jsonl_transfer(train_original_path, train_formatted_path)\n",
    "    if not os.path.exists(val_formatted_path): dataset_jsonl_transfer(val_original_path, val_formatted_path)\n",
    "    train_dataset = Dataset.from_pandas(pd.read_json(train_formatted_path, lines=True))\n",
    "    eval_dataset = Dataset.from_pandas(pd.read_json(val_formatted_path, lines=True))\n",
    "    tokenized_train_dataset = train_dataset.map(lambda x: process_func(x, tokenizer), remove_columns=train_dataset.column_names)\n",
    "    tokenized_eval_dataset = eval_dataset.map(lambda x: process_func(x, tokenizer), remove_columns=eval_dataset.column_names)\n",
    "    print(\"âœ… æ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†å®Œæˆ\")\n",
    "\n",
    "    # --- æ­¥éª¤8 & 9: é…ç½®è®­ç»ƒå‚æ•°å¹¶åˆå§‹åŒ–Trainer ---\n",
    "    print(\"\\nâš™ï¸ é…ç½®è®­ç»ƒå‚æ•°å¹¶åˆå§‹åŒ–è®­ç»ƒå™¨...\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=os.path.join(CHECKPOINTS_DIR, RUN_NAME),\n",
    "        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.1,\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=EVAL_STEPS,\n",
    "        save_steps=SAVE_STEPS,\n",
    "        save_total_limit=2,\n",
    "        bf16=True, # ä½¿ç”¨bf16æ··åˆç²¾åº¦è®­ç»ƒï¼Œå¯ä»¥æé€Ÿå¹¶èŠ‚çœæ˜¾å­˜\n",
    "        gradient_checkpointing=True, # å…³é”®çš„æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œç”¨æ—¶é—´æ¢ç©ºé—´\n",
    "        optim=\"paged_adamw_8bit\", # ä½¿ç”¨åˆ†é¡µä¼˜åŒ–å™¨ï¼Œè¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜\n",
    "        remove_unused_columns=False,\n",
    "        #report_to=\"none\", # ç¦ç”¨å¤–éƒ¨æ—¥å¿—ä¸ŠæŠ¥\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model, args=training_args, train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_eval_dataset,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)\n",
    "    )\n",
    "    print(\"âœ… è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ\")\n",
    "\n",
    "    # --- æ­¥éª¤10: å¼€å§‹è®­ç»ƒ ---\n",
    "    print(\"\\nğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...\")\n",
    "    trainer.train()\n",
    "    print(\"ğŸ‰ è®­ç»ƒå®Œæˆ!\")\n",
    "\n",
    "    # --- ã€ï¼ï¼ï¼æ ¸å¿ƒé‡ç‚¹ï¼šåˆå¹¶æƒé‡å¹¶ä¿å­˜ä¸ºå®Œæ•´æ¨¡å‹ï¼ï¼ï¼ã€‘ ---\n",
    "    print(\"\\nğŸ”— æ­£åœ¨å°†LoRAé€‚é…å™¨æƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­ï¼Œä»¥ç”Ÿæˆå¯ç‹¬ç«‹éƒ¨ç½²çš„å®Œæ•´æ¨¡å‹...\")\n",
    "    \n",
    "    # é‡Šæ”¾æ˜¾å­˜ï¼Œä¸ºåŠ è½½å…¨ç²¾åº¦æ¨¡å‹åšå‡†å¤‡\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # é‡æ–°åŠ è½½å…¨ç²¾åº¦çš„åŸºç¡€æ¨¡å‹\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir, torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True\n",
    "    )\n",
    "    # å†æ¬¡æ‰©å±•è¯æ±‡è¡¨ï¼Œç¡®ä¿ç»“æ„ä¸€è‡´\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # ä»æœ€æ–°çš„æ£€æŸ¥ç‚¹åŠ è½½æˆ‘ä»¬è®­ç»ƒå¥½çš„LoRAé€‚é…å™¨æƒé‡\n",
    "    lora_model_path = os.path.join(CHECKPOINTS_DIR, RUN_NAME)\n",
    "    latest_checkpoint = max([d for d in os.listdir(lora_model_path) if d.startswith(\"checkpoint-\")], key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    adapter_path = os.path.join(lora_model_path, latest_checkpoint)\n",
    "    print(f\"    -> æ­£åœ¨ä»æœ€æ–°çš„æ£€æŸ¥ç‚¹åŠ è½½LoRAé€‚é…å™¨: {adapter_path}\")\n",
    "    \n",
    "    model_to_merge = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "    # æ‰§è¡Œåˆå¹¶ï¼è¿™å°†è¿”å›ä¸€ä¸ªå…¨æ–°çš„ã€å®Œæ•´çš„ã€å¾®è°ƒåçš„æ¨¡å‹\n",
    "    merged_model = model_to_merge.merge_and_unload()\n",
    "    print(\"âœ… LoRAæƒé‡åˆå¹¶å®Œæˆï¼\")\n",
    "    \n",
    "    # --- æ­¥éª¤11: ä¿å­˜æœ€ç»ˆçš„å®Œæ•´æ¨¡å‹ ---\n",
    "    print(\"\\nğŸ’¾ æ­£åœ¨ä¿å­˜åˆå¹¶åçš„å®Œæ•´æ¨¡å‹...\")\n",
    "    final_model_path = os.path.join(FINAL_ADAPTER_DIR, RUN_NAME)\n",
    "    \n",
    "    # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿æˆ‘ä»¬ä¿å­˜çš„æ˜¯`merged_model`ï¼Œè€Œä¸æ˜¯æ—§çš„`model`\n",
    "    merged_model.save_pretrained(final_model_path)\n",
    "    tokenizer.save_pretrained(final_model_path)\n",
    "    \n",
    "    print(f\"âœ… æœ€ç»ˆçš„â€œå®˜æ–¹æ”¹è£…ç‰ˆâ€æ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}\")\n",
    "    print(\"\\nğŸ‰ QLoRAåŒ»å­¦é—®ç­”å¾®è°ƒæµç¨‹å…¨éƒ¨å®Œæˆ!\")\n",
    "\n",
    "# =====================================================================================\n",
    "# ç¨‹åºå…¥å£\n",
    "# =====================================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        print(\"ğŸ”š ç¨‹åºç»“æŸ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
