{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eecc10e-863e-465c-a431-2419fa3d1f73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets rouge-score bert-score modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "254837ad-9bf1-487d-982d-a3f8bd272caf",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-08-27T08:26:13.921343Z",
     "iopub.status.busy": "2025-08-27T08:26:13.921009Z",
     "iopub.status.idle": "2025-08-27T08:33:16.799258Z",
     "shell.execute_reply": "2025-08-27T08:33:16.798841Z",
     "shell.execute_reply.started": "2025-08-27T08:26:13.921326Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ‰€æœ‰æ¨¡å‹å‡å·²å‡†å¤‡å°±ç»ªã€‚\n",
      "\n",
      "æ­£åœ¨ä» './data/val.jsonl' åŠ è½½å¹¶è½¬æ¢éªŒè¯æ•°æ®...\n",
      "    -> [è°ƒè¯•æ¨¡å¼] å·²æˆªå–å‰ 50 æ¡æ•°æ®ç”¨äºè¯„ä¼°ã€‚\n",
      "\n",
      "==================== å°è¯•é«˜æ•ˆå¹¶è¡Œæ¨¡å¼ (éœ€è¦è¾ƒå¤§æ˜¾å­˜) ====================\n",
      "\n",
      "æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ from 'qwen-medical-qlora-lr0.0001-bs32-r16'...\n",
      "âœ… æ¨¡å‹ 'qwen-medical-qlora-lr0.0001-bs32-r16' åŠ è½½å®Œæˆã€‚\n",
      "\n",
      "æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ from 'Qwen3-1.7B'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c2406aa8fa47e29d8ddd56fd46548a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹ 'Qwen3-1.7B' åŠ è½½å®Œæˆã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> æ­£åœ¨ç”¨ [å¾®è°ƒå] æ¨¡å‹é«˜é€Ÿç”Ÿæˆå›ç­”: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [03:26<00:00, 15.92s/it]\n",
      "  -> æ­£åœ¨ç”¨ [åŸºç¡€] æ¨¡å‹é«˜é€Ÿç”Ÿæˆå›ç­”: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [03:30<00:00, 16.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- æ­£åœ¨è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è®¡ç®— ROUGE/å…³é”®è¯/å®‰å…¨æ€§: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 1278.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "è®¡ç®— BERTScore (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…)...\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164d792dce7044fdae97296fb8a278c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4dbb4dfc804109a836628b58abce06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.48 seconds, 33.89 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c95d0b2af6d4beb8b92fd43f943ef13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b63fd52fdc4222ab6df95b6f94e9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.33 seconds, 37.48 sentences/sec\n",
      "âœ… é«˜æ•ˆå¹¶è¡Œæ¨¡å¼æ‰§è¡ŒæˆåŠŸï¼\n",
      "\n",
      "\n",
      "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰\n",
      "âœ… è¯„ä¼°å®Œæˆï¼è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³ 'evaluation_detailed_results.csv'\n",
      "   å¹³å‡åˆ†å¯¹æ¯”å·²ä¿å­˜è‡³ 'evaluation_summary_scores.json'\n",
      "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰\n",
      "\n",
      "=================================================================\n",
      "ğŸ“Š å¾®è°ƒæ¨¡å‹ vs åŸºç¡€æ¨¡å‹å¹³å‡æŒ‡æ ‡å¯¹æ¯” (è¶Šé«˜è¶Šå¥½ï¼Œé™¤å®‰å…¨æŒ‡æ ‡å¤–)\n",
      "=================================================================\n",
      "                        Finetuned    Base\n",
      "ROUGE-1                    0.2072  0.1956\n",
      "ROUGE-L                    0.1927  0.1749\n",
      "BERTScore F1               0.8285  0.8019\n",
      "Medical Keyword            1.5000  1.7200\n",
      "Harmful Keyword (è¶Šä½è¶Šå¥½)     0.1000  0.0400\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "åŒ»å­¦é—®ç­”æ¨¡å‹å¾®è°ƒæ•ˆæœè¯„ä¼°è„šæœ¬ - ã€V8.3 æœ€ç»ˆä¿®å¤ç‰ˆã€‘\n",
    "\n",
    "ã€æœ¬è„šæœ¬çš„ä½¿å‘½ã€‘\n",
    "æ¬¢è¿æ¥åˆ°æ¨¡å‹å¾®è°ƒçš„â€œæœŸæœ«è€ƒè¯•â€ç°åœºï¼è¿™ä»½è„šæœ¬æ˜¯æ‚¨æ£€éªŒè‡ªå·±å¾®è°ƒæˆæœçš„â€œé˜…å·è€å¸ˆâ€ã€‚\n",
    "æˆ‘ä»¬åšä¿¡ï¼Œè¯„ä¼°æ˜¯å¾®è°ƒé—­ç¯ä¸­è‡³å…³é‡è¦çš„ä¸€æ­¥ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸ºæ‚¨å‡†å¤‡äº†è¿™ä»½â€œä¿å§†çº§â€çš„è¯„ä¼°æŒ‡å—ã€‚\n",
    "\n",
    "æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ï¼š\n",
    "1.  ã€è½»æ¾ä¸Šæ‰‹ã€‘: æ‚¨åªéœ€ä¿®æ”¹å°‘æ•°å‡ ä¸ªè·¯å¾„å‚æ•°ï¼Œå°±èƒ½å¯¹æ‚¨çš„æ¨¡å‹è¿›è¡Œä¸€æ¬¡å…¨é¢çš„ã€å¤šç»´åº¦çš„â€œä½“æ£€â€ã€‚\n",
    "2.  ã€æ·±åº¦ç†è§£ã€‘: é€šè¿‡é˜…è¯»æ³¨é‡Šï¼Œæ‚¨èƒ½ç†è§£æ¯ç§è¯„ä¼°æŒ‡æ ‡ï¼ˆROUGE, BERTScoreç­‰ï¼‰çš„å«ä¹‰ï¼Œä»¥åŠä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å®ƒä»¬ã€‚\n",
    "3.  ã€ç§‘å­¦å¯¹æ¯”ã€‘: è„šæœ¬ä¼šè‡ªåŠ¨å¯¹æ¯”â€œå¾®è°ƒåçš„æ¨¡å‹â€ä¸â€œåŸå§‹åŸºç¡€æ¨¡å‹â€çš„è¡¨ç°ï¼Œè®©æ‚¨ç”¨æ•°æ®ç›´è§‚åœ°çœ‹åˆ°å¾®è°ƒå¸¦æ¥çš„æå‡ã€‚\n",
    "\n",
    "ã€æ ¸å¿ƒç‰¹æ€§ã€‘\n",
    "1.  [ç¨³å®šå¯é ] å½»åº•è§£å†³äº†æ–°æ‰‹åœ¨è¯„ä¼°æ—¶æœ€å¸¸é‡åˆ°çš„`size mismatch`ï¼ˆå°ºå¯¸ä¸åŒ¹é…ï¼‰å’Œæ•°æ®æ ¼å¼ä¸ä¸€è‡´çš„é”™è¯¯ã€‚\n",
    "2.  [å¤šç»´è¯„ä¼°] åŒæ—¶ä½¿ç”¨ç»å…¸çš„ROUGEåˆ†æ•°ã€æ›´æ™ºèƒ½çš„BERTScoreã€ä»¥åŠè‡ªå®šä¹‰çš„é¢†åŸŸå…³é”®è¯å’Œå®‰å…¨è¯æ£€æµ‹ï¼Œè¿›è¡Œå…¨æ–¹ä½è¯„ä¼°ã€‚\n",
    "3.  [æ™ºèƒ½æ‰§è¡Œ] è„šæœ¬ä¼šè‡ªåŠ¨æ£€æµ‹æ‚¨çš„æ˜¾å­˜å¤§å°ï¼Œå°è¯•ä½¿ç”¨é€Ÿåº¦æ›´å¿«çš„å¹¶è¡Œæ¨¡å¼ï¼Œå¦‚æœæ˜¾å­˜ä¸è¶³åˆ™è‡ªåŠ¨åˆ‡æ¢åˆ°æ›´ç¨³å¦¥çš„ä¸²è¡Œæ¨¡å¼ã€‚\n",
    "4.  [æè‡´æ³¨é‡Š] å¯èƒ½æ˜¯æ‚¨èƒ½æ‰¾åˆ°çš„ã€æ³¨é‡Šæœ€è¯¦å°½çš„ä¸­æ–‡è¯„ä¼°è„šæœ¬ï¼Œä¸“ä¸ºæ–°æ‰‹æ‰“é€ ã€‚\n",
    "\"\"\"\n",
    "\n",
    "# =====================================================================================\n",
    "# ã€ç¬¬é›¶æ­¥ï¼šå¯¼å…¥å·¥å…·åŒ…ã€‘ - å‡†å¤‡å¥½æˆ‘ä»¬éœ€è¦çš„â€œé˜…å·å·¥å…·â€\n",
    "# =====================================================================================\n",
    "# è¿™äº›æ˜¯è„šæœ¬è¿è¡Œæ‰€ä¾èµ–çš„å·¥å…·åŒ…ï¼Œå¥½æ¯”æ˜¯é˜…å·è€å¸ˆéœ€è¦ç”¨åˆ°çš„çº¢ç¬”ã€è®¡ç®—å™¨å’Œè¯„åˆ†æ ‡å‡†ã€‚\n",
    "import json\n",
    "import pandas as pd         # ç”¨äºè½»æ¾å¤„ç†å’Œå±•ç¤ºæœ€ç»ˆçš„è¯„ä¼°ç»“æœè¡¨æ ¼ã€‚\n",
    "import torch                # PyTorchï¼Œæ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæ¡†æ¶ã€‚\n",
    "from datasets import Dataset # Hugging Faceçš„datasetsåº“ï¼Œç”¨äºåŠ è½½æˆ‘ä»¬çš„éªŒè¯æ•°æ®ã€‚\n",
    "from transformers import GenerationConfig, AutoTokenizer, AutoModelForCausalLM # Hugging Faceçš„æ ¸å¿ƒå·¥å…·ï¼Œç”¨äºåŠ è½½æ¨¡å‹ã€åˆ†è¯å™¨å’Œé…ç½®ç”Ÿæˆå‚æ•°ã€‚\n",
    "import os                   # ç”¨äºå¤„ç†æ–‡ä»¶å’Œæ–‡ä»¶å¤¹è·¯å¾„ã€‚\n",
    "from rouge_score import rouge_scorer # ç”¨äºè®¡ç®—ROUGEåˆ†æ•°ï¼Œä¸€ç§è¡¡é‡æ–‡æœ¬ç›¸ä¼¼åº¦çš„ç»å…¸æŒ‡æ ‡ã€‚\n",
    "from bert_score import score       # ç”¨äºè®¡ç®—BERTScoreï¼Œä¸€ç§æ›´â€œèªæ˜â€çš„ã€èƒ½ç†è§£è¯­ä¹‰çš„ç›¸ä¼¼åº¦æŒ‡æ ‡ã€‚\n",
    "from modelscope.hub.snapshot_download import snapshot_download # ModelScopeçš„å·¥å…·ï¼Œç”¨äºè‡ªåŠ¨ä¸‹è½½ç¼ºå¤±çš„æ¨¡å‹ã€‚\n",
    "from tqdm import tqdm        # ä¸€ä¸ªéå¸¸å‹å¥½çš„è¿›åº¦æ¡å·¥å…·ï¼Œåœ¨å¤„ç†å¤§é‡æ•°æ®æ—¶è®©æˆ‘ä»¬èƒ½çœ‹åˆ°è¿›åº¦ã€‚\n",
    "import gc                   # Pythonçš„åƒåœ¾å›æ”¶æ¨¡å—ï¼Œç”¨äºåœ¨ä»£ç ä¸­æ‰‹åŠ¨é‡Šæ”¾å†…å­˜ã€‚\n",
    "\n",
    "# =====================================================================================\n",
    "# ã€ï¼ï¼ï¼æ ¸å¿ƒé‡ç‚¹ï¼ï¼ï¼ã€‘ç¬¬ä¸€æ­¥ï¼šå…¨å±€é…ç½® (Global Configuration)\n",
    "# =====================================================================================\n",
    "# è¿™æ˜¯æ•´ä¸ªè„šæœ¬çš„â€œæ§åˆ¶ä¸­å¿ƒâ€ï¼Œæ‰€æœ‰é‡è¦çš„ã€ä½ æœ€å¯èƒ½éœ€è¦ä¿®æ”¹çš„å‚æ•°éƒ½åœ¨è¿™é‡Œã€‚\n",
    "# è¯·åƒå¡«å†™ä¸€ä»½é‡è¦çš„è¡¨æ ¼ä¸€æ ·ï¼Œä»”ç»†é˜…è¯»å¹¶ç¡®è®¤æ¯ä¸€é¡¹ã€‚\n",
    "\n",
    "# --- ã€ï¼ï¼ï¼æ ¸å¿ƒé‡ç‚¹ï¼ï¼ï¼ã€‘ç³»ç»ŸæŒ‡ä»¤ä¸ç”Ÿæˆå‚æ•°é…ç½® ---\n",
    "\n",
    "# ã€ç³»ç»ŸæŒ‡ä»¤ SYSTEM_PROMPTã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼š   ç»™æ¨¡å‹è®¾å®šçš„â€œäººè®¾â€æˆ–â€œè¡Œä¸ºå‡†åˆ™â€ã€‚æ¨¡å‹åœ¨ç”Ÿæˆå›ç­”å‰ï¼Œä¼šé¦–å…ˆé˜…è¯»è¿™æ®µæŒ‡ä»¤ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆè¿™ä¹ˆå†™ï¼šä¸ºäº†è®©æ¨¡å‹æ‰®æ¼”ä¸€ä¸ªâ€œä¸“ä¸šã€ä¸¥è°¨çš„AIåŒ»å­¦åŠ©æ‰‹â€ï¼Œå¹¶å¼ºåˆ¶å…¶åœ¨å›ç­”æœ«å°¾åŠ ä¸Šå…è´£å£°æ˜ã€‚\n",
    "# ã€ï¼ï¼ï¼æ–°æ‰‹å¿…çœ‹ï¼ï¼ï¼ã€‘è¿™é‡Œçš„SYSTEM_PROMPTå¿…é¡»ä¸ä½ å¾®è°ƒï¼ˆè®­ç»ƒï¼‰è„šæœ¬ä¸­ä½¿ç”¨çš„å®Œå…¨ä¸€è‡´ï¼\n",
    "#   - ç±»æ¯”ï¼šè¿™ç›¸å½“äºè®­ç»ƒå’Œè€ƒè¯•ç”¨çš„æ˜¯åŒä¸€ä»½â€œè€ƒè¯•è¯´æ˜â€ï¼Œå¦‚æœè¯´æ˜å˜äº†ï¼Œæ¨¡å‹çš„è¡¨ç°å°±ä¼šå¤±å‡†ï¼Œè¯„ä¼°ä¹Ÿå°±å¤±å»äº†æ„ä¹‰ã€‚\n",
    "SYSTEM_PROMPT = \"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„AIåŒ»å­¦åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·æå‡ºçš„é—®é¢˜ï¼Œæä¾›å‡†ç¡®ã€æ˜“æ‡‚ä¸”å…·æœ‰å®‰å…¨æç¤ºçš„å¥åº·ä¿¡æ¯ã€‚è¯·è®°ä½ï¼Œä½ çš„å›ç­”ä¸èƒ½æ›¿ä»£æ‰§ä¸šåŒ»å¸ˆçš„è¯Šæ–­ï¼Œå¿…é¡»åœ¨å›ç­”ç»“å°¾å¤„å£°æ˜è¿™ä¸€ç‚¹ã€‚\"\n",
    "\n",
    "# ã€ç”Ÿæˆå‚æ•° GENERATION_CONFIG_PARAMSã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šæ§åˆ¶æ¨¡å‹å¦‚ä½•â€œè¯´è¯â€ï¼ˆç”Ÿæˆæ–‡æœ¬ï¼‰çš„ä¸€ç³»åˆ—â€œæ—‹éˆ•â€ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆè¿™ä¹ˆè®¾ç½®ï¼šåŒ»ç–—åœºæ™¯è¦æ±‚å›ç­”ç²¾å‡†ã€ç¨³å®šï¼Œä¸åº”å¤©é©¬è¡Œç©ºã€‚å› æ­¤å‚æ•°è®¾ç½®åå‘äºâ€œç¡®å®šæ€§â€è€Œéâ€œåˆ›é€ æ€§â€ã€‚\n",
    "GENERATION_CONFIG_PARAMS = {\n",
    "    \"max_new_tokens\": 512,      # ã€æœ€å¤§ç”Ÿæˆé•¿åº¦ã€‘æ¨¡å‹ä¸€æ¬¡å›ç­”æœ€å¤šèƒ½â€œè¯´â€å¤šå°‘ä¸ªå­—ï¼ˆtokenï¼‰ã€‚å¦‚æœè®¾ç½®å¾—å¤ªå°ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹çš„å›ç­”è¢«â€œè…°æ–©â€ã€‚\n",
    "\n",
    "    # ã€æ¸©åº¦ Temperatureã€‘\n",
    "    #   - æ˜¯ä»€ä¹ˆï¼šæ§åˆ¶å›ç­”çš„â€œåˆ›é€ æ€§â€æˆ–â€œéšæœºæ€§â€çš„æ—‹é’®ã€‚å®ƒå½±å“ç€æ¨¡å‹åœ¨é€‰æ‹©ä¸‹ä¸€ä¸ªè¯æ—¶çš„ç­–ç•¥ã€‚\n",
    "    #   - ä¸ºä»€ä¹ˆæ˜¯0.3ï¼šå€¼è¶Šä½ï¼ˆå¦‚0.1-0.3ï¼‰ï¼Œæ¨¡å‹è¶Šå€¾å‘äºé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯ï¼Œå›ç­”å°±è¶Šç¡®å®šã€é‡å¤æ€§è¶Šé«˜ã€‚è¿™éå¸¸é€‚åˆéœ€è¦äº‹å®å‡†ç¡®æ€§çš„åœºæ™¯ï¼ˆå¦‚åŒ»ç–—ã€æ³•å¾‹ï¼‰ã€‚\n",
    "    #               å€¼è¶Šé«˜ï¼ˆå¦‚0.7-1.0ï¼‰ï¼Œæ¨¡å‹ä¼šå¼•å…¥æ›´å¤šéšæœºæ€§ï¼Œå›ç­”å°±è¶Šæœ‰åˆ›æ„ã€è¶Šå¤šæ ·ï¼Œé€‚åˆå†™æ•…äº‹ã€æƒ³ç‚¹å­ã€‚\n",
    "    #   - æ”¹äº†ä¼šæ€æ ·ï¼šåœ¨åŒ»ç–—é—®ç­”ä¸­ï¼Œè°ƒé«˜æ¸©åº¦å¯èƒ½ä¼šè®©æ¨¡å‹ç»™å‡ºä¸€äº›å¬èµ·æ¥æ–°é¢–ä½†ä¸å‡†ç¡®çš„å»ºè®®ï¼Œè¿™æ˜¯éå¸¸å±é™©çš„ã€‚\n",
    "    \"temperature\": 0.3,\n",
    "\n",
    "    \"do_sample\": True,          # ã€æ˜¯å¦é‡‡æ ·ã€‘è¿™ä¸ªå¼€å…³å¿…é¡»æ‰“å¼€ï¼Œä¸Šé¢çš„temperatureå’Œä¸‹é¢çš„top_pç­‰å‚æ•°æ‰èƒ½ç”Ÿæ•ˆã€‚å¦‚æœå…³é—­ï¼Œæ¨¡å‹åªä¼šæœºæ¢°åœ°é€‰æ‹©æ¯ä¸€æ­¥æ¦‚ç‡æœ€é«˜çš„è¯ï¼ˆè¿™ç§æ–¹å¼è¢«ç§°ä¸ºâ€œè´ªå¿ƒæœç´¢â€ï¼‰ã€‚\n",
    "\n",
    "    # ã€Top-p (æ ¸å¿ƒé‡‡æ · Nucleus Sampling)ã€‘\n",
    "    #   - æ˜¯ä»€ä¹ˆï¼šå¦ä¸€ç§æ§åˆ¶éšæœºæ€§çš„æ–¹æ³•ï¼Œæ¯”æ¸©åº¦æ›´æ™ºèƒ½ã€‚æ¨¡å‹ä¼šä»ä¸€ä¸ªæ¦‚ç‡æ€»å’Œåˆšå¥½è¶…è¿‡top_pçš„â€œå€™é€‰è¯æ±‡è¡¨â€ä¸­è¿›è¡ŒæŠ½æ ·ã€‚\n",
    "    #   - ä¸ºä»€ä¹ˆæ˜¯0.8ï¼šè®¾ç½®ä¸º0.8æ„å‘³ç€æ¨¡å‹ä¼šè€ƒè™‘æ¦‚ç‡æœ€é«˜çš„é‚£äº›è¯ï¼Œç›´åˆ°å®ƒä»¬çš„æ¦‚ç‡æ€»å’Œè¾¾åˆ°80%ï¼Œç„¶åä»è¿™ä¸ªâ€œå€™é€‰åå•â€ä¸­éšæœºé€‰ä¸€ä¸ªã€‚è¿™å¯ä»¥åœ¨ä¿è¯å›ç­”è´¨é‡ï¼ˆæ’é™¤äº†é‚£äº›éå¸¸ä¸é è°±çš„è¯ï¼‰çš„åŒæ—¶ï¼Œä¿ç•™ä¸€å®šçš„çµæ´»æ€§ï¼Œé˜²æ­¢å›ç­”è¿‡äºæ­»æ¿ã€‚\n",
    "    #   - æ”¹äº†ä¼šæ€æ ·ï¼šè°ƒä½ï¼ˆå¦‚0.5ï¼‰ä¼šè®©å›ç­”æ›´ä¿å®ˆã€æ›´èšç„¦äºé«˜é¢‘è¯ï¼›è°ƒé«˜ï¼ˆå¦‚0.95ï¼‰åˆ™ä¼šç»™æ¨¡å‹æ›´å¤§çš„é€‰æ‹©ç©ºé—´ï¼Œå›ç­”æ›´å¤šæ ·ã€‚\n",
    "    \"top_p\": 0.8,\n",
    "\n",
    "    # ã€é‡å¤æƒ©ç½š Repetition Penaltyã€‘\n",
    "    #   - æ˜¯ä»€ä¹ˆï¼šä¸€ä¸ªå¤§äº1çš„æ•°å€¼ï¼Œç”¨äºç»™é‚£äº›å·²ç»å‡ºç°è¿‡çš„è¯â€œé™æƒâ€ï¼Œé™ä½å®ƒä»¬å†æ¬¡è¢«é€‰æ‹©çš„æ¦‚ç‡ã€‚\n",
    "    #   - ä¸ºä»€ä¹ˆæ˜¯1.1ï¼šè½»å¾®åœ°ï¼ˆ10%ï¼‰æƒ©ç½šé‡å¤è¯è¯­ï¼Œå¯ä»¥æœ‰æ•ˆé¿å…æ¨¡å‹é™·å…¥â€œæˆ‘çŸ¥é“äº†ï¼Œæˆ‘çŸ¥é“äº†ï¼Œæˆ‘çŸ¥é“äº†â€è¿™æ ·çš„â€œå¤è¯»æœºâ€æ¨¡å¼ï¼Œè®©å›ç­”æ›´è‡ªç„¶ã€ç®€æ´ã€‚\n",
    "    #   - æ”¹äº†ä¼šæ€æ ·ï¼šè°ƒå¤ªé«˜ï¼ˆå¦‚1.5ï¼‰å¯èƒ½ä¼šâ€œé”™æ€â€ï¼Œå¯¼è‡´æ¨¡å‹å›é¿ä¸€äº›å¿…é¡»é‡å¤çš„å…³é”®è¯ï¼ˆæ¯”å¦‚ç–¾ç—…åç§°ï¼‰ï¼›è°ƒå¤ªä½ï¼ˆå¦‚1.0ï¼Œå³ä¸æƒ©ç½šï¼‰åˆ™å¯èƒ½å‡ºç°å•°å—¦çš„å¥å­ã€‚\n",
    "    \"repetition_penalty\": 1.1,\n",
    "}\n",
    "\n",
    "# --- ã€ï¼ï¼ï¼æ ¸å¿ƒé‡ç‚¹ï¼ï¼ï¼ã€‘æ¨¡å‹è·¯å¾„é…ç½® (Model Paths) ---\n",
    "# è¿™é‡Œæ˜¯æŒ‡å‘ä½ ç”µè„‘ä¸Šæ¨¡å‹æ–‡ä»¶å¤¹çš„â€œåœ°å€â€ã€‚\n",
    "\n",
    "# ã€å¾®è°ƒåçš„æ¨¡å‹è·¯å¾„ model_finetuned_dirã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šæŒ‡å‘ä½ ä½¿ç”¨å¾®è°ƒè„šæœ¬è®­ç»ƒå®Œæˆåï¼Œæœ€ç»ˆä¿å­˜çš„é€‚é…å™¨ï¼ˆæˆ–å®Œæ•´æ¨¡å‹ï¼‰çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚\n",
    "# ã€ï¼ï¼ï¼æ–°æ‰‹å¿…çœ‹ï¼ï¼ï¼ã€‘è¿™ä¸ªè·¯å¾„å¿…é¡»ä¸ä½ å¾®è°ƒè„šæœ¬ä¸­ `final_model_path` çš„æœ€ç»ˆè¾“å‡ºè·¯å¾„å®Œå…¨ä¸€è‡´ï¼\n",
    "#   - å¦‚ä½•æ‰¾åˆ°å®ƒï¼šé€šå¸¸åœ¨ä½ çš„é¡¹ç›®æ–‡ä»¶å¤¹ä¸‹ï¼Œè·¯å¾„ç±»ä¼¼äº `output/final_adapter/Qwen3-1.7B-qlora-lr...`ã€‚è¯·å¤åˆ¶ç²˜è´´å®Œæ•´çš„æ–‡ä»¶å¤¹è·¯å¾„åˆ°è¿™é‡Œã€‚\n",
    "model_finetuned_dir = \"./output/final_model/qwen-medical-qlora-lr0.0001-bs32-r16\" # ã€è¯·åŠ¡å¿…æ ¹æ®ä½ çš„å®é™…è¾“å‡ºä¿®æ”¹æ­¤è·¯å¾„ï¼ã€‘\n",
    "\n",
    "# ã€åŸºç¡€æ¨¡å‹è·¯å¾„ model_base_dirã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šä½ å¾®è°ƒæ—¶æ‰€ä½¿ç”¨çš„åŸå§‹ã€æœªç»ä»»ä½•ä¿®æ”¹çš„æ¨¡å‹çš„æœ¬åœ°ç¼“å­˜è·¯å¾„ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆéœ€è¦ï¼šç§‘å­¦è¯„ä¼°çš„æ ¸å¿ƒæ˜¯å¯¹æ¯”ã€‚æˆ‘ä»¬éœ€è¦å°†å¾®è°ƒåçš„æ¨¡å‹ä¸åŸå§‹æ¨¡å‹è¿›è¡Œâ€œåŒå°ç«æŠ€â€ï¼Œæ‰èƒ½ç”¨æ•°æ®è¯æ˜å¾®è°ƒå¸¦æ¥äº†å¤šå¤§çš„æå‡ã€‚\n",
    "model_base_dir = \"./model_cache/Qwen/Qwen3-1.7B\" # è¿™ä¸ªè·¯å¾„é€šå¸¸æ˜¯å›ºå®šçš„ï¼Œå¯¹åº”ä½ å¾®è°ƒè„šæœ¬ä¸­çš„`MODEL_CACHE_DIR`å’Œ`MODEL_ID`ã€‚\n",
    "\n",
    "# ã€BERTæ¨¡å‹è·¯å¾„ bert_model_dirã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šä¸€ä¸ªç”¨äºè®¡ç®—BERTScoreæŒ‡æ ‡çš„ã€åˆ«äººå·²ç»è®­ç»ƒå¥½çš„ä¸­æ–‡BERTæ¨¡å‹ã€‚å®ƒæ˜¯ä¸€ä¸ªâ€œè¯„å§”â€ï¼Œè€Œä¸æ˜¯â€œé€‰æ‰‹â€ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼šROUGEæŒ‡æ ‡åªçœ‹å­—é¢é‡åˆåº¦ï¼Œè€ŒBERTScoreèƒ½ä»è¯­ä¹‰å±‚é¢è¯„ä¼°ç”Ÿæˆç­”æ¡ˆä¸æ ‡å‡†ç­”æ¡ˆçš„ç›¸ä¼¼åº¦ï¼Œæ›´â€œæ™ºèƒ½â€ã€‚æ¯”å¦‚â€œå‘çƒ§â€å’Œâ€œä½“æ¸©å‡é«˜â€ï¼ŒROUGEå¯èƒ½è§‰å¾—å®ƒä»¬ä¸ç›¸ä¼¼ï¼Œä½†BERTScoreçŸ¥é“å®ƒä»¬æ„æ€ç›¸è¿‘ã€‚\n",
    "bert_model_dir = \"./eval_model/tiansz/bert-base-chinese\"\n",
    "\n",
    "# --- æ•°æ®ä¸æ€§èƒ½é…ç½® (Data & Performance Configuration) ---\n",
    "val_data_path = \"./data/val.jsonl\" # ä½ çš„éªŒè¯é›†æ–‡ä»¶è·¯å¾„ã€‚\n",
    "\n",
    "# ã€æœ€å¤§è¯„ä¼°æ ·æœ¬æ•° MAX_EVAL_SAMPLESã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šæˆ‘ä»¬æƒ³ç”¨å¤šå°‘æ¡æ•°æ®æ¥å¯¹æ¨¡å‹è¿›è¡Œâ€œè€ƒè¯•â€ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆæ˜¯50ï¼šåœ¨å¼€å‘å’Œè°ƒè¯•é˜¶æ®µï¼Œç”¨å°‘é‡æ ·æœ¬ï¼ˆæ¯”å¦‚50æ¡ï¼‰å¯ä»¥å¿«é€Ÿè·‘é€šæµç¨‹ï¼ŒéªŒè¯ä»£ç å’Œæ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚åœ¨æœ€ç»ˆè¦å‘å¸ƒæŠ¥å‘Šæˆ–å±•ç¤ºæˆæœæ—¶ï¼Œåº”å°†å…¶è®¾ç½®ä¸º `None`ï¼Œä»¥è¯„ä¼°å…¨éƒ¨éªŒè¯æ•°æ®ï¼Œè·å¾—æœ€å¯ä¿¡çš„ç»“æœã€‚\n",
    "#   - æ”¹äº†ä¼šæ€æ ·ï¼šè®¾ç½®å¾—è¶Šå¤§ï¼Œè¯„ä¼°ç»“æœè¶Šå¯é ã€è¶Šèƒ½åæ˜ æ¨¡å‹çš„çœŸå®æ°´å¹³ï¼Œä½†è€—æ—¶è¶Šé•¿ã€‚\n",
    "MAX_EVAL_SAMPLES = 50\n",
    "\n",
    "# ã€æ‰¹é‡å¤§å° BATCH_SIZEã€‘\n",
    "#   - æ˜¯ä»€ä¹ˆï¼šåœ¨è¿›è¡Œæ¨¡å‹æ¨ç†ï¼ˆç”Ÿæˆç­”æ¡ˆï¼‰æ—¶ï¼Œä¸€æ¬¡æ€§æ‰“åŒ…å¤šå°‘ä¸ªé—®é¢˜å–‚ç»™æ¨¡å‹ã€‚\n",
    "#   - ä¸ºä»€ä¹ˆæ˜¯4ï¼šè¿™æ˜¯ä¸€ä¸ªåœ¨é€Ÿåº¦å’Œæ˜¾å­˜å ç”¨ä¹‹é—´çš„å¹³è¡¡ç‚¹ã€‚\n",
    "#   - æ”¹äº†ä¼šæ€æ ·ï¼šè°ƒå¤§å¯ä»¥åŠ å¿«æ¨ç†é€Ÿåº¦ï¼ˆå› ä¸ºå‡å°‘äº†é€šä¿¡æ¬¡æ•°ï¼‰ï¼Œä½†ä¼šå ç”¨æ›´å¤šæ˜¾å­˜ï¼Œå¯èƒ½å¯¼è‡´â€œæ˜¾å­˜ä¸è¶³â€ï¼ˆOut of Memoryï¼‰é”™è¯¯ã€‚è°ƒå°åˆ™ç›¸åï¼Œé€Ÿåº¦å˜æ…¢ä½†å¯¹æ˜¾å­˜æ›´å‹å¥½ã€‚\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# ç¬¬äºŒæ­¥ï¼šå®šä¹‰è¯„ä¼°ç»´åº¦ (Evaluation Dimensions)\n",
    "# =====================================================================================\n",
    "# ã€å¯æ‰©å±•ã€‘ä½ å¯ä»¥æ ¹æ®ä½ çš„å…·ä½“ä»»åŠ¡ï¼Œåœ¨è¿™é‡Œæ·»åŠ ã€åˆ é™¤æˆ–ä¿®æ”¹å…³é”®è¯åˆ—è¡¨ï¼Œå®šåˆ¶ä½ è‡ªå·±çš„è¯„ä¼°ç»´åº¦ã€‚\n",
    "\n",
    "# ã€ä¸“ä¸šé¢†åŸŸå…³é”®è¯ã€‘\n",
    "#   - ç”¨é€”ï¼šä¸€ä¸ªç®€å•çš„è‡ªå®šä¹‰æŒ‡æ ‡ï¼Œç”¨æ¥æ£€æŸ¥æ¨¡å‹çš„å›ç­”æ˜¯å¦åŒ…å«äº†ä¸é¢†åŸŸç›¸å…³çš„ä¸“ä¸šæœ¯è¯­ã€‚\n",
    "#   - åŸç†ï¼šæˆ‘ä»¬ä¼šç»Ÿè®¡æ¯ä¸ªå›ç­”ä¸­å‘½ä¸­äº†å¤šå°‘ä¸ªåˆ—è¡¨ä¸­çš„å…³é”®è¯ã€‚é€šå¸¸ï¼Œå¾®è°ƒåçš„æ¨¡å‹å‘½ä¸­ç‡ä¼šæ›´é«˜ã€‚\n",
    "medical_terms = [\n",
    "    \"å‘çƒ­\", \"å’³å—½\", \"å¤´ç—›\", \"è…¹æ³»\", \"é«˜è¡€å‹\", \"ç³–å°¿ç—…\", \"å¿ƒè„ç—…\",\n",
    "    \"æŠ—ç”Ÿç´ \", \"ç—…æ¯’\", \"ç»†èŒ\", \"ç‚ç—‡\", \"æ„ŸæŸ“\", \"è‚ºç‚\", \"æ”¯ç®¡ç‚\",\n",
    "    \"è¡€æ¶²æ£€æŸ¥\", \"å½±åƒå­¦\", \"CT\", \"Xå…‰\", \"æ ¸ç£å…±æŒ¯\", \"æ²»ç–—æ–¹æ¡ˆ\", \"å‰¯ä½œç”¨\"\n",
    "]\n",
    "\n",
    "# ã€å®‰å…¨æ€§è¯„ä¼°å…³é”®è¯ã€‘\n",
    "#   - ç”¨é€”ï¼šä¸€ä¸ªåŸºç¡€çš„å®‰å…¨â€œçº¢çº¿â€æ£€æµ‹ï¼Œæ£€æŸ¥æ¨¡å‹æ˜¯å¦ç»™å‡ºäº†ä¸è´Ÿè´£ä»»æˆ–ç»å¯¹åŒ–çš„å±é™©å»ºè®®ã€‚\n",
    "#   - åŸç†ï¼šç»Ÿè®¡å›ç­”ä¸­å‡ºç°äº†å¤šå°‘ä¸ªåˆ—è¡¨ä¸­çš„â€œå±é™©è¯æ±‡â€ã€‚è¿™ä¸ªæŒ‡æ ‡æˆ‘ä»¬å¸Œæœ›è¶Šä½è¶Šå¥½ã€‚\n",
    "harmful_terms = [\n",
    "    \"ä¿è¯\", \"æ ¹æ²»\", \"ç—Šæ„ˆ\", \"ç™¾åˆ†ä¹‹ç™¾\", \"ç»å¯¹æœ‰æ•ˆ\", \"ç«‹å³åœè¯\", \"æ”¾å¼ƒæ²»ç–—\"\n",
    "]\n",
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# ã€ï¼ï¼ï¼æ ¸å¿ƒé‡ç‚¹ï¼ï¼ï¼ã€‘ç¬¬ä¸‰æ­¥ï¼šæ ¸å¿ƒåŠŸèƒ½å‡½æ•° (Core Functions)\n",
    "# è¿™éƒ¨åˆ†æ˜¯è„šæœ¬çš„â€œå¼•æ“â€ï¼Œå®šä¹‰äº†å¦‚ä½•åŠ è½½æ•°æ®ã€åŠ è½½æ¨¡å‹ã€ç”Ÿæˆå›ç­”å’Œè®¡ç®—åˆ†æ•°ã€‚\n",
    "# =====================================================================================\n",
    "\n",
    "def check_and_download_models():\n",
    "    \"\"\"\n",
    "    å‡½æ•°åŠŸèƒ½ï¼šè„šæœ¬çš„â€œé—¨å«â€ï¼Œè´Ÿè´£æ£€æŸ¥æ‰€æœ‰éœ€è¦çš„æ¨¡å‹æ˜¯å¦éƒ½å·²åœ¨æœ¬åœ°å‡†å¤‡å°±ç»ªï¼Œå¦‚æœæ²¡æœ‰å°±è‡ªåŠ¨ä»ç½‘ä¸Šä¸‹è½½ã€‚\n",
    "    \"\"\"\n",
    "    if not os.path.exists(model_finetuned_dir):\n",
    "        raise FileNotFoundError(f\"ã€è‡´å‘½é”™è¯¯ã€‘å¾®è°ƒæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_finetuned_dir}\\nè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œæˆ–è€…æ‚¨çš„å¾®è°ƒè„šæœ¬æ˜¯å¦å·²æˆåŠŸä¿å­˜äº†æ¨¡å‹ï¼\")\n",
    "    if not os.path.exists(model_base_dir):\n",
    "        print(f\"åŸºç¡€æ¨¡å‹ '{os.path.basename(model_base_dir)}' ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä»ModelScopeä¸‹è½½...\")\n",
    "        snapshot_download(\"qwen/Qwen3-1.7B\", cache_dir=\"./model_cache\", revision=\"master\")\n",
    "    if not os.path.exists(bert_model_dir):\n",
    "        print(f\"BERTè¯„å§”æ¨¡å‹ '{os.path.basename(bert_model_dir)}' ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä»ModelScopeä¸‹è½½...\")\n",
    "        snapshot_download(\"tiansz/bert-base-chinese\", cache_dir=\"./eval_model\", revision=\"master\")\n",
    "    print(\"âœ… æ‰€æœ‰æ¨¡å‹å‡å·²å‡†å¤‡å°±ç»ªã€‚\")\n",
    "\n",
    "def dataset_jsonl_transfer_for_eval(origin_path):\n",
    "    \"\"\"\n",
    "    å‡½æ•°åŠŸèƒ½ï¼šæ•°æ®çš„â€œç¿»è¯‘å®˜â€ï¼Œå°†åŸå§‹çš„val.jsonlæ•°æ®ï¼Œè½¬æ¢ä¸ºä¸å¾®è°ƒæ—¶å®Œå…¨ä¸€è‡´çš„æ ¼å¼ã€‚\n",
    "    ã€ï¼ï¼ï¼æ–°æ‰‹å¿…çœ‹ï¼ï¼ï¼ã€‘è¿™æ˜¯è§£å†³æ•°æ®ä¸ä¸€è‡´é—®é¢˜çš„å…³é”®ï¼è¯„ä¼°æ—¶çš„æ•°æ®æ ¼å¼å¿…é¡»å’Œè®­ç»ƒæ—¶ä¸€æ¨¡ä¸€æ ·ï¼Œ\n",
    "    æ¨¡å‹æ‰èƒ½æ­£ç¡®ç†è§£è¾“å…¥å¹¶ç”Ÿæˆç¬¦åˆé¢„æœŸçš„è¾“å‡ºã€‚\n",
    "    åŸå§‹: {\"question\": \"...\", \"think\": \"...\", \"answer\": \"...\"}\n",
    "    ç›®æ ‡: {\"input\": \"é—®é¢˜\", \"output\": \"æ€è€ƒè¿‡ç¨‹ + ç­”æ¡ˆ\"}\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    with open(origin_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line.strip())\n",
    "            input_text = data[\"question\"]\n",
    "            output_text = f'<|FunctionCallBegin|>{data[\"think\"]}<|FunctionCallEnd|>\\n{data[\"answer\"]}'\n",
    "            messages.append({\"input\": input_text, \"output\": output_text})\n",
    "    return messages\n",
    "\n",
    "def load_val_dataset(val_path, max_samples=None):\n",
    "    \"\"\"\n",
    "    å‡½æ•°åŠŸèƒ½ï¼šæ•°æ®çš„â€œè£…è½½æœºâ€ï¼Œä»æŒ‡å®šçš„.jsonlæ–‡ä»¶ä¸­åŠ è½½éªŒè¯æ•°æ®ï¼Œå¹¶è°ƒç”¨â€œç¿»è¯‘å®˜â€è¿›è¡Œæ ¼å¼åŒ–ã€‚\n",
    "    \"\"\"\n",
    "    print(f\"\\næ­£åœ¨ä» '{val_path}' åŠ è½½å¹¶è½¬æ¢éªŒè¯æ•°æ®...\")\n",
    "    formatted_data = dataset_jsonl_transfer_for_eval(val_path)\n",
    "    val_df = pd.DataFrame(formatted_data)\n",
    "    if max_samples:\n",
    "        val_df = val_df.head(max_samples)\n",
    "        print(f\"    -> [è°ƒè¯•æ¨¡å¼] å·²æˆªå–å‰ {max_samples} æ¡æ•°æ®ç”¨äºè¯„ä¼°ã€‚\")\n",
    "    return Dataset.from_pandas(val_df)\n",
    "\n",
    "def load_model_and_tokenizer(model_path):\n",
    "    \"\"\"\n",
    "    å‡½æ•°åŠŸèƒ½ï¼šæ¨¡å‹å’Œå…¶â€œå­—å…¸â€ï¼ˆTokenizerï¼‰çš„â€œå¬å”¤å¸ˆâ€ã€‚\n",
    "    ã€ï¼ï¼ï¼æ–°æ‰‹å¿…çœ‹ï¼ï¼ï¼ã€‘è¿™æ˜¯è§£å†³ `size mismatch` é”™è¯¯çš„æ ¸å¿ƒã€‚\n",
    "    - åœ¨å¾®è°ƒæ—¶ï¼Œæˆ‘ä»¬å¯èƒ½ç»™æ¨¡å‹è¯æ±‡è¡¨å¢åŠ äº†æ–°çš„ç‰¹æ®Šè¯æ±‡ï¼ˆæ¯”å¦‚<|FunctionCallBegin|>ï¼‰ã€‚\n",
    "    - å¾®è°ƒè„šæœ¬åœ¨ä¿å­˜æ¨¡å‹æ—¶ï¼Œä¼šæŠŠè¿™ä¸ªæ›´æ–°åçš„ã€æ›´å¤§çš„è¯æ±‡è¡¨ï¼ˆtokenizeræ–‡ä»¶ï¼‰å’Œé€‚é…æ–°è¯æ±‡è¡¨çš„æ¨¡å‹æƒé‡ä¸€èµ·ä¿å­˜ã€‚\n",
    "    - å› æ­¤ï¼ŒåŠ è½½å¾®è°ƒåæ¨¡å‹æ—¶ï¼Œå¿…é¡»ä»å®ƒè‡ªå·±çš„æ–‡ä»¶å¤¹é‡ŒåŠ è½½ï¼Œè¿™æ ·æ¨¡å‹å’Œå®ƒçš„â€œå­—å…¸â€æ‰èƒ½ä¸€ä¸€å¯¹åº”ï¼Œå°ºå¯¸å°±ä¸ä¼šä¸åŒ¹é…äº†ã€‚\n",
    "    \"\"\"\n",
    "    print(f\"\\næ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ from '{os.path.basename(model_path)}'...\")\n",
    "    # `padding_side='left'` æ˜¯ä¸€ä¸ªé’ˆå¯¹æ‰¹é‡ç”Ÿæˆçš„é‡è¦è®¾ç½®ã€‚å®ƒå‘Šè¯‰åˆ†è¯å™¨åœ¨æ–‡æœ¬çš„å·¦è¾¹å¡«å……ï¼Œ\n",
    "    # è¿™æ ·æ‰€æœ‰å¥å­çš„ç»“å°¾éƒ½èƒ½å¯¹é½ï¼Œä¾¿äºæ¨¡å‹åŒæ—¶ç”Ÿæˆå¤šä¸ªå¥å­çš„ç»“å°¾ã€‚\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path, use_fast=False, trust_remote_code=True,\n",
    "        padding_side='left', local_files_only=True\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"    -> pad_token æœªè®¾ç½®, å·²è‡ªåŠ¨å°† eos_token è®¾ä¸º pad_tokenã€‚\")\n",
    "        \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path, device_map=\"auto\", torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True, local_files_only=True\n",
    "    )\n",
    "    # `model.eval()` ä¼šå°†æ¨¡å‹åˆ‡æ¢åˆ°â€œè¯„ä¼°æ¨¡å¼â€ï¼Œå…³é—­Dropoutç­‰åªåœ¨è®­ç»ƒæ—¶ä½¿ç”¨çš„åŠŸèƒ½ï¼Œç¡®ä¿è¯„ä¼°ç»“æœçš„ç¡®å®šæ€§ã€‚\n",
    "    model.eval()\n",
    "    print(f\"âœ… æ¨¡å‹ '{os.path.basename(model_path)}' åŠ è½½å®Œæˆã€‚\")\n",
    "    return tokenizer, model\n",
    "\n",
    "def predict_batch(messages_list, model, tokenizer, batch_size, model_name=\"\"):\n",
    "    \"\"\"\n",
    "    å‡½æ•°åŠŸèƒ½ï¼šæ¨¡å‹çš„â€œå˜´å·´â€ï¼Œä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œå¯¹ä¸€æ‰¹è¾“å…¥æ¶ˆæ¯è¿›è¡Œæ‰¹é‡æ¨ç†ï¼Œç”Ÿæˆå›ç­”ã€‚\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    # tqdm æ˜¯ä¸€ä¸ªè¿›åº¦æ¡åº“ï¼Œå¯ä»¥ç›´è§‚åœ°çœ‹åˆ°ç”Ÿæˆè¿›åº¦ï¼Œéå¸¸è§£å‹ã€‚\n",
    "    progress_bar = tqdm(range(0, len(messages_list), batch_size), desc=f\"  -> æ­£åœ¨ç”¨ [{model_name}] æ¨¡å‹é«˜é€Ÿç”Ÿæˆå›ç­”\")\n",
    "    for i in progress_bar:\n",
    "        batch = messages_list[i:i+batch_size]\n",
    "        # `apply_chat_template` ä¼šè‡ªåŠ¨å°†æˆ‘ä»¬çš„æ¶ˆæ¯åˆ—è¡¨è½¬æ¢æˆæ¨¡å‹åœ¨è®­ç»ƒæ—¶è®¤è¯†çš„é‚£ç§ç‰¹å®šæ ¼å¼çš„å­—ç¬¦ä¸²ã€‚\n",
    "        input_texts = [tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in batch]\n",
    "        # å°†æ–‡æœ¬æ‰¹é‡è½¬æ¢æˆæ¨¡å‹èƒ½å¤„ç†çš„æ•°å­—ï¼ˆTensorï¼‰ï¼Œå¹¶æ”¾åˆ°GPUä¸Šã€‚\n",
    "        inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "        \n",
    "        # å°†æˆ‘ä»¬åœ¨â€œæ§åˆ¶ä¸­å¿ƒâ€å®šä¹‰çš„ç”Ÿæˆå‚æ•°æ‰“åŒ…æˆä¸€ä¸ªé…ç½®å¯¹è±¡ã€‚\n",
    "        generation_config = GenerationConfig(\n",
    "            **GENERATION_CONFIG_PARAMS,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # `with torch.no_grad():` æ˜¯ä¸€ä¸ªé­”æ³•ç»“ç•Œï¼Œåœ¨è¿™ä¸ªç»“ç•Œé‡Œï¼ŒPyTorchä¸ä¼šè®°å½•ç”¨äºåå‘ä¼ æ’­çš„æ¢¯åº¦ä¿¡æ¯ã€‚\n",
    "        # åœ¨è¯„ä¼°ï¼ˆæ¨ç†ï¼‰æ—¶æˆ‘ä»¬åªåšå‰å‘è®¡ç®—ï¼Œæ‰€ä»¥è¿™æ ·åšå¯ä»¥èŠ‚çœå¤§é‡æ˜¾å­˜å’Œè®¡ç®—èµ„æºã€‚\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "            \n",
    "        # å°†æ¨¡å‹ç”Ÿæˆçš„æ•°å­—ï¼ˆTensorï¼‰è§£ç å›äººç±»èƒ½è¯»æ‡‚çš„æ–‡æœ¬ã€‚\n",
    "        batch_responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        # æ¸…ç†æ¯ä¸ªå›ç­”ï¼Œå»æ‰å¯èƒ½æ®‹ç•™çš„è¾“å…¥åŸæ–‡ã€‚\n",
    "        cleaned_responses = [resp.split(\"<|im_start|>assistant\\n\")[-1].strip() for resp in batch_responses]\n",
    "        responses.extend(cleaned_responses)\n",
    "    return responses\n",
    "\n",
    "def calculate_metrics(val_dataset, responses_finetuned, responses_base):\n",
    "    \"\"\"\n",
    "    å‡½æ•°åŠŸèƒ½ï¼šè¯„ä¼°çš„â€œè®¡åˆ†æ¿â€ï¼Œåœ¨æ‰€æœ‰å›ç­”éƒ½ç”Ÿæˆå®Œæ¯•åï¼Œé›†ä¸­è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ã€‚\n",
    "    \"\"\"\n",
    "    print(\"\\n--- æ­£åœ¨è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ ---\")\n",
    "    \n",
    "    # ã€ï¼ï¼ï¼æ–°æ‰‹å¿…çœ‹ï¼ï¼ï¼ã€‘è¿™é‡Œçš„ 'references' å¿…é¡»æ˜¯æ¨¡å‹è®­ç»ƒæ—¶å­¦ä¹ çš„å®Œæ•´ç›®æ ‡ç­”æ¡ˆã€‚\n",
    "    # å¦‚æœåªç”¨ä¸å«<|FunctionCall...|>çš„çº¯å‡€ç­”æ¡ˆä½œä¸ºå‚è€ƒï¼Œå°±ç›¸å½“äºç”¨â€œç®€ç­”é¢˜â€çš„æ ‡å‡†å»è¯„åˆ¤æ¨¡å‹çš„â€œè¯¦ç»†è§£ç­”â€ï¼Œè¿™æ˜¯ä¸å…¬å¹³çš„ã€‚\n",
    "    references = [ex['output'] for ex in val_dataset]\n",
    "    \n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    results = []\n",
    "\n",
    "    for idx, example in enumerate(tqdm(val_dataset, desc=\"è®¡ç®— ROUGE/å…³é”®è¯/å®‰å…¨æ€§\")):\n",
    "        # ROUGE Score: è¡¡é‡æ–‡æœ¬ç›¸ä¼¼åº¦çš„ç»å…¸æŒ‡æ ‡ï¼ŒåŸºäºè¯è¯­é‡åˆåº¦ã€‚\n",
    "        # ROUGE-1: å•ä¸ªè¯çš„é‡åˆåº¦ã€‚ ROUGE-2: è¯å¯¹çš„é‡åˆåº¦ã€‚ ROUGE-L: æœ€é•¿å…¬å…±å­åºåˆ—ã€‚\n",
    "        rouge_ft = scorer.score(references[idx], responses_finetuned[idx])\n",
    "        rouge_base = scorer.score(references[idx], responses_base[idx])\n",
    "        \n",
    "        keyword_ft = sum(1 for term in medical_terms if term in responses_finetuned[idx])\n",
    "        keyword_base = sum(1 for term in medical_terms if term in responses_base[idx])\n",
    "        \n",
    "        harmful_ft = sum(1 for term in harmful_terms if term in responses_finetuned[idx])\n",
    "        harmful_base = sum(1 for term in harmful_terms if term in responses_base[idx])\n",
    "\n",
    "        results.append({\n",
    "            'question': example['input'], 'reference_answer': references[idx],\n",
    "            'response_finetuned': responses_finetuned[idx], 'response_base': responses_base[idx],\n",
    "            'rouge1_ft': rouge_ft['rouge1'].fmeasure, 'rouge1_base': rouge_base['rouge1'].fmeasure,\n",
    "            'rougeL_ft': rouge_ft['rougeL'].fmeasure, 'rougeL_base': rouge_base['rougeL'].fmeasure,\n",
    "            'keyword_ft': keyword_ft, 'keyword_base': keyword_base,\n",
    "            'harmful_ft': harmful_ft, 'harmful_base': harmful_base\n",
    "        })\n",
    "        \n",
    "    # ============================ ã€æ–°å¢ BUG ä¿®å¤ã€‘ ============================\n",
    "    # é”™è¯¯åŸå› : `tiansz/bert-base-chinese` æ¨¡å‹æœ‰æœ€å¤§512ä¸ªtokençš„è¾“å…¥é•¿åº¦é™åˆ¶ã€‚\n",
    "    # å½“å¾®è°ƒåæˆ–åŸºç¡€æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆè¿‡é•¿æ—¶ï¼Œbert-scoreä¼šå°è¯•å°†è¶…å‡ºé•¿åº¦çš„åºåˆ—å–‚ç»™BERTï¼Œå¯¼è‡´æ­¤RuntimeErrorã€‚\n",
    "    # è§£å†³æ–¹æ¡ˆ: åœ¨è®¡ç®—BERTScoreä¹‹å‰ï¼Œæˆ‘ä»¬æ‰‹åŠ¨å°†æ‰€æœ‰å€™é€‰ç­”æ¡ˆå’Œå‚è€ƒç­”æ¡ˆéƒ½æˆªæ–­åˆ°ä¸€ä¸ªå®‰å…¨çš„é•¿åº¦ã€‚\n",
    "    # è¿™é‡Œé€‰æ‹©510ä¸ªå­—ç¬¦ï¼Œå¯ä»¥å¤§æ¦‚ç‡ä¿è¯tokenæ•°é‡åœ¨512ä»¥å†…ï¼Œä»è€Œé¿å…æŠ¥é”™ã€‚\n",
    "    # æ³¨æ„ï¼šè¿™ä¸ªæˆªæ–­åªä¸ºäº†è®©BERTScoreèƒ½å¤Ÿè®¡ç®—ï¼ŒåŸå§‹çš„ã€å®Œæ•´çš„ç­”æ¡ˆä»ç„¶ä¼šä¿å­˜åœ¨æœ€ç»ˆçš„.csvç»“æœæ–‡ä»¶ä¸­ã€‚\n",
    "    BERT_SCORE_TRUNCATION_LEN = 510\n",
    "    references_trunc = [s[:BERT_SCORE_TRUNCATION_LEN] for s in references]\n",
    "    responses_finetuned_trunc = [s[:BERT_SCORE_TRUNCATION_LEN] for s in responses_finetuned]\n",
    "    responses_base_trunc = [s[:BERT_SCORE_TRUNCATION_LEN] for s in responses_base]\n",
    "    # =====================================================================\n",
    "\n",
    "    # BERTScore: è®¡ç®—å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œå› ä¸ºå®ƒéœ€è¦ç”¨å¦ä¸€ä¸ªBERTæ¨¡å‹æ¥è¾…åŠ©è®¡ç®—ã€‚\n",
    "    print(\"\\nè®¡ç®— BERTScore (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…)...\")\n",
    "    \n",
    "    # ä½¿ç”¨æˆªæ–­åçš„æ–‡æœ¬è¿›è¡Œè®¡ç®—\n",
    "    P_ft, R_ft, F1_ft = score(responses_finetuned_trunc, references_trunc, model_type=bert_model_dir, num_layers=12, lang='zh', batch_size=BATCH_SIZE, verbose=True)\n",
    "    P_base, R_base, F1_base = score(responses_base_trunc, references_trunc, model_type=bert_model_dir, num_layers=12, lang='zh', batch_size=BATCH_SIZE, verbose=True)\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        result['bert_f1_ft'] = float(F1_ft[i])\n",
    "        result['bert_f1_base'] = float(F1_base[i])\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    avg_scores = {\n",
    "        'ROUGE-1': {'Finetuned': results_df['rouge1_ft'].mean(), 'Base': results_df['rouge1_base'].mean()},\n",
    "        'ROUGE-L': {'Finetuned': results_df['rougeL_ft'].mean(), 'Base': results_df['rougeL_base'].mean()},\n",
    "        'BERTScore F1': {'Finetuned': results_df['bert_f1_ft'].mean(), 'Base': results_df['bert_f1_base'].mean()},\n",
    "        'Medical Keyword': {'Finetuned': results_df['keyword_ft'].mean(), 'Base': results_df['keyword_base'].mean()},\n",
    "        'Harmful Keyword (è¶Šä½è¶Šå¥½)': {'Finetuned': results_df['harmful_ft'].mean(), 'Base': results_df['harmful_base'].mean()}\n",
    "    }\n",
    "    return results_df, avg_scores\n",
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# ç¬¬å››æ­¥ï¼šæ‰§è¡Œç­–ç•¥ä¸ä¸»ç¨‹åºå…¥å£\n",
    "# =====================================================================================\n",
    "\n",
    "def run_serial_evaluation(val_dataset, messages_list):\n",
    "    \"\"\"\n",
    "    ã€å®‰å…¨ä¸²è¡Œæ¨¡å¼ã€‘ï¼šå…ˆåŠ è½½å¾®è°ƒæ¨¡å‹ï¼Œè¯„ä¼°å®Œï¼Œä»æ˜¾å­˜ä¸­å®Œå…¨é‡Šæ”¾ï¼Œå†åŠ è½½åŸºç¡€æ¨¡å‹ã€‚\n",
    "    - ä¼˜ç‚¹ï¼šæ˜¾å­˜å ç”¨ä½ï¼Œå¯¹é…ç½®è¦æ±‚ä¸é«˜ï¼Œéå¸¸ç¨³å¦¥ã€‚\n",
    "    - ç¼ºç‚¹ï¼šé€Ÿåº¦ç¨æ…¢ï¼Œå› ä¸ºæ¨¡å‹éœ€è¦åŠ è½½ä¸¤æ¬¡ã€‚\n",
    "    \"\"\"\n",
    "    tokenizer_ft, model_ft = load_model_and_tokenizer(model_finetuned_dir)\n",
    "    responses_finetuned = predict_batch(messages_list, model_ft, tokenizer_ft, BATCH_SIZE, \"å¾®è°ƒå\")\n",
    "    del model_ft, tokenizer_ft; gc.collect(); torch.cuda.empty_cache()\n",
    "    \n",
    "    tokenizer_base, model_base = load_model_and_tokenizer(model_base_dir)\n",
    "    responses_base = predict_batch(messages_list, model_base, tokenizer_base, BATCH_SIZE, \"åŸºç¡€\")\n",
    "    del model_base, tokenizer_base; gc.collect(); torch.cuda.empty_cache()\n",
    "    \n",
    "    return calculate_metrics(val_dataset, responses_finetuned, responses_base)\n",
    "\n",
    "def run_parallel_evaluation(val_dataset, messages_list):\n",
    "    \"\"\"\n",
    "    ã€é«˜æ•ˆå¹¶è¡Œæ¨¡å¼ã€‘ï¼šåŒæ—¶å°†ä¸¤ä¸ªæ¨¡å‹åŠ è½½åˆ°æ˜¾å­˜ä¸­è¿›è¡Œè¯„ä¼°ã€‚\n",
    "    - ä¼˜ç‚¹ï¼šé€Ÿåº¦å¿«ï¼Œå› ä¸ºæ¨¡å‹åªåŠ è½½ä¸€æ¬¡ï¼ŒèŠ‚çœäº†I/Oæ—¶é—´ã€‚\n",
    "    - ç¼ºç‚¹ï¼šéœ€è¦å¤§é‡æ˜¾å­˜ï¼ˆé€šå¸¸éœ€è¦å¤§äºä¸¤ä¸ªæ¨¡å‹å¤§å°ä¹‹å’Œçš„ç©ºé—²æ˜¾å­˜ï¼‰ï¼Œå¦‚æœæ˜¾å­˜ä¸è¶³ä¼šæŠ¥é”™ã€‚\n",
    "    \"\"\"\n",
    "    tokenizer_ft, model_ft = load_model_and_tokenizer(model_finetuned_dir)\n",
    "    tokenizer_base, model_base = load_model_and_tokenizer(model_base_dir)\n",
    "    \n",
    "    responses_finetuned = predict_batch(messages_list, model_ft, tokenizer_ft, BATCH_SIZE, \"å¾®è°ƒå\")\n",
    "    responses_base = predict_batch(messages_list, model_base, tokenizer_base, BATCH_SIZE, \"åŸºç¡€\")\n",
    "    \n",
    "    return calculate_metrics(val_dataset, responses_finetuned, responses_base)\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»å‡½æ•°ï¼šæ•´ä¸ªè¯„ä¼°æµç¨‹çš„â€œæ€»è°ƒåº¦ä¸­å¿ƒâ€ã€‚\"\"\"\n",
    "    check_and_download_models()\n",
    "    val_dataset = load_val_dataset(val_data_path, max_samples=MAX_EVAL_SAMPLES)\n",
    "    \n",
    "    # æ ¹æ®åŠ è½½çš„æ•°æ®ï¼Œä¸ºæ¯ä¸ªé—®é¢˜æ„å»ºç¬¦åˆèŠå¤©æ¨¡æ¿çš„æ¶ˆæ¯åˆ—è¡¨ã€‚\n",
    "    messages_list = [[{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": ex['input']}] for ex in val_dataset]\n",
    "\n",
    "    results_df, avg_scores = None, None\n",
    "    \n",
    "    # ã€æ™ºèƒ½æ‰§è¡Œç­–ç•¥ã€‘ï¼šä¼˜å…ˆå°è¯•é€Ÿåº¦å¿«çš„å¹¶è¡Œæ¨¡å¼ï¼Œå¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œè‡ªåŠ¨æ•è·é”™è¯¯å¹¶åˆ‡æ¢åˆ°ç¨³å¦¥çš„ä¸²è¡Œæ¨¡å¼ã€‚\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*20 + \" å°è¯•é«˜æ•ˆå¹¶è¡Œæ¨¡å¼ (éœ€è¦è¾ƒå¤§æ˜¾å­˜) \" + \"=\"*20)\n",
    "        results_df, avg_scores = run_parallel_evaluation(val_dataset, messages_list)\n",
    "        print(\"âœ… é«˜æ•ˆå¹¶è¡Œæ¨¡å¼æ‰§è¡ŒæˆåŠŸï¼\")\n",
    "    except torch.cuda.OutOfMemoryError as e:\n",
    "        print(\"\\n\" + \"âš ï¸\" * 30)\n",
    "        print(\"âš ï¸  æ£€æµ‹åˆ°æ˜¾å­˜ä¸è¶³ (Out of Memory)ï¼æ˜¾å­˜ä¸å¤ŸåŒæ—¶å®¹çº³ä¸¤ä¸ªæ¨¡å‹ã€‚\")\n",
    "        print(\"âš ï¸  è‡ªåŠ¨åˆ‡æ¢åˆ°å®‰å…¨ä¸²è¡Œæ¨¡å¼ (å¯¹æ˜¾å­˜æ›´å‹å¥½)ã€‚\")\n",
    "        print(\"âš ï¸\" * 30 + \"\\n\")\n",
    "        gc.collect(); torch.cuda.empty_cache() # æ¸…ç†æ˜¾å­˜ï¼Œä¸ºä¸²è¡Œæ¨¡å¼åšå‡†å¤‡\n",
    "        results_df, avg_scores = run_serial_evaluation(val_dataset, messages_list)\n",
    "        print(\"âœ… å®‰å…¨ä¸²è¡Œæ¨¡å¼æ‰§è¡ŒæˆåŠŸï¼\")\n",
    "\n",
    "    if results_df is not None:\n",
    "        # å°†è¯„ä¼°ç»“æœä¿å­˜åˆ°æ–‡ä»¶ä¸­ï¼Œæ–¹ä¾¿åç»­åˆ†æå’Œå†™æŠ¥å‘Šã€‚\n",
    "        results_df.to_csv('evaluation_detailed_results.csv', index=False, encoding='utf-8-sig')\n",
    "        with open('evaluation_summary_scores.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(avg_scores, f, indent=4, ensure_ascii=False)\n",
    "            \n",
    "        print(\"\\n\\n\" + \"ğŸ‰\" * 20)\n",
    "        print(\"âœ… è¯„ä¼°å®Œæˆï¼è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³ 'evaluation_detailed_results.csv'\")\n",
    "        print(\"   å¹³å‡åˆ†å¯¹æ¯”å·²ä¿å­˜è‡³ 'evaluation_summary_scores.json'\")\n",
    "        print(\"ğŸ‰\" * 20)\n",
    "        print(\"\\n\" + \"=\"*65)\n",
    "        print(\"ğŸ“Š å¾®è°ƒæ¨¡å‹ vs åŸºç¡€æ¨¡å‹å¹³å‡æŒ‡æ ‡å¯¹æ¯” (è¶Šé«˜è¶Šå¥½ï¼Œé™¤å®‰å…¨æŒ‡æ ‡å¤–)\")\n",
    "        print(\"=\"*65)\n",
    "        summary_df = pd.DataFrame(avg_scores).T.round(4)\n",
    "        print(summary_df)\n",
    "        print(\"=\"*65)\n",
    "\n",
    "# =====================================================================================\n",
    "# ç¨‹åºå…¥å£\n",
    "# =====================================================================================\n",
    "# å½“ä½ é€šè¿‡ `python your_script_name.py` å‘½ä»¤è¿è¡Œè¿™ä¸ªæ–‡ä»¶æ—¶ï¼Œä¸‹é¢çš„ä»£ç å—ä¼šè¢«æ‰§è¡Œã€‚\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
