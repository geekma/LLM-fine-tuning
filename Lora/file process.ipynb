{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc2093e7-5c61-4e11-a4ec-74960f48e370",
   "metadata": {},
   "source": [
    "第0步：环境准备 (Setup)\n",
    "\n",
    "操作：在你的终端或CMD中，或者直接在Jupyter Notebook的一个代码块里运行以下命令（去掉前面的!）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf297b0-50c6-47b5-b10b-58c85ff15f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyMuPDF openai tqdm pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce84d26-e465-4b78-a3e8-1650ca869d27",
   "metadata": {},
   "source": [
    "第1步：导入所有需要的工具包 (Import Libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9fbce6-ddde-4cc4-9fe2-bc75cd2b59b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF库，用于读取PDF\n",
    "import openai # OpenAI官方库，用于调用GPT模型\n",
    "from tqdm.notebook import tqdm # 一个漂亮的进度条库，适用于Notebook\n",
    "import pandas as pd # 强大的数据处理库，我们用它来处理和分割数据\n",
    "from sklearn.model_selection import train_test_split # 一个机器学习工具，可以帮我们轻松分割数据集\n",
    "import json\n",
    "import re\n",
    "\n",
    "print(\"✅ 所有工具包已成功导入！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e701887-99ef-4380-a2e5-68f64d99ac14",
   "metadata": {},
   "source": [
    "第2步：【！！！核心配置区！！！】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b420509-e377-4207-bf46-0f1854340854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# 2. 【！！！核心配置区！！！】\n",
    "# =====================================================================================\n",
    "# 是什么：这里是你唯一需要手动修改的地方！请根据你的实际情况填写下面的参数。\n",
    "\n",
    "# --- 2.1: 配置你要使用的AI模型服务 ---\n",
    "# 说明：我们提供多种预设，你可以选择一个，并填入你的信息。\n",
    "#       将你不需要的配置块用三个引号 \"\"\" 注释掉。\n",
    "\n",
    "# ---【选项A：官方OpenAI】---\n",
    "AI_PROVIDER = \"openai\"\n",
    "OPENAI_API_KEY = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"  # <--- 在这里填入你的OpenAI API Key\n",
    "OPENAI_BASE_URL = \"https://api.openai.com/v1\" # 官方地址，通常无需修改\n",
    "MODEL_NAME = \"gpt-4-turbo-preview\" # 使用的模型名称\n",
    "\n",
    "\"\"\"\n",
    "# ---【选项B：国内服务商/自定义API (示例)】---\n",
    "# 适用于Kimi (Moonshot AI), 阿里通义千问, Baichuan, ZhipuAI等提供了兼容OpenAI接口的服务\n",
    "AI_PROVIDER = \"custom\"\n",
    "CUSTOM_API_KEY = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" # <--- 填入你的服务商提供的Key\n",
    "CUSTOM_BASE_URL = \"https://api.moonshot.cn/v1\" # <--- 【重要】填入服务商的API地址\n",
    "MODEL_NAME = \"moonshot-v1-8k\" # <--- 【重要】填入服务商指定的模型名称\n",
    "\"\"\"\n",
    "\n",
    "# --- 2.2: 指定你要处理的PDF文件或文件夹 ---\n",
    "# 说明：三种模式任选其一，将你不需要的模式用#号注释掉即可。\n",
    "\n",
    "# 模式A：处理单个PDF文件\n",
    "pdf_paths = [\"./中枢性性早熟诊断与治疗专家共识(2022).pdf\"]\n",
    "\n",
    "# # 模式B：处理多个指定的PDF文件\n",
    "# pdf_paths = [\n",
    "#     \"./医学文档1.pdf\",\n",
    "#     \"./医学文档2.pdf\",\n",
    "# ]\n",
    "\n",
    "# # 模式C：处理一个文件夹中所有的PDF文件\n",
    "# pdf_folder = \"./My_PDFs/\" # <--- 指定你的PDF文件夹路径\n",
    "# pdf_paths = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "\n",
    "# --- 2.3: 指定输出文件的路径 ---\n",
    "output_train_file = \"./train_from_pdf.jsonl\"\n",
    "output_val_file = \"./val_from_pdf.jsonl\"\n",
    "\n",
    "# --- 2.4: 配置数据分割比例 ---\n",
    "VALIDATION_SET_RATIO = 0.1\n",
    "\n",
    "# =====================================================================================\n",
    "# --- 配置自动加载 ---\n",
    "# 下面的代码会根据你上面的选择，自动创建API客户端，无需修改\n",
    "# =====================================================================================\n",
    "try:\n",
    "    if AI_PROVIDER == \"openai\":\n",
    "        client = openai.OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)\n",
    "        print(f\"✅ 已配置使用官方OpenAI服务。模型: {MODEL_NAME}\")\n",
    "    elif AI_PROVIDER == \"custom\":\n",
    "        client = openai.OpenAI(api_key=CUSTOM_API_KEY, base_url=CUSTOM_BASE_URL)\n",
    "        print(f\"✅ 已配置使用自定义API服务。模型: {MODEL_NAME}, API地址: {CUSTOM_BASE_URL}\")\n",
    "    else:\n",
    "        raise ValueError(\"无效的AI_PROVIDER选项，请选择 'openai' 或 'custom'\")\n",
    "except NameError:\n",
    "    raise NameError(\"请在上面的配置区选择一个AI服务商并填入信息！\")\n",
    "\n",
    "print(\"--- 配置检查 ---\")\n",
    "print(f\"待处理的PDF文件数量: {len(pdf_paths)}\")\n",
    "print(\"PDF文件列表:\")\n",
    "for p in pdf_paths:\n",
    "    print(f\"  - {p}\")\n",
    "print(f\"训练集输出路径: {output_train_file}\")\n",
    "print(f\"验证集输出路径: {output_val_file}\")\n",
    "print(\"--- 配置检查完毕 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b587b8f2-2b6e-4d3f-8c83-c44d5f745d71",
   "metadata": {},
   "source": [
    "第3步：定义核心功能函数 (Define Core Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbf2191-438f-4d76-bc54-f6879c850b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# 第3步：定义核心功能函数 (Define Core Functions)\n",
    "# =====================================================================================\n",
    "# 说明：这里我们把整个流程中需要用到的功能，封装成一个个独立的“小工具”（函数）。\n",
    "#       你不需要修改这部分代码。\n",
    "\n",
    "def extract_text_from_pdfs(pdf_paths):\n",
    "    \"\"\"\n",
    "    函数功能：从一个或多个PDF文件中提取所有文本。\n",
    "    \"\"\"\n",
    "    full_text = \"\"\n",
    "    print(\"--- 开始提取PDF文本 ---\")\n",
    "    for pdf_path in tqdm(pdf_paths, desc=\"提取PDF进度\"):\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            full_text += f\"\\n\\n===== Start of Document: {os.path.basename(pdf_path)} =====\\n\\n\"\n",
    "            for page in doc:\n",
    "                full_text += page.get_text(\"text\")\n",
    "            full_text += f\"\\n\\n===== End of Document: {os.path.basename(pdf_path)} =====\\n\\n\"\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️警告：读取文件 {pdf_path} 时出错: {e}\")\n",
    "    print(f\"✅ 所有PDF文本提取完成，总计 {len(full_text)} 个字符。\")\n",
    "    return full_text\n",
    "\n",
    "def chunk_text(text, min_chunk_size=300, max_chunk_size=700):\n",
    "    \"\"\"\n",
    "    函数功能：将长文本智能地切分成多个小块。\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 开始智能文本分块 ---\")\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if len(p.strip()) > 10]\n",
    "    chunks, final_chunks, temp_chunk = [], [], \"\"\n",
    "    for para in paragraphs:\n",
    "        if len(para) <= max_chunk_size: chunks.append(para)\n",
    "        else:\n",
    "            sentences = re.split(r'(?<=[。？！])\\s*', para)\n",
    "            current_chunk = \"\"\n",
    "            for sentence in sentences:\n",
    "                if len(current_chunk) + len(sentence) <= max_chunk_size: current_chunk += sentence\n",
    "                else: chunks.append(current_chunk); current_chunk = sentence\n",
    "            if current_chunk: chunks.append(current_chunk)\n",
    "    for chunk in chunks:\n",
    "        if len(temp_chunk) < min_chunk_size: temp_chunk += \"\\n\" + chunk\n",
    "        else: final_chunks.append(temp_chunk); temp_chunk = chunk\n",
    "    if temp_chunk: final_chunks.append(temp_chunk)\n",
    "    print(f\"✅ 文本被成功切分为 {len(final_chunks)} 个小块。\")\n",
    "    return final_chunks\n",
    "\n",
    "def generate_qa_from_chunk(text_chunk, model_name):\n",
    "    \"\"\"\n",
    "    函数功能：调用已配置好的大模型API客户端，从单个文本块生成Q&A。\n",
    "    \"\"\"\n",
    "    prompt_template = f\"\"\"\n",
    "# 角色\n",
    "你是一位专业的医学知识库构建专家。\n",
    "\n",
    "# 任务\n",
    "你的任务是根据下面提供的【原始文本】，生成3到5个高质量的、符合JSONL格式的问答(Q&A)对。\n",
    "\n",
    "# 要求\n",
    "1. **忠于原文**：所有答案都必须完全基于【原始文本】的内容，不允许捏造或推理【原始文本】中没有的信息。\n",
    "2. **一问一答**：每个问题都应该有一个简洁、完整、独立的答案。\n",
    "3. **多样性**：问题应该从不同角度提出，涵盖定义、原因、诊断标准、治疗方法、注意事项等。避免提出答案相似的重复问题。\n",
    "4. **格式严格**：每个Q&A对必须是独立的JSON对象，格式为 `{{\"input\": \"你的问题\", \"output\": \"你的答案\"}}`，并且每行一个JSON对象。不要在开头或结尾添加任何额外的解释或代码块标记。\n",
    "\n",
    "# 【原始文本】\n",
    "{text_chunk}\n",
    "\"\"\"\n",
    "    try:\n",
    "        # 【！！！核心修改！！！】\n",
    "        # 使用我们在第2步创建的全局变量 `client` 来发送请求。\n",
    "        # 这样无论你配置的是OpenAI还是自定义服务，这里的代码都无需改变。\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output well-formed JSONL.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_template}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=1024,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"❌ API调用失败: {e}\")\n",
    "        return None\n",
    "\n",
    "# =====================================================================================\n",
    "# 第4步：执行主流程 (Run the Main Process)\n",
    "# =====================================================================================\n",
    "# 说明：现在，我们将上面定义好的所有“小工具”按顺序串联起来，执行完整的处理流程。\n",
    "# 操作：直接运行下面的代码块即可。\n",
    "\n",
    "# --- 流程1: 提取文本 ---\n",
    "full_text = extract_text_from_pdfs(pdf_paths)\n",
    "\n",
    "# --- 流程2: 文本分块 ---\n",
    "text_chunks = chunk_text(full_text)\n",
    "\n",
    "# --- 流程3: AI生成Q&A ---\n",
    "print(\"\\n--- 开始调用AI批量生成Q&A (这可能需要一些时间) ---\")\n",
    "all_qa_pairs = []\n",
    "for chunk in tqdm(text_chunks, desc=\"AI生成进度\"):\n",
    "    # 【！！！核心修改！！！】将全局配置的MODEL_NAME传入函数\n",
    "    qa_pairs_str = generate_qa_from_chunk(chunk, MODEL_NAME)\n",
    "    if qa_pairs_str:\n",
    "        for line in qa_pairs_str.strip().split(\"\\n\"):\n",
    "            try:\n",
    "                qa_pair = json.loads(line)\n",
    "                if \"input\" in qa_pair and \"output\" in qa_pair:\n",
    "                    all_qa_pairs.append(qa_pair)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"⚠️警告：无法解析AI返回的一行内容: {line}\")\n",
    "\n",
    "print(f\"\\n✅ 成功生成了 {len(all_qa_pairs)} 条Q&A数据！\")\n",
    "\n",
    "# --- 流程4: 数据清洗和分割 ---\n",
    "if all_qa_pairs:\n",
    "    print(\"\\n--- 开始数据清洗和分割 ---\")\n",
    "    df = pd.DataFrame(all_qa_pairs)\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(subset=[\"input\"], inplace=True)\n",
    "    \n",
    "    train_df, val_df = train_test_split(df, test_size=VALIDATION_SET_RATIO, random_state=42)\n",
    "    \n",
    "    train_df.to_json(output_train_file, orient='records', lines=True, force_ascii=False)\n",
    "    val_df.to_json(output_val_file, orient='records', lines=True, force_ascii=False)\n",
    "    \n",
    "    print(f\"✅ 数据处理完成！\")\n",
    "    print(f\"   - 训练集 ({len(train_df)}条) 已保存到: {output_train_file}\")\n",
    "    print(f\"   - 验证集 ({len(val_df)}条) 已保存到: {output_val_file}\")\n",
    "else:\n",
    "    print(\"❌ 未能生成任何Q&A数据，请检查API配置或PDF内容。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
